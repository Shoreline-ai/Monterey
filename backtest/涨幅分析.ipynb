{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiwei/Desktop/git/Monterey/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/yiwei/Desktop/git/Monterey/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quantstats as qs # Keep for potential later use or plotting, but not core to the request\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr # Needed for IC calculation\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Quantstats might issue user warnings\n",
    "# --- Function Definitions ---\n",
    "\n",
    "# Step 1: Load Data (Unchanged - Ensure 'open', 'high', 'close', 'pct_chg' exist)\n",
    "def load_data(cb_path, index_path):\n",
    "    \"\"\"Loads CB and index data, ensures DatetimeIndex.\"\"\"\n",
    "    print(\"--- Step 1: Loading Data ---\")\n",
    "    try:\n",
    "        df = pd.read_parquet(cb_path)\n",
    "        index_df = pd.read_parquet(index_path) # Index data might not be needed unless comparing results later\n",
    "\n",
    "        # Ensure index_df has DatetimeIndex (optional but good practice)\n",
    "        if index_df is not None and not isinstance(index_df.index, pd.DatetimeIndex):\n",
    "            index_df.index = pd.to_datetime(index_df.index)\n",
    "\n",
    "        # Ensure df has correct MultiIndex with DatetimeIndex for trade_date\n",
    "        required_levels = ['code', 'trade_date']\n",
    "        if all(level in df.index.names for level in required_levels):\n",
    "            date_level_idx = df.index.names.index('trade_date')\n",
    "            if not isinstance(df.index.levels[date_level_idx], pd.DatetimeIndex):\n",
    "                 df.index = df.index.set_levels(pd.to_datetime(df.index.levels[date_level_idx]), level='trade_date')\n",
    "        else: # Try setting index if columns exist\n",
    "            if all(col in df.columns for col in required_levels):\n",
    "                 df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "                 df = df.set_index(required_levels)\n",
    "            else: raise ValueError(\"CB data missing 'code' or 'trade_date' for index.\")\n",
    "\n",
    "        # --- Add check for required return calculation columns ---\n",
    "        required_cols = ['open', 'high', 'close', 'pct_chg'] # pct_chg is raw % change from prev close to current close\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "             raise ValueError(f\"Required columns for return calculation missing: {missing_cols}\")\n",
    "        # --- End Check ---\n",
    "\n",
    "        print(f\"Loaded CB data shape: {df.shape}\")\n",
    "        if index_df is not None:\n",
    "            print(f\"Loaded Index data shape: {index_df.shape}\")\n",
    "        return df, index_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Step 2: Filter Data (Unchanged - But ensure factors needed are just the ones to analyze)\n",
    "def filter_data(df, start_date, end_date, filter_rules, factors_to_analyze):\n",
    "    \"\"\"Applies date range and custom filters.\"\"\"\n",
    "    print(\"--- Step 2: Filtering Data ---\")\n",
    "    if df is None: return None\n",
    "\n",
    "    # Check if factor columns exist BEFORE filtering\n",
    "    missing_factors = [f for f in factors_to_analyze if f not in df.columns]\n",
    "    if missing_factors:\n",
    "        print(f\"Error: Required factor columns missing from data: {missing_factors}\")\n",
    "        return None\n",
    "\n",
    "    # Date filtering\n",
    "    try:\n",
    "        if 'trade_date' not in df.index.names:\n",
    "             raise KeyError(\"'trade_date' not found in DataFrame index levels.\")\n",
    "        trade_date_level = df.index.get_level_values('trade_date')\n",
    "        date_mask = (trade_date_level >= start_date) & (trade_date_level <= end_date)\n",
    "        df_filtered = df[date_mask].copy()\n",
    "        if df_filtered.empty: raise ValueError(\"No data remaining after date filtering.\")\n",
    "        print(f\"Filtered by date: {start_date} to {end_date}. Shape: {df_filtered.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during date filtering: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Apply standard filters (Redemption, Listing days)\n",
    "    df_filtered['filter_out'] = False\n",
    "    redeem_statuses = ['已公告强赎', '公告到期赎回', '公告实施强赎', '公告提示强赎', '已满足强赎条件']\n",
    "    if 'is_call' in df_filtered.columns: df_filtered.loc[df_filtered['is_call'].isin(redeem_statuses), 'filter_out'] = True\n",
    "    if 'list_days' in df_filtered.columns: df_filtered.loc[df_filtered['list_days'] <= 3, 'filter_out'] = True\n",
    "\n",
    "    # Apply custom filters\n",
    "    print(\"Applying custom filters...\")\n",
    "    for rule in filter_rules:\n",
    "        try:\n",
    "            print(f\" - Applying: {rule}\")\n",
    "            matching_indices = df_filtered.query(rule).index\n",
    "            df_filtered.loc[matching_indices, 'filter_out'] = True\n",
    "        except Exception as e:\n",
    "            print(f\"  - Warning: Could not apply filter rule '{rule}'. Error: {e}\")\n",
    "\n",
    "    # --- IMPORTANT: Filter out rows where any needed factor is NaN before calculating returns ---\n",
    "    # We need valid factor values *today* to correlate with *tomorrow's* return\n",
    "    print(\"Filtering rows with NaN in factor values...\")\n",
    "    nan_mask = df_filtered[factors_to_analyze].isna().any(axis=1)\n",
    "    df_filtered.loc[nan_mask, 'filter_out'] = True\n",
    "\n",
    "    eligible_count = len(df_filtered[~df_filtered['filter_out']])\n",
    "    print(f\"Filtering complete. Eligible bond-days: {eligible_count}\")\n",
    "    if eligible_count == 0:\n",
    "        print(\"Warning: No bonds eligible after applying all filters.\")\n",
    "    return df_filtered\n",
    "\n",
    "# --- NEW Step 3: Calculate Multiple Forward Returns ---\n",
    "def calculate_multiple_fwd_returns(df, pulse_percentages):\n",
    "    \"\"\"\n",
    "    Calculates various next-day return metrics for each bond.\n",
    "    - 'fwd_ret_close': Raw percentage change from current close to next close.\n",
    "    - 'fwd_ret_pulse_X': Return based on pulse stop-profit logic at X%.\n",
    "    \"\"\"\n",
    "    print(\"--- Step 3: Calculating Multiple Forward Returns ---\")\n",
    "    if df is None: return None\n",
    "    required_cols = ['open', 'high', 'close', 'pct_chg']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Error: Missing required columns for return calc: {required_cols}\")\n",
    "        return None\n",
    "    if not isinstance(df.index, pd.MultiIndex) or 'code' not in df.index.names:\n",
    "        print(\"Error: DataFrame needs MultiIndex with 'code' level for forward returns.\")\n",
    "        return None\n",
    "\n",
    "    df_with_fwd = df.copy()\n",
    "    grouped = df_with_fwd.groupby(level='code')\n",
    "\n",
    "    # Get next day's data\n",
    "    df_with_fwd['next_open'] = grouped['open'].shift(-1)\n",
    "    df_with_fwd['next_high'] = grouped['high'].shift(-1)\n",
    "    df_with_fwd['next_close'] = grouped['close'].shift(-1)\n",
    "    # Note: 'pct_chg' is the change from T-1 close to T close.\n",
    "    # We need T close to T+1 close. Calculate manually or use shifted pct_chg.\n",
    "    # Using shifted pct_chg assumes it represents close-to-close.\n",
    "    df_with_fwd['fwd_ret_close'] = grouped['pct_chg'].shift(-1)\n",
    "\n",
    "    # Calculate pulse returns\n",
    "    current_close = df_with_fwd['close']\n",
    "    next_open = df_with_fwd['next_open']\n",
    "    next_high = df_with_fwd['next_high']\n",
    "    next_close = df_with_fwd['next_close'] # Not strictly needed if using fwd_ret_close\n",
    "    raw_next_day_ret = df_with_fwd['fwd_ret_close'] # (next_close - current_close) / current_close\n",
    "\n",
    "    # Handle cases where next day data is missing (last day for a bond)\n",
    "    valid_next_day = next_open.notna() & next_high.notna() & current_close.notna() & (current_close > 0)\n",
    "\n",
    "    for pct in pulse_percentages:\n",
    "        ret_col_name = f'fwd_ret_pulse_{pct:.1f}' # e.g., fwd_ret_pulse_2.5\n",
    "        stop_profit_pct = pct / 100.0\n",
    "        threshold_price = current_close * (1 + stop_profit_pct)\n",
    "\n",
    "        # Initialize return column with NaN\n",
    "        df_with_fwd[ret_col_name] = np.nan\n",
    "\n",
    "        # Calculate returns only where next day data is valid\n",
    "        # Condition 1: Triggered at open\n",
    "        cond_open_trig = valid_next_day & (next_open >= threshold_price)\n",
    "        df_with_fwd.loc[cond_open_trig, ret_col_name] = (next_open[cond_open_trig] - current_close[cond_open_trig]) / current_close[cond_open_trig]\n",
    "\n",
    "        # Condition 2: Triggered intraday (high >= threshold, but open < threshold)\n",
    "        cond_intra_trig = valid_next_day & ~cond_open_trig & (next_high >= threshold_price)\n",
    "        df_with_fwd.loc[cond_intra_trig, ret_col_name] = stop_profit_pct\n",
    "\n",
    "        # Condition 3: Not triggered, use close-to-close return\n",
    "        cond_no_trig = valid_next_day & ~cond_open_trig & ~cond_intra_trig\n",
    "        df_with_fwd.loc[cond_no_trig, ret_col_name] = raw_next_day_ret[cond_no_trig] # Use pre-calculated raw return\n",
    "\n",
    "    # Clean up intermediate columns if desired\n",
    "    # df_with_fwd = df_with_fwd.drop(columns=['next_open', 'next_high', 'next_close'])\n",
    "\n",
    "    return_cols = ['fwd_ret_close'] + [f'fwd_ret_pulse_{pct:.1f}' for pct in pulse_percentages]\n",
    "    nan_counts = df_with_fwd[return_cols].isna().sum()\n",
    "    print(f\"Calculated forward returns. Example NaN counts:\\n{nan_counts}\")\n",
    "    return df_with_fwd, return_cols\n",
    "\n",
    "\n",
    "# --- NEW Step 4: Analyze Factor vs. Each Return Type Relationship (IC/IR) ---\n",
    "def analyze_factor_return_relationships(df, factors, return_cols):\n",
    "    \"\"\"\n",
    "    Calculates Information Coefficient (IC) and Information Ratio (IR)\n",
    "    for each factor against each specified forward return column.\n",
    "    \"\"\"\n",
    "    print(f\"--- Step 4: Analyzing Factor Relationships with {len(return_cols)} Return Types ---\")\n",
    "    if df is None:\n",
    "        print(\"Error: DataFrame is missing.\")\n",
    "        return None\n",
    "    if not isinstance(df.index, pd.MultiIndex) or 'trade_date' not in df.index.names:\n",
    "        print(\"Error: DataFrame needs MultiIndex with 'trade_date' level for IC calc.\")\n",
    "        return None\n",
    "\n",
    "    all_ic_results = {}\n",
    "    daily_ic_data = {} # Optional: Store all daily ICs for plotting if needed\n",
    "\n",
    "    # --- Use eligible rows only ---\n",
    "    # Filter_out was applied before return calculation.\n",
    "    # Now, we need rows that were eligible AND have valid return values for the specific return column being analyzed.\n",
    "    df_eligible_base = df[~df['filter_out']].copy()\n",
    "\n",
    "    if df_eligible_base.empty:\n",
    "        print(\"Warning: No eligible bond-days found based on initial filters.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Iterate through each type of forward return\n",
    "    for return_col in tqdm(return_cols, desc=\"Analyzing Return Types\"):\n",
    "        if return_col not in df_eligible_base.columns:\n",
    "            print(f\"Warning: Return column '{return_col}' not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n-- Analyzing Factors vs. Return: '{return_col}' --\")\n",
    "        # Drop rows where *this specific* return is NaN for this analysis pass\n",
    "        df_analysis = df_eligible_base.dropna(subset=[return_col])\n",
    "\n",
    "        if df_analysis.empty:\n",
    "            print(f\"Warning: No eligible data with valid '{return_col}' for IC calculation.\")\n",
    "            continue\n",
    "\n",
    "        # Group by date to calculate daily IC\n",
    "        grouped = df_analysis.groupby(level='trade_date')\n",
    "        num_days = len(grouped)\n",
    "        print(f\"Analyzing {num_days} days for '{return_col}'...\")\n",
    "\n",
    "        # Iterate through each factor for the current return type\n",
    "        for factor in factors:\n",
    "            if factor not in df_analysis.columns:\n",
    "                print(f\"Warning: Factor '{factor}' not found. Skipping for '{return_col}'.\")\n",
    "                continue\n",
    "\n",
    "            # Function to safely calculate Spearman correlation per day\n",
    "            def safe_spearman(group):\n",
    "                # Drop NaNs *within the group* for this specific factor and return_col\n",
    "                # Factor NaNs should have been filtered already, but double-check\n",
    "                group_cleaned = group[[factor, return_col]].dropna()\n",
    "                if len(group_cleaned) < 5: # Need sufficient pairs for meaningful correlation (adjust as needed)\n",
    "                    return np.nan\n",
    "                try:\n",
    "                    corr, p_val = spearmanr(group_cleaned[factor], group_cleaned[return_col])\n",
    "                    return corr\n",
    "                except ValueError: # Handle cases like zero variance\n",
    "                    return np.nan\n",
    "\n",
    "            # Apply the function to each day's group\n",
    "            try:\n",
    "                daily_ic = grouped.apply(safe_spearman)\n",
    "                daily_ic_clean = daily_ic.dropna() # Store non-NaN ICs for stats\n",
    "\n",
    "                # Store daily IC series if needed later\n",
    "                # daily_ic_data[(factor, return_col)] = daily_ic_clean\n",
    "\n",
    "                if daily_ic_clean.empty:\n",
    "                    print(f\" - Factor '{factor}': No valid daily ICs calculated.\")\n",
    "                    mean_ic, std_ic, ir, ic_positive_ratio, num_obs = np.nan, np.nan, np.nan, np.nan, 0\n",
    "                elif len(daily_ic_clean) < 2:\n",
    "                    # print(f\" - Factor '{factor}': Only 1 valid daily IC. Cannot calculate Std Dev/IR.\")\n",
    "                    mean_ic = daily_ic_clean.mean()\n",
    "                    std_ic = np.nan\n",
    "                    ir = np.nan\n",
    "                    ic_positive_ratio = (daily_ic_clean > 0).mean()\n",
    "                    num_obs = len(daily_ic_clean)\n",
    "                else:\n",
    "                    mean_ic = daily_ic_clean.mean()\n",
    "                    std_ic = daily_ic_clean.std()\n",
    "                    ir = mean_ic / std_ic if std_ic != 0 and not np.isnan(std_ic) else np.nan # Avoid division by zero/NaN\n",
    "                    ic_positive_ratio = (daily_ic_clean > 0).mean()\n",
    "                    num_obs = len(daily_ic_clean)\n",
    "\n",
    "                all_ic_results[(factor, return_col)] = {\n",
    "                    'Mean IC': mean_ic,\n",
    "                    'IC Std Dev': std_ic,\n",
    "                    'IR (IC Mean/Std)': ir,\n",
    "                    'IC > 0 Ratio': ic_positive_ratio,\n",
    "                    'Num Observations (Days)': num_obs\n",
    "                }\n",
    "                # Optional: Print summary per factor\n",
    "                # print(f\"   - Factor '{factor}': Mean IC={mean_ic:.4f}, IR={ir:.4f}, Obs={num_obs}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating IC for factor '{factor}' vs '{return_col}': {e}\")\n",
    "                all_ic_results[(factor, return_col)] = {k: np.nan for k in ['Mean IC', 'IC Std Dev', 'IR (IC Mean/Std)', 'IC > 0 Ratio', 'Num Observations (Days)']}\n",
    "\n",
    "    print(\"\\nIC/IR calculation complete for all factor/return pairs.\")\n",
    "    if not all_ic_results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Format results into a DataFrame\n",
    "    results_df = pd.DataFrame.from_dict(all_ic_results, orient='index')\n",
    "    results_df.index = pd.MultiIndex.from_tuples(results_df.index, names=['Factor', 'Return Type'])\n",
    "    results_df = results_df.sort_index()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Step 5: Analyze Factor Correlation (Optional, Unchanged)\n",
    "def analyze_factor_correlation(df, factors):\n",
    "    \"\"\"Calculates and displays the correlation matrix for the selected factors.\"\"\"\n",
    "    print(\"--- Step 5: Analyzing Factor Correlation (Optional) ---\")\n",
    "    if df is None or 'filter_out' not in df.columns: return None\n",
    "    if not isinstance(df.index, pd.MultiIndex) or not all(name in df.index.names for name in ['code', 'trade_date']):\n",
    "         print(f\"Error: DataFrame index is not the expected MultiIndex for correlation. Index: {df.index}\")\n",
    "         return None\n",
    "\n",
    "    # Use data *before* forward returns were added, but after filtering\n",
    "    df_eligible = df[~df['filter_out']].copy()\n",
    "    if df_eligible.empty: print(\"Warning: No eligible bonds for correlation.\"); return None\n",
    "    if len(df_eligible) < 2: print(\"Warning: Less than 2 data points for correlation.\"); return None\n",
    "\n",
    "    missing_factors = [f for f in factors if f not in df_eligible.columns]\n",
    "    if missing_factors: print(f\"Warning: Factors missing for correlation: {missing_factors}\");\n",
    "    present_factors = [f for f in factors if f in df_eligible.columns]\n",
    "    if len(present_factors) < 2: print(\"Warning: Need at least 2 factors for correlation.\"); return None\n",
    "\n",
    "    factor_data = df_eligible[present_factors]\n",
    "    factor_data = factor_data.replace([np.inf, -np.inf], np.nan)\n",
    "    # Drop rows with NaNs only in the columns used for this specific calculation\n",
    "    factor_data = factor_data.dropna() # Drop rows if *any* factor is NaN\n",
    "\n",
    "    if len(factor_data) < 2: print(\"Warning: Less than 2 valid data points after NaN drop for correlation.\"); return None\n",
    "    if len(factor_data) > 50000: # Limit size for performance if dataset is huge\n",
    "        print(f\"Sampling {50000} rows for factor correlation calculation...\")\n",
    "        factor_data = factor_data.sample(50000, random_state=42)\n",
    "\n",
    "\n",
    "    print(\"Calculating Spearman rank correlation matrix...\")\n",
    "    try:\n",
    "        correlation_matrix = factor_data.corr(method='spearman')\n",
    "        print(\"Factor Correlation Matrix:\")\n",
    "        plt.figure(figsize=(max(6, len(present_factors)*0.8), max(5, len(present_factors)*0.6))) # Adjust size\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, annot_kws={\"size\": 8})\n",
    "        plt.title('Factor Spearman Rank Correlation Heatmap')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return correlation_matrix\n",
    "    except Exception as corr_e:\n",
    "         print(f\"Error calculating or plotting correlation: {corr_e}\")\n",
    "         return None\n",
    "\n",
    "\n",
    "# --- Main Execution Function (Simplified) ---\n",
    "def run_simplified_factor_analysis(config):\n",
    "    \"\"\"Orchestrates the simplified factor vs. return analysis.\"\"\"\n",
    "\n",
    "    # Step 1: Load Data\n",
    "    df_cb_raw, _ = load_data(config['cb_data_path'], config.get('index_data_path')) # Index not strictly needed now\n",
    "    if df_cb_raw is None: return None\n",
    "\n",
    "    # Extract config parameters\n",
    "    start_date = config['start_date']\n",
    "    end_date = config['end_date']\n",
    "    filters = config['filters']\n",
    "    factors_to_analyze = config['factors_to_analyze'] # List of factor names\n",
    "    pulse_percentages = config['pulse_percentages']\n",
    "\n",
    "    # Step 2: Filter Data\n",
    "    df_filtered = filter_data(df_cb_raw, start_date, end_date, filters, factors_to_analyze)\n",
    "    if df_filtered is None or df_filtered[~df_filtered['filter_out']].empty:\n",
    "        print(\"Stopping analysis due to filtering issues or no eligible data.\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: Calculate Multiple Forward Returns\n",
    "    df_with_returns, return_cols = calculate_multiple_fwd_returns(df_filtered, pulse_percentages)\n",
    "    if df_with_returns is None:\n",
    "         print(\"Stopping analysis: Failed to calculate forward returns.\")\n",
    "         return None\n",
    "\n",
    "    # Step 4: Analyze Factor vs. Each Return Type Relationship\n",
    "    ic_results_df = analyze_factor_return_relationships(df_with_returns, factors_to_analyze, return_cols)\n",
    "\n",
    "    # Step 5: Analyze Factor Correlation (Optional)\n",
    "    factor_correlation_matrix = None\n",
    "    if config.get('analyze_factor_correlation', False): # Add a flag in config\n",
    "        # Pass df_filtered (before returns were added) to avoid issues if return calc failed partially\n",
    "        factor_correlation_matrix = analyze_factor_correlation(df_filtered, factors_to_analyze)\n",
    "\n",
    "\n",
    "    # --- Step 6: Report Results ---\n",
    "    print(\"\\n\" + \"=\"*30 + \" Simplified Factor Analysis Report \" + \"=\"*30)\n",
    "\n",
    "    # --- IC / IR Results ---\n",
    "    print(\"\\n--- Factor vs. Return Relationship Analysis (IC/IR) ---\")\n",
    "    if ic_results_df is not None and not ic_results_df.empty:\n",
    "        display(ic_results_df.style.format({\n",
    "            'Mean IC': '{:.4f}',\n",
    "            'IC Std Dev': '{:.4f}',\n",
    "            'IR (IC Mean/Std)': '{:.3f}',\n",
    "            'IC > 0 Ratio': '{:.1%}',\n",
    "            'Num Observations (Days)': '{:,.0f}'\n",
    "        }))\n",
    "\n",
    "        # --- Highlight Strongest Relationships ---\n",
    "        print(\"\\n--- Strongest Relationships (Highest Absolute Mean IC per Return Type) ---\")\n",
    "        # Group by return type and find the factor with max abs Mean IC\n",
    "        idx_max_abs_ic = ic_results_df.loc[ic_results_df.groupby(level='Return Type')['Mean IC'].idxmax(skipna=True)]\n",
    "        idx_min_abs_ic = ic_results_df.loc[ic_results_df.groupby(level='Return Type')['Mean IC'].idxmin(skipna=True)]\n",
    "\n",
    "        # Combine and select the one with larger absolute value\n",
    "        strongest_ic = {}\n",
    "        for ret_type in return_cols:\n",
    "             max_row = idx_max_abs_ic.loc[idx_max_abs_ic.index.get_level_values('Return Type') == ret_type]\n",
    "             min_row = idx_min_abs_ic.loc[idx_min_abs_ic.index.get_level_values('Return Type') == ret_type]\n",
    "\n",
    "             best_row = None\n",
    "             if not max_row.empty and not min_row.empty:\n",
    "                 if abs(max_row['Mean IC'].iloc[0]) >= abs(min_row['Mean IC'].iloc[0]):\n",
    "                     best_row = max_row\n",
    "                 else:\n",
    "                     best_row = min_row\n",
    "             elif not max_row.empty:\n",
    "                 best_row = max_row\n",
    "             elif not min_row.empty:\n",
    "                 best_row = min_row\n",
    "\n",
    "             if best_row is not None:\n",
    "                strongest_ic[ret_type] = best_row\n",
    "\n",
    "        if strongest_ic:\n",
    "            summary_df = pd.concat(strongest_ic.values())\n",
    "            display(summary_df.style.format({\n",
    "                'Mean IC': '{:.4f}', 'IC Std Dev': '{:.4f}', 'IR (IC Mean/Std)': '{:.3f}',\n",
    "                'IC > 0 Ratio': '{:.1%}', 'Num Observations (Days)': '{:,.0f}'\n",
    "            }))\n",
    "        else:\n",
    "            print(\"Could not determine strongest relationships.\")\n",
    "\n",
    "\n",
    "        print(\"\\n--- Strongest Relationships (Highest Absolute IR per Return Type) ---\")\n",
    "        # Similar logic for IR\n",
    "        # Need to handle NaN IRs carefully\n",
    "        ic_results_df_ir = ic_results_df.copy()\n",
    "        ic_results_df_ir['Abs IR'] = ic_results_df_ir['IR (IC Mean/Std)'].abs()\n",
    "        idx_max_abs_ir = ic_results_df_ir.loc[ic_results_df_ir.groupby(level='Return Type')['Abs IR'].idxmax(skipna=True)]\n",
    "\n",
    "        if not idx_max_abs_ir.empty:\n",
    "             display(idx_max_abs_ir.drop(columns=['Abs IR']).style.format({ # Display original columns\n",
    "                 'Mean IC': '{:.4f}', 'IC Std Dev': '{:.4f}', 'IR (IC Mean/Std)': '{:.3f}',\n",
    "                 'IC > 0 Ratio': '{:.1%}', 'Num Observations (Days)': '{:,.0f}'\n",
    "             }))\n",
    "        else:\n",
    "             print(\"Could not determine strongest relationships based on IR (potentially all NaNs).\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Factor vs. Return relationship results are not available.\")\n",
    "\n",
    "    # --- Factor Correlation Results ---\n",
    "    if config.get('analyze_factor_correlation', False):\n",
    "        print(\"\\n--- Factor Correlation Matrix ---\")\n",
    "        if factor_correlation_matrix is not None:\n",
    "            print(\"(See heatmap plot above)\")\n",
    "        else:\n",
    "            print(\"Factor correlation matrix could not be calculated.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30 + \" Analysis Complete \" + \"=\"*30)\n",
    "\n",
    "    # Return key results\n",
    "    return {\n",
    "        \"factor_return_ic_ir\": ic_results_df,\n",
    "        \"factor_correlation\": factor_correlation_matrix,\n",
    "        \"final_data_with_returns\": df_with_returns # Include data for inspection\n",
    "    }\n",
    "\n",
    "# --- Example Configuration (Simplified) ---\n",
    "CONFIG = {\n",
    "    'cb_data_path': 'cb_data.parquet',       # Replace with your actual path\n",
    "    'index_data_path': 'index_data.parquet', # Optional, not used in core analysis now\n",
    "    'start_date': '2022-01-01',\n",
    "    'end_date': '2023-12-31',\n",
    "    'filters': [\n",
    "        \"`转股溢价率` < 0.5\",   # Example: Premium < 50%\n",
    "        \"`剩余规模` > 0.1\",     # Example: Remaining size > 0.1 Billion\n",
    "        \"`close` < 150\",       # Example: Price < 150\n",
    "        # Add more filter rules as strings usable by df.query()\n",
    "    ],\n",
    "    'factors_to_analyze': [\n",
    "        'ytm',              # Example: Yield to maturity\n",
    "        '转股溢价率',        # Example: Conversion premium\n",
    "        '剩余规模',         # Example: Remaining size\n",
    "        'volume_ratio',     # Example: Volume ratio (ensure this exists)\n",
    "        'double_low',       # Example: Double low value (ensure this exists)\n",
    "        # Add all factor column names you want to analyze\n",
    "    ],\n",
    "    'pulse_percentages': [2.5, 2.8, 3.0, 3.8, 5.0, 5.8, 8.0], # Define the stop-profit thresholds\n",
    "    'analyze_factor_correlation': True # Set to True to run factor correlation analysis\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Data ---\n",
      "Loaded CB data shape: (593654, 374)\n",
      "Loaded Index data shape: (1765, 8)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'factors_to_analyze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 45\u001b[0m\n\u001b[1;32m      2\u001b[0m CONFIG \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# File Paths\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcb_data_path\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/yiwei/Desktop/git/cb_data_with_factors2.pq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# CHANGE TO YOUR PATH\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     }\n\u001b[1;32m     41\u001b[0m }\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Run the simplified analysis\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m analysis_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_simplified_factor_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# You can access results like:\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# if analysis_results:\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m#     ic_ir_df = analysis_results[\"factor_return_ic_ir\"]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m#     if ic_ir_df is not None:\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m#          print(\"\\n --- IC/IR Results Head ---\")\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m#          display(ic_ir_df.head())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 351\u001b[0m, in \u001b[0;36mrun_simplified_factor_analysis\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    349\u001b[0m end_date \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    350\u001b[0m filters \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 351\u001b[0m factors_to_analyze \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfactors_to_analyze\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# List of factor names\u001b[39;00m\n\u001b[1;32m    352\u001b[0m pulse_percentages \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpulse_percentages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Step 2: Filter Data\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'factors_to_analyze'"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    # File Paths\n",
    "    'cb_data_path': '/Users/yiwei/Desktop/git/cb_data_with_factors2.pq', # CHANGE TO YOUR PATH\n",
    "    'index_data_path': '/Users/yiwei/Desktop/git/index.pq', # CHANGE TO YOUR PATH\n",
    "\n",
    "    # Analysis Settings\n",
    "    'start_date': '20220801',\n",
    "    'end_date': '20240101',   # Adjust end date as needed\n",
    "    'num_bins': 5,           # Number of quantiles (e.g., 5 for quintiles)\n",
    "    'benchmark_col': 'index_jsl',\n",
    "\n",
    "    # Stop-Profit & Commission (for return calculation)\n",
    "    'stop_profit_pct': 0.03,\n",
    "    'commission_rate': 2 / 1000,\n",
    "\n",
    "    # Data Filtering Rules (applied first)\n",
    "    'filters': [\n",
    "        \"close < 102\",\n",
    "        \"close > 155\", # Example: wider price range for analysis\n",
    "        \"left_years < 0.5\",\n",
    "        \"amount < 500\",  # Example: lower liquidity threshold\n",
    "        # Add other essential filters if needed (like redeem status, list_days)\n",
    "        # \"is_call.isin(['已公告强赎', '公告到期赎回', '公告实施强赎', '公告提示强赎', '已满足强赎条件']) == False\", # Example keeping only non-redeem\n",
    "        # \"list_days > 3\"\n",
    "    ],\n",
    "\n",
    "    # Factors to Analyze and their Weights for the Composite Score\n",
    "    # Keys: Factor column names from df.\n",
    "    # Values: Weight (positive means higher factor value -> better score contribution,\n",
    "    #         negative means lower factor value -> better score contribution).\n",
    "    # The ranking logic internally handles this direction based on weight sign.\n",
    "    'factors_and_weights': {\n",
    "        'ytm': 1.0,           # Higher YTM is better\n",
    "        'conv_prem': -1.0,    # Lower premium is better\n",
    "        'turnover_5': 1.5,    # Higher turnover might indicate interest\n",
    "        'bond_prem': -1.0,    # Lower bond premium (closer to pure bond value) might be safer\n",
    "        'theory_bias': -1,\n",
    "        # Add other factors you want to combine\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Run the simplified analysis\n",
    "analysis_results = run_simplified_factor_analysis(CONFIG)\n",
    "\n",
    "    # You can access results like:\n",
    "    # if analysis_results:\n",
    "    #     ic_ir_df = analysis_results[\"factor_return_ic_ir\"]\n",
    "    #     if ic_ir_df is not None:\n",
    "    #          print(\"\\n --- IC/IR Results Head ---\")\n",
    "    #          display(ic_ir_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
