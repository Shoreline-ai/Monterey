{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiwei/Desktop/git/Monterey/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_excel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "import copy\n",
    "from pandas import IndexSlice as idx\n",
    "pd.set_option('display.max_columns', None)  # å½“åˆ—å¤ªå¤šæ—¶ä¸æ¢è¡Œ\n",
    "from numpy import exp, nan\n",
    "import quantstats as qs\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # å¿½ç•¥è­¦å‘Š\n",
    "import pandas as pd\n",
    "from pandas import IndexSlice as idx\n",
    "\n",
    "import talib as ta\n",
    "# è®¡ç®—natr\n",
    "def natr(df, n):\n",
    "    high = np.array([float(x) for x in df['high']])\n",
    "    low = np.array([float(x) for x in df['low']])\n",
    "    close = np.array([float(x) for x in df['close']])\n",
    "    df['natr'] = ta.NATR(high, low, close, timeperiod=n)\n",
    "    return df['natr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»¥ä¸‹ä¸ºå®Œæ•´çš„å¯è½¬å€ºå› å­åˆ†ç±»ä¸è®¡ç®—é¡ºåºæ•´ç†ï¼š\n",
    "\n",
    "# ä¸€ã€åŸºæœ¬ä»·æ ¼ä¸æ³¢åŠ¨ç±»å› å­ï¼ˆè½¬å€ºæœ¬èº«ï¼‰\n",
    "\n",
    "# high, low, close, vol åŸºç¡€å­—æ®µ\n",
    "\n",
    "# ma_20, momentum_20, volatility_20\n",
    "\n",
    "# max_value, max_value_position\n",
    "\n",
    "# zhengfu, zhengfu_cha\n",
    "\n",
    "# natr_14, natr_[1,3,5,10,20]\n",
    "\n",
    "# aft_high_cur_close\n",
    "\n",
    "# äºŒã€OBVé‡èƒ½æŒ‡æ ‡ï¼ˆè½¬å€ºï¼‰\n",
    "\n",
    "# obv, obv_5, obv_10, obv_ratio_5_10\n",
    "\n",
    "# ä¸‰ã€æ¢æ‰‹ä¸å¸‚å€¼ç±»å› å­\n",
    "\n",
    "# turnover_pct\n",
    "\n",
    "# cap_float_share_rate\n",
    "\n",
    "# turnover_[5,10,20,60]_avg\n",
    "\n",
    "# rolling_[1,5,20,50]_avg\n",
    "\n",
    "# rolling_1_to_5_avg, rolling_5_to_20_avg, rolling_20_to_50_avg\n",
    "\n",
    "# å››ã€åŒºé—´æ”¶ç›Šç‡ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\n",
    "\n",
    "# pct_chg_[5,20]\n",
    "\n",
    "# pct_chg_stk_[5,20]\n",
    "\n",
    "# äº”ã€æˆäº¤é‡å‡å€¼æ¯”å› å­ï¼ˆè½¬å€ºï¼‰\n",
    "\n",
    "# vol_[3,5,10,20,30,60]_avg\n",
    "\n",
    "# vol_[N]to[M]\n",
    "\n",
    "# å…­ã€æ³¢åŠ¨ç‡ä¸æŒ¯å¹…ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\n",
    "\n",
    "# bodong_[5,10,20,60], bodong_[5,10,20,60]_bd, bodong_20_to_bodong_60\n",
    "\n",
    "# zhengfu_[1,5,10,20,60], zhengfu_[1,5,10,20,60]_bodong\n",
    "\n",
    "# ä¸ƒã€è·³ç©ºä¸ç¼ºå£ç±»å› å­ï¼ˆè½¬å€ºï¼‰\n",
    "\n",
    "# high_jump, low_gap, open_jump, gap_body_ratio\n",
    "\n",
    "# high_jump_count_[20,100,250]\n",
    "\n",
    "# low_gap_count_[20,100,250]\n",
    "\n",
    "# high_jump_count_[20,100,250]pct, low_gap_count[20,100,250]_pct\n",
    "\n",
    "# å…«ã€Kçº¿ç»“æ„å› å­ï¼ˆè½¬å€ºï¼‰\n",
    "\n",
    "# close_to_high_ratio, close_to_low_ratio\n",
    "\n",
    "# body_position, upper_shadow_ratio, lower_shadow_ratio\n",
    "\n",
    "# ä¹ã€è¶‹åŠ¿åè½¬ç±»Alphaå› å­ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\n",
    "\n",
    "# alpha6, alpha6_stk\n",
    "\n",
    "# alpha12, alpha12_stk\n",
    "\n",
    "# alpha83, alpha83_stk\n",
    "\n",
    "# alpha18, alpha18_stk\n",
    "\n",
    "# alpha36, alpha36_stk\n",
    "\n",
    "# alpha89, alpha89_stk\n",
    "\n",
    "# alpha65, alpha65_stk\n",
    "\n",
    "# alpha76, alpha76_stk\n",
    "\n",
    "# alpha92, alpha92_stk\n",
    "\n",
    "# alpha99, alpha99_stk\n",
    "\n",
    "# åã€è‚¡ç¥¨ä¸è½¬å€ºè”åŠ¨å› å­\n",
    "\n",
    "# stk_up_bond_flat, stk_down_bond_weak, bond_hold_stk_rebound\n",
    "\n",
    "# stk_chg_[3,5], bond_chg_[3,5]\n",
    "\n",
    "# stk_up_bond_flat_[3,5]\n",
    "\n",
    "# stk_down_then_up, bond_rebound, bond_follow_stk_rebound\n",
    "\n",
    "# åä¸€ã€æ¨ªçºµå‘èƒŒç¦»å› å­ï¼ˆè‚¡ç¥¨ä¸è½¬å€ºï¼‰\n",
    "\n",
    "# dev_bond_vs_stk_[3,5,10]\n",
    "\n",
    "# dev_bond_short[3]_long[20], dev_bond_short[5]_long[30]\n",
    "\n",
    "# dev_stk_short[3]_long[20], dev_stk_short[5]_long[30]\n",
    "\n",
    "# cb_vs_stk_ret_rank_diff_[3,5,10]\n",
    "\n",
    "# åäºŒã€é£é™©ä¸å›æ’¤ç›¸å…³å› å­ï¼ˆè½¬å€ºï¼‰\n",
    "\n",
    "# cb_dev_from_low_5\n",
    "\n",
    "# cb_close_std_5\n",
    "\n",
    "# cb_drawdown_5\n",
    "\n",
    "# cb_dd_prob_estimate\n",
    "\n",
    "# åä¸‰ã€éœ‡è¡æ”¶æ•›ç±»å› å­ï¼ˆè½¬å€ºï¼‰\n",
    "\n",
    "# atr_5, atr_10, atr_decay_5_10\n",
    "\n",
    "# close_std_5, close_std_10, vol_shrink_ratio\n",
    "\n",
    "# body_pct_mean_5\n",
    "\n",
    "# shadow_mean_5\n",
    "\n",
    "# small_body_shadow_ratio, doji_ratio_5\n",
    "\n",
    "# zhengfu_decay_5_20, range_ratio_5_20\n",
    "\n",
    "# åå››ã€è„‰å†²ä¸åŠ¨èƒ½å› å­ï¼ˆè½¬å€ºï¼‰\n",
    "\n",
    "# high_jump_[15,20,30,40,50,60]\n",
    "\n",
    "# open_gap_mean_[5,10], open_gap_max_[5,10]\n",
    "\n",
    "# jump_atr_[3,5,10]\n",
    "\n",
    "# zscore_pctchg_20\n",
    "\n",
    "# vol_spike_ratio\n",
    "\n",
    "# vol_std_decay\n",
    "\n",
    "# score_high_jump_[15,20,30,40,50,60]_20\n",
    "\n",
    "# range_jump_potential\n",
    "\n",
    "# gap_and_go_flag\n",
    "\n",
    "# åäº”ã€è·Œä¸åŠ¨å› å­ï¼ˆè½¬å€ºï¼‰\n",
    "\n",
    "# down_freq_[5,10]\n",
    "\n",
    "# down_amp_[5,10]\n",
    "\n",
    "# no_fall_score_[5,10]\n",
    "\n",
    "# åå…­ã€Kçº¿ç»“æ„è¿ç»­æ€§\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(filepath):\n",
    "    df = pd.read_parquet(filepath)\n",
    "    df['high'] = df['high'].astype(float)\n",
    "    df['low'] = df['low'].astype(float)\n",
    "    df['close'] = df['close'].astype(float)\n",
    "    df['vol'] = df['vol'].astype(float)\n",
    "\n",
    "    # å¸¸ç”¨å› å­\n",
    "    df['natr_14'] = ta.NATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "    df['ma_20'] = ta.SMA(df['close'], timeperiod=20)\n",
    "    df['momentum_20'] = df['close'] / df['close'].shift(20)\n",
    "    df['volatility_20'] = df['close'].rolling(20).std()\n",
    "    df['max_value'] = df.groupby('code')['close'].cummax().shift(1)\n",
    "    df['max_value_position'] = df['close'] / df['max_value']\n",
    "    df['zhengfu'] = (df['high'] - df['low']) / df['close']\n",
    "    df['zhengfu_cha'] = (df['high'] - df['close']) / (df['open'] - df['close']).abs()\n",
    "\n",
    "    # OBVï¼ˆOn Balance volï¼‰åŠå…¶è¡ç”ŸæŒ‡æ ‡\n",
    "    df['obv'] = df.groupby('code').apply(lambda x: ta.OBV(x['close'], x['vol'])).reset_index(level=0, drop=True)\n",
    "    df['obv_5'] = df.groupby('code')['obv'].rolling(5).mean().reset_index(level=0, drop=True)\n",
    "    df['obv_10'] = df.groupby('code')['obv'].rolling(10).mean().reset_index(level=0, drop=True)\n",
    "    df['obv_ratio_5_10'] = df['obv_5'] / df['obv_10']\n",
    "\n",
    "    # å¤šå‘¨æœŸNATR\n",
    "    for n in [1, 3, 5, 10, 20]:\n",
    "        df[f'natr_{n}'] = df.groupby('code').apply(natr, n=n).reset_index(level=0, drop=True)\n",
    "\n",
    "    # æ¬¡æ—¥æ­¢ç›ˆç‰¹å¾\n",
    "    df['aft_high1'] = df.groupby('code')['high'].shift(-1)\n",
    "    df['aft_high_cur_close'] = (df['aft_high1'] - df['close']) / df['close']\n",
    "\n",
    "    # æ¢æ‰‹ç‡ç™¾åˆ†ä½ã€æµé€šå¸‚å€¼\n",
    "    df['turnover_pct'] = df.groupby('trade_date')['turnover'].rank(pct=True)\n",
    "    df['cap_float_share_rate'] = df['remain_cap'] * 10000 / (df['float_share'] * df['close_stk'])\n",
    "\n",
    "    # åŒºé—´æ”¶ç›Šç‡\n",
    "    for win in [5, 20]:\n",
    "        df['tmp'] = df['pct_chg'] + 1\n",
    "        df[f'pct_chg_{win}'] = df.groupby('code')['tmp'].rolling(win, min_periods=1).apply(np.prod, raw=True).reset_index(level=0, drop=True) - 1\n",
    "        del df['tmp']\n",
    "        df['tmp2'] = df['pct_chg_stk'] + 1\n",
    "        df[f'pct_chg_stk_{win}'] = df.groupby('code')['tmp2'].rolling(win, min_periods=1).apply(np.prod, raw=True).reset_index(level=0, drop=True) - 1\n",
    "        del df['tmp2']\n",
    "\n",
    "    # å‡å€¼æ¢æ‰‹ç‡\n",
    "    for win in [5, 10, 20, 60]:\n",
    "        df[f'turnover_{win}_avg'] = df.groupby('code')['turnover'].rolling(window=win).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "    # åˆ†ä½æ¢æ‰‹ç‡\n",
    "    for win in [1, 5, 20, 50]:\n",
    "        df[f'rolling_{win}_avg'] = df.groupby('code')['turnover_pct'].rolling(window=win).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "    df['rolling_1_to_5_avg'] = df['rolling_1_avg'] / df['rolling_5_avg']\n",
    "    df['rolling_5_to_20_avg'] = df['rolling_5_avg'] / df['rolling_20_avg']\n",
    "    df['rolling_20_to_50_avg'] = df['rolling_20_avg'] / df['rolling_50_avg']\n",
    "\n",
    "    # è‡ªåŠ¨ç”Ÿæˆå¤šç»„ N:M é‡èƒ½æ¯”\n",
    "    vol_windows = [3, 5, 10, 20, 30, 60]\n",
    "    for n in vol_windows:\n",
    "        df[f'vol_{n}_avg'] = df.groupby('code')['vol'].rolling(n).mean().reset_index(level=0, drop=True)\n",
    "    for n in vol_windows:\n",
    "        for m in vol_windows:\n",
    "            if n < m:\n",
    "                df[f'vol_{n}_to_{m}'] = df[f'vol_{n}_avg'] / df[f'vol_{m}_avg']\n",
    "\n",
    "    # æ³¢åŠ¨ç‡\n",
    "    for win in [5, 10, 20, 60]:\n",
    "        df[f'bodong_{win}'] = df.groupby('code')['pct_chg_stk'].rolling(win).std().reset_index(level=0, drop=True) * (win ** 0.5)\n",
    "        df[f'bodong_{win}_bd'] = df.groupby('code')['pct_chg'].rolling(win).std().reset_index(level=0, drop=True) * (win ** 0.5)\n",
    "\n",
    "    df['bodong_20_to_bodong_60'] = df['bodong_20'] / df['bodong_60']\n",
    "\n",
    "    # æŒ¯å¹…æ³¢åŠ¨\n",
    "    for win in [1, 5, 10, 20, 60]:\n",
    "        df[f'zhengfu_{win}'] = df.groupby('code')['zhengfu'].rolling(win).std().reset_index(level=0, drop=True)\n",
    "        df[f'zhengfu_{win}_bodong'] = df[f'zhengfu_{win}'] * (win ** 0.5)\n",
    "\n",
    "    # # è·³ç©ºä¸Šæ¶¨/æš´è·Œç»Ÿè®¡\n",
    "    # df['high_jump'] = (df['high'] / df['pre_close'] - 1) > 0.025\n",
    "    # df['close_drop'] = (df['close'] / df['pre_close'] - 1) < -0.02\n",
    "    # for win in [100, 250]:\n",
    "    #     df[f'high_jump_count_{win}'] = df.groupby('code')['high_jump'].rolling(window=win, min_periods=1).sum().reset_index(0, drop=True)\n",
    "    #     df[f'close_drop_count_{win}'] = df.groupby('code')['close_drop'].rolling(window=win, min_periods=1).sum().reset_index(0, drop=True)\n",
    "    #     df[f'high_jump_count_{win}_pct'] = df.groupby('trade_date')[f'high_jump_count_{win}'].rank(pct=True)\n",
    "    #     df.loc[df[f'high_jump_count_{win}_pct'] < 0.1, 'filter'] = True\n",
    "\n",
    "#     # =========================\n",
    "# # åŸæœ‰ä»£ç ä¸­çš„æŒ‡æ ‡è®¡ç®—ï¼ˆä¿ç•™ï¼‰\n",
    "# # =========================\n",
    "# # ... ä½ çš„åŸæœ‰æŒ‡æ ‡è®¡ç®—é€»è¾‘ä¿ç•™ä¸å˜ ...\n",
    "\n",
    "# # =========================\n",
    "# # å¢å¼ºè·³ç©ºä¸Kçº¿ç»“æ„æŒ‡æ ‡ï¼ˆæ–°å¢ï¼‰\n",
    "# # =========================\n",
    "#     df['high_jump'] = (df['high'] / df['pre_close'] - 1) > 0.025\n",
    "#     df['low_gap'] = (df['low'] / df['pre_close'] - 1) < -0.025\n",
    "#     df['open_jump'] = (df['open'] / df['pre_close'] - 1).abs()\n",
    "#     df['gap_body_ratio'] = (df['open'] - df['pre_close']) / (df['close'] - df['open']).replace(0, np.nan)\n",
    "\n",
    "#     for win in [20, 100, 250]:\n",
    "#         df[f'high_jump_count_{win}'] = df.groupby('code')['high_jump'].rolling(window=win, min_periods=1).sum().reset_index(0, drop=True)\n",
    "#         df[f'low_gap_count_{win}'] = df.groupby('code')['low_gap'].rolling(window=win, min_periods=1).sum().reset_index(0, drop=True)\n",
    "#         df[f'high_jump_count_{win}_pct'] = df.groupby('trade_date')[f'high_jump_count_{win}'].rank(pct=True)\n",
    "#         df[f'low_gap_count_{win}_pct'] = df.groupby('trade_date')[f'low_gap_count_{win}'].rank(pct=True)\n",
    "#         df.loc[df[f'high_jump_count_{win}_pct'] < 0.1, 'filter'] = True\n",
    "\n",
    "#     # æ”¶ç›˜ä»·ä¸é«˜ä½ç‚¹å…³ç³»\n",
    "#     df['close_to_high_ratio'] = (df['close'] - df['low']) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "#     df['close_to_low_ratio'] = (df['high'] - df['close']) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "#     df['body_position'] = (df['close'] - df['open']) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "#     df['upper_shadow_ratio'] = (df['high'] - df[['close', 'open']].max(axis=1)) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "#     df['lower_shadow_ratio'] = (df[['close', 'open']].min(axis=1) - df['low']) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "\n",
    "# # =========================\n",
    "# # ç¤ºä¾‹ç­–ç•¥ï¼ˆæ”»å‡»å‹å½¢æ€é€‰è‚¡é…ç½®ï¼‰\n",
    "# # =========================\n",
    "# # æ”»å‡»å½¢æ€ç‰¹å¾ï¼š\n",
    "# # - é«˜è·³ç©ºæ¦‚ç‡ + æ”¶ç›˜æ¥è¿‘æœ€é«˜\n",
    "# # - Kçº¿é˜³çº¿ã€å®ä½“å¤§ã€ä¸Šå½±çº¿çŸ­\n",
    "# # å¯ä½œä¸ºæ‰“åˆ†æˆ–è¿‡æ»¤æ¡ä»¶ï¼š\n",
    "# # config = {\n",
    "# #     'score_factors': {\n",
    "# #         'high_jump_count_20': 'desc',\n",
    "# #         'close_to_high_ratio': 'desc',\n",
    "# #         'upper_shadow_ratio': 'asc',\n",
    "# #         'body_position': 'desc'\n",
    "# #     }\n",
    "# # }\n",
    "\n",
    "#     # =========================\n",
    "# # Alpha101 & è”åŠ¨å› å­æ¨¡å—ï¼ˆæ‰©å±•+æ³¨é‡Šï¼‰\n",
    "# # =========================\n",
    "#     # Alpha6: -corr(rank(delta(close, 10)), rank(vol), 10)\n",
    "#     df['delta_close_10'] = df.groupby('code')['close'].diff(10)\n",
    "#     df['rank_delta_close_10'] = df.groupby('trade_date')['delta_close_10'].rank()\n",
    "#     df['rank_vol'] = df.groupby('trade_date')['vol'].rank()\n",
    "#     df['alpha6'] = df.groupby('code').apply(lambda x: x['rank_delta_close_10'].rolling(10).corr(x['rank_vol'])).reset_index(level=0, drop=True) * -1\n",
    "\n",
    "#     # Alpha12: sign(delta(vol, 1)) * -1 * delta(close, 1)\n",
    "#     df['delta_vol_1'] = df.groupby('code')['vol'].diff(1)\n",
    "#     df['delta_close_1'] = df.groupby('code')['close'].diff(1)\n",
    "#     df['alpha12'] = np.sign(df['delta_vol_1']) * -1 * df['delta_close_1']\n",
    "\n",
    "#     # Alpha83: rank(ts_argmax(close, 30)) â†’ 30æ—¥å†…æ”¶ç›˜æœ€é«˜ä»·çš„ä½ç½®\n",
    "#     df['alpha83'] = df.groupby('code')['close'].rolling(30).apply(lambda x: 29 - np.argmax(x[::-1]), raw=True).reset_index(level=0, drop=True)\n",
    "\n",
    "#     # è”åŠ¨å› å­ï¼šè‚¡ç¥¨æ¶¨ã€å¯è½¬å€ºæ²¡æ¶¨\n",
    "#     df['stk_up_bond_flat'] = ((df['pct_chg_stk'] > 0.03) & (df['pct_chg'] < 0.01)).astype(int)\n",
    "#     df['stk_down_bond_weak'] = ((df['pct_chg_stk'] < -0.03) & (df['pct_chg'] < df['pct_chg_stk'])).astype(int)\n",
    "#     df['bond_hold_stk_rebound'] = ((df['pct_chg_stk'].shift(1) < -0.03) & (df['pct_chg_stk'] > 0.01) & (df['pct_chg'] > 0.005)).astype(int)\n",
    "\n",
    "#     # ä¿å­˜åè¿”å›\n",
    "#     # Alpha6: è¶‹åŠ¿ä¸æˆäº¤é‡ç›¸å…³æ€§ï¼Œåè½¬ä¿¡å·ï¼Œå€¼è¶Šå°å¯èƒ½è¶Šå¼ºåŠ¿ï¼ˆéœ€åšæ»‘åŠ¨ç›¸å…³ï¼‰\n",
    "#     # Alpha6_stk: æ­£è‚¡ç‰ˆæœ¬\n",
    "#     df['delta_close_10_stk'] = df.groupby('code')['close_stk'].diff(10)\n",
    "#     df['rank_delta_close_10_stk'] = df.groupby('trade_date')['delta_close_10_stk'].rank()\n",
    "#     df['rank_vol_stk'] = df.groupby('trade_date')['vol_stk'].rank()\n",
    "#     df['alpha6_stk'] = df.groupby('code').apply(lambda x: x['rank_delta_close_10_stk'].rolling(10).corr(x['rank_vol_stk'])).reset_index(level=0, drop=True) * -1\n",
    "\n",
    "#     # Alpha12: æˆäº¤é‡å˜åŠ¨çš„åå‘åŠ¨é‡ä¿¡å·\n",
    "#     df['delta_vol_1_stk'] = df.groupby('code')['vol_stk'].diff(1)\n",
    "#     df['delta_close_1_stk'] = df.groupby('code')['close_stk'].diff(1)\n",
    "#     df['alpha12_stk'] = np.sign(df['delta_vol_1_stk']) * -1 * df['delta_close_1_stk']\n",
    "\n",
    "#     # Alpha83: è¿‘30æ—¥é«˜ç‚¹å‡ºç°æ—¶é—´ï¼Œæ•°å€¼è¶Šå°è¶Šå¼º\n",
    "#     df['alpha83_stk'] = df.groupby('code')['close_stk'].rolling(30).apply(lambda x: 29 - np.argmax(x[::-1]), raw=True).reset_index(level=0, drop=True)\n",
    "\n",
    "#     # è”åŠ¨å› å­å¢å¼ºï¼šè·¨æ—¥å¤šå‘¨æœŸç‰ˆæœ¬\n",
    "#     df['stk_chg_3'] = df.groupby('code')['pct_chg_stk'].rolling(3).mean().reset_index(level=0, drop=True)\n",
    "#     df['bond_chg_3'] = df.groupby('code')['pct_chg'].rolling(3).mean().reset_index(level=0, drop=True)\n",
    "#     df['stk_chg_5'] = df.groupby('code')['pct_chg_stk'].rolling(5).mean().reset_index(level=0, drop=True)\n",
    "#     df['bond_chg_5'] = df.groupby('code')['pct_chg'].rolling(5).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "#     # æ»æ¶¨å› å­ï¼šè‚¡ç¥¨æ¶¨ï¼Œè½¬å€ºä¸æ¶¨ï¼ˆè¡¥æ¶¨æ½œåŠ›ï¼‰\n",
    "#     df['stk_up_bond_flat_3'] = ((df['stk_chg_3'] > 0.03) & (df['bond_chg_3'] < 0.01)).astype(int)\n",
    "#     df['stk_up_bond_flat_5'] = ((df['stk_chg_5'] > 0.05) & (df['bond_chg_5'] < 0.01)).astype(int)\n",
    "\n",
    "#     # è”åŠ¨åè½¬ï¼šè‚¡ç¥¨å¤§è·Œååå¼¹ï¼Œè½¬å€ºè·Ÿæ¶¨ï¼ˆå¼¹æ€§æœºä¼šï¼‰\n",
    "#     df['stk_down_then_up'] = ((df['pct_chg_stk'].shift(2) < -0.03) & (df['pct_chg_stk'] > 0.02)).astype(int)\n",
    "#     df['bond_rebound'] = (df['pct_chg'] > 0.01).astype(int)\n",
    "#     df['bond_follow_stk_rebound'] = ((df['stk_down_then_up'] == 1) & (df['bond_rebound'] == 1)).astype(int)\n",
    "\n",
    "    # =========================\n",
    "# Alpha101 & è”åŠ¨å› å­æ¨¡å—ï¼ˆå¤§å¹…æ‰©å±• + æ³¨é‡Š + è‚¡ç¥¨/è½¬å€ºè”åŠ¨ï¼‰\n",
    "# =========================\n",
    "    # Alpha6: -corr(rank(delta(close, 10)), rank(vol), 10)\n",
    "    df['delta_close_10'] = df.groupby('code')['close'].diff(10)\n",
    "    df['rank_delta_close_10'] = df.groupby('trade_date')['delta_close_10'].rank()\n",
    "    df['rank_vol'] = df.groupby('trade_date')['vol'].rank()\n",
    "    df['alpha6'] = df.groupby('code').apply(lambda x: x['rank_delta_close_10'].rolling(10).corr(x['rank_vol'])).reset_index(level=0, drop=True) * -1\n",
    "\n",
    "    # Alpha12: sign(delta(vol, 1)) * -1 * delta(close, 1)\n",
    "    df['delta_vol_1'] = df.groupby('code')['vol'].diff(1)\n",
    "    df['delta_close_1'] = df.groupby('code')['close'].diff(1)\n",
    "    df['alpha12'] = np.sign(df['delta_vol_1']) * -1 * df['delta_close_1']\n",
    "\n",
    "    # Alpha83: rank(ts_argmax(close, 30)) â†’ 30æ—¥å†…æ”¶ç›˜æœ€é«˜ä»·çš„ä½ç½®\n",
    "    df['alpha83'] = df.groupby('code')['close'].rolling(30).apply(lambda x: 29 - np.argmax(x[::-1]), raw=True).reset_index(level=0, drop=True)\n",
    "\n",
    "    # è”åŠ¨å› å­ï¼šè‚¡ç¥¨æ¶¨ã€å¯è½¬å€ºæ²¡æ¶¨\n",
    "    df['stk_up_bond_flat'] = ((df['pct_chg_stk'] > 0.03) & (df['pct_chg'] < 0.01)).astype(int)\n",
    "    df['stk_down_bond_weak'] = ((df['pct_chg_stk'] < -0.03) & (df['pct_chg'] < df['pct_chg_stk'])).astype(int)\n",
    "    df['bond_hold_stk_rebound'] = ((df['pct_chg_stk'].shift(1) < -0.03) & (df['pct_chg_stk'] > 0.01) & (df['pct_chg'] > 0.005)).astype(int)\n",
    "\n",
    "    # ä¿å­˜åè¿”å›\n",
    "    # Alpha6: è¶‹åŠ¿ä¸æˆäº¤é‡ç›¸å…³æ€§ï¼Œåè½¬ä¿¡å·ï¼Œå€¼è¶Šå°å¯èƒ½è¶Šå¼ºåŠ¿ï¼ˆéœ€åšæ»‘åŠ¨ç›¸å…³ï¼‰\n",
    "    # Alpha6_stk: æ­£è‚¡ç‰ˆæœ¬\n",
    "    df['delta_close_10_stk'] = df.groupby('code')['close_stk'].diff(10)\n",
    "    df['rank_delta_close_10_stk'] = df.groupby('trade_date')['delta_close_10_stk'].rank()\n",
    "    df['rank_vol_stk'] = df.groupby('trade_date')['vol_stk'].rank()\n",
    "    df['alpha6_stk'] = df.groupby('code').apply(lambda x: x['rank_delta_close_10_stk'].rolling(10).corr(x['rank_vol_stk'])).reset_index(level=0, drop=True) * -1\n",
    "\n",
    "    # Alpha12: æˆäº¤é‡å˜åŠ¨çš„åå‘åŠ¨é‡ä¿¡å·\n",
    "    df['delta_vol_1_stk'] = df.groupby('code')['vol_stk'].diff(1)\n",
    "    df['delta_close_1_stk'] = df.groupby('code')['close_stk'].diff(1)\n",
    "    df['alpha12_stk'] = np.sign(df['delta_vol_1_stk']) * -1 * df['delta_close_1_stk']\n",
    "\n",
    "    # Alpha83: è¿‘30æ—¥é«˜ç‚¹å‡ºç°æ—¶é—´ï¼Œæ•°å€¼è¶Šå°è¶Šå¼º\n",
    "    df['alpha83_stk'] = df.groupby('code')['close_stk'].rolling(30).apply(lambda x: 29 - np.argmax(x[::-1]), raw=True).reset_index(level=0, drop=True)\n",
    "\n",
    "    # è”åŠ¨å› å­å¢å¼ºï¼šè·¨æ—¥å¤šå‘¨æœŸç‰ˆæœ¬ï¼ˆæ¨ªå‘/çºµå‘è”åŠ¨èƒŒç¦»åˆ†æï¼‰\n",
    "    # è‚¡ç¥¨ & è½¬å€ºæ”¶ç›Šæ¨ªå‘å¯¹æ¯”ï¼ˆå¸‚åœºæ»æ¶¨ã€å¼‚åŠ¨è¯†åˆ«ï¼‰\n",
    "    for win in [3, 5, 10]:\n",
    "        df[f'stk_ret_{win}'] = df.groupby('code')['pct_chg_stk'].rolling(win).mean().reset_index(level=0, drop=True)\n",
    "        df[f'bond_ret_{win}'] = df.groupby('code')['pct_chg'].rolling(win).mean().reset_index(level=0, drop=True)\n",
    "        df[f'dev_bond_vs_stk_{win}'] = df[f'bond_ret_{win}'] - df[f'stk_ret_{win}']  # æ¨ªå‘èƒŒç¦»å€¼\n",
    "\n",
    "    # è½¬å€ºè‡ªèº«å†å²åç¦»ï¼ˆçºµå‘ï¼‰ï¼šè¿‘æœŸè¡¨ç° vs é•¿æœŸå‡å€¼\n",
    "    for win_short, win_long in [(3, 20), (5, 30)]:\n",
    "        short = df.groupby('code')['pct_chg'].rolling(win_short).mean().reset_index(level=0, drop=True)\n",
    "        long = df.groupby('code')['pct_chg'].rolling(win_long).mean().reset_index(level=0, drop=True)\n",
    "        df[f'dev_bond_short{win_short}_long{win_long}'] = short - long\n",
    "\n",
    "    # æ­£è‚¡è‡ªèº«å†å²åç¦»ï¼ˆçºµå‘ï¼‰ï¼šæœ€è¿‘å‡ å¤©è¡¨ç° vs è‡ªèº«é•¿æœŸå‡å€¼\n",
    "    for win_short, win_long in [(3, 20), (5, 30)]:\n",
    "        short = df.groupby('code')['pct_chg_stk'].rolling(win_short).mean().reset_index(level=0, drop=True)\n",
    "        long = df.groupby('code')['pct_chg_stk'].rolling(win_long).mean().reset_index(level=0, drop=True)\n",
    "        df[f'dev_stk_short{win_short}_long{win_long}'] = short - long\n",
    "    df['stk_chg_3'] = df.groupby('code')['pct_chg_stk'].rolling(3).mean().reset_index(level=0, drop=True)\n",
    "    df['bond_chg_3'] = df.groupby('code')['pct_chg'].rolling(3).mean().reset_index(level=0, drop=True)\n",
    "    df['stk_chg_5'] = df.groupby('code')['pct_chg_stk'].rolling(5).mean().reset_index(level=0, drop=True)\n",
    "    df['bond_chg_5'] = df.groupby('code')['pct_chg'].rolling(5).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "    # æ»æ¶¨å› å­ï¼šè‚¡ç¥¨æ¶¨ï¼Œè½¬å€ºä¸æ¶¨ï¼ˆè¡¥æ¶¨æ½œåŠ›ï¼‰\n",
    "    df['stk_up_bond_flat_3'] = ((df['stk_chg_3'] > 0.03) & (df['bond_chg_3'] < 0.01)).astype(int)\n",
    "    df['stk_up_bond_flat_5'] = ((df['stk_chg_5'] > 0.05) & (df['bond_chg_5'] < 0.01)).astype(int)\n",
    "\n",
    "    # è”åŠ¨åè½¬ï¼šè‚¡ç¥¨å¤§è·Œååå¼¹ï¼Œè½¬å€ºè·Ÿæ¶¨ï¼ˆå¼¹æ€§æœºä¼šï¼‰\n",
    "    df['stk_down_then_up'] = ((df['pct_chg_stk'].shift(2) < -0.03) & (df['pct_chg_stk'] > 0.02)).astype(int)\n",
    "    df['bond_rebound'] = (df['pct_chg'] > 0.01).astype(int)\n",
    "    df['bond_follow_stk_rebound'] = ((df['stk_down_then_up'] == 1) & (df['bond_rebound'] == 1)).astype(int)\n",
    "\n",
    "    # Alpha101 é™„åŠ å› å­ï¼ˆè½¬å€º & è‚¡ç¥¨åŒç‰ˆæœ¬ï¼‰\n",
    "\n",
    "    # Alpha18: close / rank(mean(close, 20))ï¼ŒåŠ¨é‡å‡å€¼åç¦»ï¼ˆè¶Šå¤§è¶Šå¼ºï¼‰\n",
    "    df['mean_close_20'] = df.groupby('code')['close'].rolling(20).mean().reset_index(level=0, drop=True)\n",
    "    df['rank_mean_close_20'] = df.groupby('trade_date')['mean_close_20'].rank()\n",
    "    df['alpha18'] = df['close'] / df['rank_mean_close_20']\n",
    "\n",
    "    df['mean_close_20_stk'] = df.groupby('code')['close_stk'].rolling(20).mean().reset_index(level=0, drop=True)\n",
    "    df['rank_mean_close_20_stk'] = df.groupby('trade_date')['mean_close_20_stk'].rank()\n",
    "    df['alpha18_stk'] = df['close_stk'] / df['rank_mean_close_20_stk']\n",
    "\n",
    "    # Alpha36: (rank(correlation(vol, close, 5)) + rank(correlation(vol, open, 5)))ï¼Œé‡ä»·ç›¸å…³æ€§\n",
    "    df['alpha36'] = df.groupby('code').apply(\n",
    "        lambda x: x['vol'].rolling(5).corr(x['close']) + x['vol'].rolling(5).corr(x['open'])\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    df['alpha36_stk'] = df.groupby('code').apply(\n",
    "        lambda x: x['vol_stk'].rolling(5).corr(x['close_stk']) + x['vol_stk'].rolling(5).corr(x['open_stk'])\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Alpha89: (rank(ts_argmax(close, 30)) / rank(ts_argmin(close, 30)))ï¼Œåè½¬æ—¶æœº\n",
    "    max_idx = df.groupby('code')['close'].rolling(30).apply(lambda x: 29 - np.argmax(x[::-1]), raw=True).reset_index(level=0, drop=True)\n",
    "    min_idx = df.groupby('code')['close'].rolling(30).apply(lambda x: 29 - np.argmin(x[::-1]), raw=True).reset_index(level=0, drop=True)\n",
    "    df['alpha89'] = max_idx / (min_idx + 1e-9)\n",
    "\n",
    "    max_idx_stk = df.groupby('code')['close_stk'].rolling(30).apply(lambda x: 29 - np.argmax(x[::-1]), raw=True).reset_index(level=0, drop=True)\n",
    "    min_idx_stk = df.groupby('code')['close_stk'].rolling(30).apply(lambda x: 29 - np.argmin(x[::-1]), raw=True).reset_index(level=0, drop=True)\n",
    "    df['alpha89_stk'] = max_idx_stk / (min_idx_stk + 1e-9)\n",
    "\n",
    "    # ä½¿ç”¨è¯´æ˜ï¼ˆæ³¨é‡Šï¼‰ï¼š\n",
    "    # alpha6, alpha12, alpha83: åè½¬ç±»å› å­ï¼ŒåŸºäºä»·æ ¼å˜åŒ–å’Œæˆäº¤é‡æ–¹å‘è¯†åˆ«è¶‹åŠ¿ä¸´ç•Œç‚¹\n",
    "    # alpha18: åŠ¨é‡åç¦»ï¼ˆå¼ºè€…æ’å¼ºï¼‰\n",
    "    # alpha36: é‡ä»·è”åŠ¨æ€§ï¼Œé€‚åˆç¡®è®¤æ”¾é‡è·Ÿæ¶¨æˆ–ç¼©é‡æ»æ¶¨\n",
    "    # alpha89: æç«¯èµ°åŠ¿æ—¶æœºåˆ¤æ–­ï¼ˆå¦‚ä½ç‚¹åè½¬ï¼‰\n",
    "    # \n",
    "    # é€‚ç”¨äºç»„åˆï¼šå¦‚â€œä»·æ ¼æ»æ¶¨ + æ­£è‚¡æŒç»­å¼ºåŠ¿ + OBVä¸Šå‡â€ç­›é€‰è¡¥æ¶¨å¯è½¬å€º\n",
    "    # åç»­å»ºè®®ä½¿ç”¨ evaluate_factors(df, groupby='è½¬è‚¡æº¢ä»·') åˆ†ææ¯ç»„æœ‰æ•ˆæ€§\n",
    "\n",
    "    # Alpha65: correlation(rank(close), rank(vol), 6) â†’ è¶‹åŠ¿ä¼´éšæ”¾é‡ï¼ˆæ­£ç›¸å…³ä¸ºå¼ºï¼‰\n",
    "    df['alpha65'] = df.groupby('code').apply(\n",
    "        lambda x: x['close'].rank().rolling(6).corr(x['vol'].rank())\n",
    "    ).reset_index(level=0, drop=True)\n",
    "    df['alpha65_stk'] = df.groupby('code').apply(\n",
    "        lambda x: x['close_stk'].rank().rolling(6).corr(x['vol_stk'].rank())\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Alpha76: -1 * ts_rank(correlation(close, vol, 10), 10)\n",
    "    df['alpha76'] = df.groupby('code').apply(\n",
    "        lambda x: -1 * x['close'].rolling(10).corr(x['vol']).rolling(10).apply(lambda x: pd.Series(x).rank().iloc[-1])\n",
    "    ).reset_index(level=0, drop=True)\n",
    "    df['alpha76_stk'] = df.groupby('code').apply(\n",
    "        lambda x: -1 * x['close_stk'].rolling(10).corr(x['vol_stk']).rolling(10).apply(lambda x: pd.Series(x).rank().iloc[-1])\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Alpha92: (delta(close, 5)/close) * vol â†’ å›è°ƒå¹…åº¦ä¸é‡èƒ½ç»“åˆåˆ¤æ–­æ´—ç›˜/åè½¬\n",
    "    df['alpha92'] = df.groupby('code').apply(\n",
    "        lambda x: (x['close'].diff(5) / x['close']) * x['vol']\n",
    "    ).reset_index(level=0, drop=True)\n",
    "    df['alpha92_stk'] = df.groupby('code').apply(\n",
    "        lambda x: (x['close_stk'].diff(5) / x['close_stk']) * x['vol_stk']\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Alpha99: -1 * ts_rank(cov(rank(close), rank(vol), 5), 5)\n",
    "    df['alpha99'] = df.groupby('code').apply(\n",
    "        lambda x: -1 * x['close'].rank().rolling(5).cov(x['vol'].rank()).rolling(5).apply(lambda x: pd.Series(x).rank().iloc[-1])\n",
    "    ).reset_index(level=0, drop=True)\n",
    "    df['alpha99_stk'] = df.groupby('code').apply(\n",
    "        lambda x: -1 * x['close_stk'].rank().rolling(5).cov(x['vol_stk'].rank()).rolling(5).apply(lambda x: pd.Series(x).rank().iloc[-1])\n",
    "    ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # =========================\n",
    "    # ğŸ“‰ å¯è½¬å€º-è‚¡ç¥¨åè½¬ä¸å›æ’¤é£é™©å› å­\n",
    "    # =========================\n",
    "\n",
    "    # 1. å¯è½¬å€º vs è‚¡ç¥¨ æ”¶ç›Šå¼ºå¼±æ¨ªå‘å¯¹æ¯”ï¼ˆç›¸å¯¹å¼ºåº¦ï¼‰\n",
    "    for win in [3, 5, 10]:\n",
    "        df[f'cb_ret_rank_{win}'] = df.groupby('trade_date')['bond_ret_' + str(win)].rank()\n",
    "        df[f'stk_ret_rank_{win}'] = df.groupby('trade_date')['stk_ret_' + str(win)].rank()\n",
    "        df[f'cb_vs_stk_ret_rank_diff_{win}'] = df[f'cb_ret_rank_{win}'] - df[f'stk_ret_rank_{win}']  # è¶Šé«˜è¯´æ˜è½¬å€ºç›¸å¯¹å¼º\n",
    "\n",
    "    # 2. å¯è½¬å€ºè·è¿‘æœŸä½ç‚¹è·ç¦»ï¼ˆæ˜¯å¦å·²è¶…è·Œï¼‰\n",
    "    df['cb_low_5'] = df.groupby('code')['close'].rolling(5).min().reset_index(level=0, drop=True)\n",
    "    df['cb_dev_from_low_5'] = (df['close'] - df['cb_low_5']) / df['cb_low_5']  # è¶Šå¤§è¯´æ˜å·²åå¼¹\n",
    "\n",
    "    # 3. å¯è½¬å€ºä»·æ ¼æ³¢åŠ¨æ€§ï¼ˆé£é™©è¯†åˆ«ï¼‰\n",
    "    df['cb_close_std_5'] = df.groupby('code')['close'].rolling(5).std().reset_index(level=0, drop=True)\n",
    "\n",
    "    # 4. å¯è½¬å€ºè¿‘5æ—¥æœ€å¤§å›æ’¤ï¼ˆæœ€é«˜ç‚¹åˆ°å½“å‰ï¼‰\n",
    "    df['cb_high_5'] = df.groupby('code')['close'].rolling(5).max().reset_index(level=0, drop=True)\n",
    "    df['cb_drawdown_5'] = (df['close'] - df['cb_high_5']) / df['cb_high_5']  # è¶Šè´Ÿè¯´æ˜é£é™©é‡Šæ”¾\n",
    "\n",
    "    # 5. ä¸‹è·Œé£é™©é¢„ä¼°ï¼ˆå†å²æ¦‚ç‡ Ã— å¹…åº¦ï¼‰\n",
    "    df['cb_ret_1'] = df.groupby('code')['pct_chg'].shift(1)\n",
    "    df['cb_fall_flag'] = (df['cb_ret_1'] < 0).astype(int)\n",
    "    df['cb_fall_freq_10'] = df.groupby('code')['cb_fall_flag'].rolling(10).mean().reset_index(level=0, drop=True)\n",
    "    df['cb_fall_amp_10'] = df.groupby('code')['cb_ret_1'].rolling(10).apply(lambda x: x[x < 0].mean() if (x < 0).any() else 0).reset_index(level=0, drop=True)\n",
    "    df['cb_dd_prob_estimate'] = df['cb_fall_freq_10'] * df['cb_fall_amp_10']  # è¶Šè´Ÿé£é™©è¶Šå¤§\n",
    "\n",
    "    df['high_jump'] = (df['high'] / df['pre_close'] - 1) > 0.025\n",
    "    df['low_gap'] = (df['low'] / df['pre_close'] - 1) < -0.025\n",
    "    df['open_jump'] = (df['open'] / df['pre_close'] - 1).abs()\n",
    "    df['gap_body_ratio'] = (df['open'] - df['pre_close']) / (df['close'] - df['open']).replace(0, np.nan)\n",
    "\n",
    "    for win in [20, 100, 250]:\n",
    "        df[f'high_jump_count_{win}'] = df.groupby('code')['high_jump'].rolling(window=win, min_periods=1).sum().reset_index(0, drop=True)\n",
    "        df[f'low_gap_count_{win}'] = df.groupby('code')['low_gap'].rolling(window=win, min_periods=1).sum().reset_index(0, drop=True)\n",
    "        df[f'high_jump_count_{win}_pct'] = df.groupby('trade_date')[f'high_jump_count_{win}'].rank(pct=True)\n",
    "        df[f'low_gap_count_{win}_pct'] = df.groupby('trade_date')[f'low_gap_count_{win}'].rank(pct=True)\n",
    "        df.loc[df[f'high_jump_count_{win}_pct'] < 0.1, 'filter'] = True\n",
    "\n",
    "    # æ”¶ç›˜ä»·ä¸é«˜ä½ç‚¹å…³ç³»\n",
    "    df['close_to_high_ratio'] = (df['close'] - df['low']) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "    df['close_to_low_ratio'] = (df['high'] - df['close']) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "    df['body_position'] = (df['close'] - df['open']) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "    df['upper_shadow_ratio'] = (df['high'] - df[['close', 'open']].max(axis=1)) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "    df['lower_shadow_ratio'] = (df[['close', 'open']].min(axis=1) - df['low']) / (df['high'] - df['low']).replace(0, np.nan)\n",
    "\n",
    "\n",
    "\n",
    "    # æ–°å¢éƒ¨åˆ†ï¼šæ¶¨ä¸åŠ¨ + è·Œä¸åŠ¨ + è„‰å†²å¯èƒ½æ€§å› å­ç»„åˆï¼ˆå«è¡¥å……å› å­ï¼‰\n",
    "# =========================\n",
    "\n",
    "# 1. æ¶¨ä¸åŠ¨ & è·Œä¸åŠ¨ï¼ˆéœ‡è¡æ”¶æ•›ç±»ï¼‰\n",
    "# -----------------------------------\n",
    "# ATR è¡°å‡ç‡ï¼ˆéœ‡è¡å¹…åº¦å˜çª„ï¼‰\n",
    "    df['atr_5'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(5).mean()).reset_index(0, drop=True)\n",
    "    df['atr_20'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(20).mean()).reset_index(0, drop=True)\n",
    "    df['atr_5_decay'] = df['atr_5'] / df['atr_20']\n",
    "\n",
    "# æŒ¯å¹…è¡°å‡ï¼ˆé«˜ä½ä»·å·®ç¼©å°ï¼‰\n",
    "    df['zhengfu_5'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(5).mean()).reset_index(0, drop=True)\n",
    "    df['zhengfu_20'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(20).mean()).reset_index(0, drop=True)\n",
    "    df['zhengfu_decay_5_20'] = df['zhengfu_5'] / df['zhengfu_20']\n",
    "\n",
    "# é«˜ä½ä»·å·®æ¯”å‡å€¼\n",
    "    range_5 = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(5).mean()).reset_index(0, drop=True)\n",
    "    range_20 = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(20).mean()).reset_index(0, drop=True)\n",
    "    df['range_ratio_5_20'] = range_5 / range_20\n",
    "\n",
    "# æå°å®ä½“ + é•¿å½±çº¿ç»“æ„\n",
    "    body = (df['close'] - df['open']).abs()\n",
    "    shadow = (df['high'] - df['low']) - body\n",
    "    df['small_body_shadow_ratio'] = shadow / (body + 1e-6)\n",
    "\n",
    "# åå­—æ˜Ÿå‡ºç°é¢‘ç‡\n",
    "    is_doji = (body / (df['high'] - df['low'] + 1e-6)) < 0.15\n",
    "    df['doji_ratio_5'] = is_doji.groupby(df['code']).rolling(5).mean().reset_index(0, drop=True)\n",
    "\n",
    "# 2. è„‰å†²æ¦‚ç‡ Ã— è„‰å†²å¹…åº¦ Ã— æƒ…ç»ªæ³¢åŠ¨\n",
    "# -----------------------------------\n",
    "# high_jump å¤šæ¡£ä½\n",
    "    for thres in [0.015, 0.02, 0.03, 0.04, 0.05, 0.06]:\n",
    "        df[f'high_jump_{int(thres*1000)}'] = ((df['high'] / df['pre_close'] - 1) > thres).astype(int)\n",
    "        df[f'count_high_jump_{int(thres*1000)}_20'] = df.groupby('code')[f'high_jump_{int(thres*1000)}'].rolling(20).sum().reset_index(0, drop=True)\n",
    "        df[f'mean_high_jump_{int(thres*1000)}_20'] = df.groupby('code')['pct_chg'].where(df[f'high_jump_{int(thres*1000)}'] == 1).rolling(20).mean().reset_index(0, drop=True)\n",
    "        df[f'score_high_jump_{int(thres*1000)}_20'] = df[f'count_high_jump_{int(thres*1000)}_20'] * df[f'mean_high_jump_{int(thres*1000)}_20']\n",
    "\n",
    "# è„‰å†² Z-scoreï¼ˆæ¶¨å¹…å¼‚å¸¸æ€§ï¼‰\n",
    "    df['zscore_pctchg_20'] = df.groupby('code')['pct_chg'].transform(lambda x: (x - x.rolling(20).mean()) / (x.rolling(20).std() + 1e-6))\n",
    "\n",
    "# æˆäº¤é‡è„‰å†²\n",
    "    vol_ma20 = df.groupby('code')['vol'].rolling(20).mean().reset_index(0, drop=True)\n",
    "    df['vol_spike_ratio'] = df['vol'] / (vol_ma20 + 1e-6)\n",
    "\n",
    "# æˆäº¤é‡æ³¢åŠ¨æ”¶æ•›\n",
    "    vol_std_5 = df.groupby('code')['vol'].rolling(5).std().reset_index(0, drop=True)\n",
    "    vol_std_20 = df.groupby('code')['vol'].rolling(20).std().reset_index(0, drop=True)\n",
    "    df['vol_std_decay'] = vol_std_5 / (vol_std_20 + 1e-6)\n",
    "\n",
    "# 3. è„‰å†²æŒç»­æ€§ï¼ˆè·³ç©º+é˜³çº¿ï¼‰\n",
    "# -----------------------------------\n",
    "    df['gap_and_go_flag'] = ((df['open'] > df['pre_close'] * 1.02) & (df['close'] > df['open'])).astype(int)\n",
    "    df['gap_body_ratio'] = (df['open'] - df['pre_close']) / (df['close'] - df['open']).replace(0, np.nan)\n",
    "    df.to_parquet('/Users/yiwei/Desktop/git/cb_data_with_factors.pq')\n",
    "\n",
    "\n",
    "\n",
    "    # æ–°å¢éƒ¨åˆ†ï¼šæ¶¨ä¸åŠ¨ + è·Œä¸åŠ¨ + è„‰å†²å¯èƒ½æ€§å› å­ç»„åˆ\n",
    "# =========================\n",
    "\n",
    "# 1. æ¶¨ä¸åŠ¨ & è·Œä¸åŠ¨ï¼ˆéœ‡è¡æ”¶æ•›ç±»ï¼‰\n",
    "# -----------------------------------\n",
    "# ATR è¡°å‡ç‡ï¼ˆéœ‡è¡å¹…åº¦å˜çª„ï¼‰\n",
    "    df['atr_5'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(5).mean()).reset_index(0, drop=True)\n",
    "    df['atr_10'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(10).mean()).reset_index(0, drop=True)\n",
    "    df['atr_decay_5_10'] = df['atr_5'] / df['atr_10']  # è¶‹è¿‘ 1 ä¸ºéœ‡è¡ï¼Œè¿œå°äº 1 ä¸ºæ”¶æ•›\n",
    "\n",
    "# æ”¶ç›˜ä»·æ³¢åŠ¨ç‡ç¼©å°ï¼ˆæ ‡å‡†å·®ä¸‹é™ï¼‰\n",
    "    df['close_std_5'] = df.groupby('code')['close'].rolling(5).std().reset_index(0, drop=True)\n",
    "    df['close_std_10'] = df.groupby('code')['close'].rolling(10).std().reset_index(0, drop=True)\n",
    "    df['vol_shrink_ratio'] = df['close_std_5'] / df['close_std_10']  # å°äº1è¯´æ˜éœ‡è¡æ”¶æ•›\n",
    "\n",
    "# Kçº¿å®ä½“å˜çŸ­ï¼ˆç»å¯¹æ¶¨è·Œå¹…å˜å°ï¼‰\n",
    "    df['body_pct'] = (df['close'] - df['open']).abs() / df['pre_close']\n",
    "    df['body_pct_mean_5'] = df.groupby('code')['body_pct'].rolling(5).mean().reset_index(0, drop=True)\n",
    "\n",
    "# ä¸Šä¸‹å½±çº¿å¢å¤šï¼ˆéœ‡è¡ç‰¹å¾ï¼‰\n",
    "    df['shadow_ratio'] = ((df['high'] - df['low']) - (df['close'] - df['open']).abs()) / df['pre_close']\n",
    "    df['shadow_mean_5'] = df.groupby('code')['shadow_ratio'].rolling(5).mean().reset_index(0, drop=True)\n",
    "\n",
    "# 2. è„‰å†²æ¦‚ç‡ Ã— è„‰å†²å¹…åº¦\n",
    "# -----------------------------------\n",
    "# high_jump å¤šé˜ˆå€¼\n",
    "    for thres in [0.015, 0.02, 0.03, 0.04, 0.05, 0.06]:\n",
    "        df[f'high_jump_{int(thres*1000)}'] = ((df['high'] / df['pre_close'] - 1) > thres).astype(int)\n",
    "\n",
    "# è·³ç©ºå¹…åº¦ï¼ˆå¸¦æ–¹å‘ï¼‰ç»Ÿè®¡\n",
    "    for n in [5, 10]:\n",
    "        df[f'open_gap_mean_{n}'] = df.groupby('code')['open_jump'].rolling(n).mean().reset_index(0, drop=True)\n",
    "        df[f'open_gap_max_{n}'] = df.groupby('code')['open_jump'].rolling(n).max().reset_index(0, drop=True)\n",
    "\n",
    "# N æ—¥è„‰å†² ATRï¼šé«˜ç‚¹è¿œç¦»å‡å€¼ï¼ˆçŸ­æœŸå‰§çƒˆæ‹‰å‡ï¼‰\n",
    "    for n in [3, 5, 10]:\n",
    "        high_mean = df.groupby('code')['high'].rolling(n).mean().reset_index(0, drop=True)\n",
    "        close_mean = df.groupby('code')['close'].rolling(n).mean().reset_index(0, drop=True)\n",
    "        df[f'jump_atr_{n}'] = (df['high'] - close_mean) / (df.groupby('code')['close'].rolling(n).std().reset_index(0, drop=True) + 1e-6)\n",
    "\n",
    "# 3. è·Œä¸åŠ¨ï¼ˆä¸‹è·Œæ¦‚ç‡ä½ + å¹…åº¦å°ï¼‰\n",
    "# -----------------------------------\n",
    "    for win in [5, 10]:\n",
    "        df[f'down_freq_{win}'] = df.groupby('code')['pct_chg'].apply(lambda x: x.rolling(win).apply(lambda s: (s < 0).mean())).reset_index(0, drop=True)\n",
    "        df[f'down_amp_{win}'] = df.groupby('code')['pct_chg'].apply(lambda x: x.rolling(win).apply(lambda s: s[s < 0].mean() if (s < 0).any() else 0)).reset_index(0, drop=True)\n",
    "        df[f'no_fall_score_{win}'] = (1 - df[f'down_freq_{win}']) * (-df[f'down_amp_{win}'])  # è¶Šå¤§è¶Šâ€œè·Œä¸åŠ¨â€\n",
    "\n",
    "# 4. é«˜è„‰å†²åŠ¨èƒ½ï¼ˆhigh_jump / atr / shadow ç­‰é›†ä¸­çˆ†å‘ï¼‰\n",
    "# -----------------------------------\n",
    "    df['range_today'] = df['high'] - df['low']\n",
    "    df['range_atr_5'] = df['range_today'] / df.groupby('code')['range_today'].rolling(5).mean().reset_index(0, drop=True)\n",
    "    df['range_jump_potential'] = (df['range_atr_5'] > 1.5).astype(int)\n",
    "\n",
    "# 5. Kçº¿ç»“æ„è¿ç»­æ€§åˆ¤æ–­ï¼ˆé˜´é˜³äº¤é”™ã€è·³ç©ºæ¥åŠ›ï¼‰\n",
    "# -----------------------------------\n",
    "    df['kline_direction'] = np.sign(df['close'] - df['open'])\n",
    "    df['kline_direction_shift1'] = df.groupby('code')['kline_direction'].shift(1)\n",
    "    df['kline_flip'] = (df['kline_direction'] * df['kline_direction_shift1'] < 0).astype(int)\n",
    "    df['kline_flip_ratio_5'] = df.groupby('code')['kline_flip'].rolling(5).mean().reset_index(0, drop=True)  # å¤šä¸º 0 åˆ™è¶‹åŠ¿ç¨³å®š\n",
    "\n",
    "# =========================\n",
    "# æ‰€æœ‰æ–°å¢å­—æ®µåœ¨åç»­å¯ç»„åˆä½¿ç”¨ï¼šå¦‚ï¼ˆæ¶¨ä¸åŠ¨ + è·Œä¸åŠ¨ + low_gap_count å°ï¼‰è¯†åˆ«è“„åŠ›éœ‡è¡ï¼›æˆ–ï¼ˆè„‰å†²æ¦‚ç‡é«˜ Ã— æœ€è¿‘æ”¶æ•›ï¼‰è¯†åˆ«çˆ†å‘è¡Œæƒ…å‰å…†\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (safe_division, rolling_downside_stats, rolling_high_jump_stats)\n",
    "# ... (keep the helper functions as they were) ...\n",
    "def safe_division(numerator, denominator, default=np.nan):\n",
    "    \"\"\"Performs division, returning default value if denominator is zero or NaN.\"\"\"\n",
    "    denominator = denominator.replace(0, np.nan)\n",
    "    result = numerator / denominator\n",
    "    return result.fillna(default)\n",
    "\n",
    "def rolling_downside_stats(series, window):\n",
    "    \"\"\"Calculates downside frequency, mean amplitude, and std amplitude.\"\"\"\n",
    "    is_down = series < 0\n",
    "    freq = is_down.rolling(window, min_periods=max(1, int(window * 0.6))).mean().fillna(0)\n",
    "    down_series = series.where(is_down)\n",
    "    mean_amp = down_series.rolling(window, min_periods=max(1, int(window * 0.6))).mean().fillna(0)\n",
    "    std_amp = down_series.rolling(window, min_periods=max(2, int(window * 0.6))).std().fillna(0)\n",
    "    return freq, mean_amp, std_amp\n",
    "\n",
    "def rolling_high_jump_stats(jump_flag, pct_chg, window):\n",
    "    \"\"\"Calculates high jump count, mean jump return, and std jump return.\"\"\"\n",
    "    count = jump_flag.rolling(window, min_periods=max(1, int(window * 0.6))).sum().fillna(0)\n",
    "    jump_returns = pct_chg.where(jump_flag)\n",
    "    mean_ret = jump_returns.rolling(window, min_periods=max(1, int(window * 0.6))).mean().fillna(0)\n",
    "    std_ret = jump_returns.rolling(window, min_periods=max(2, int(window * 0.6))).std().fillna(0)\n",
    "    return count, mean_ret, std_ret\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def calculate_factors(df, restore_multiindex=False): # Added option to restore index\n",
    "    \"\"\"\n",
    "    Calculates convertible bond and corresponding stock factors based on the checklist.\n",
    "    Handles DataFrame with 'code' and 'trade_date' as columns OR MultiIndex levels.\n",
    "    Excludes rank, percentage rank (pct=True), and explicit 'score_' factors.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        restore_multiindex (bool): If True, sets ['code', 'trade_date'] back as index at the end.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added factor columns.\n",
    "    \"\"\"\n",
    "    print(\"Starting factor calculation...\")\n",
    "\n",
    "    # --- Input Validation and Index Handling ---\n",
    "    has_code_col = 'code' in df.columns\n",
    "    has_date_col = 'trade_date' in df.columns\n",
    "    has_code_idx = 'code' in df.index.names\n",
    "    has_date_idx = 'trade_date' in df.index.names\n",
    "\n",
    "    if has_code_col and has_date_col:\n",
    "        print(\"Found 'code' and 'trade_date' in columns.\")\n",
    "        # Sort directly if columns exist\n",
    "        df = df.sort_values(by=['code', 'trade_date']).copy() # Use copy to avoid SettingWithCopyWarning later\n",
    "    elif has_code_idx and has_date_idx:\n",
    "        print(\"Found 'code' and 'trade_date' in MultiIndex. Resetting index.\")\n",
    "        df = df.reset_index()\n",
    "        # Now sort by the newly created columns\n",
    "        df = df.sort_values(by=['code', 'trade_date']).copy() # Use copy\n",
    "    else:\n",
    "        missing = []\n",
    "        if not (has_code_col or has_code_idx):\n",
    "            missing.append('code')\n",
    "        if not (has_date_col or has_date_idx):\n",
    "            missing.append('trade_date')\n",
    "        raise ValueError(f\"DataFrame must contain 'code' and 'trade_date' either as columns or index levels. Missing: {missing}\")\n",
    "    # --- End Index Handling ---\n",
    "\n",
    "\n",
    "    # 0. Data Type Preparation & Safety\n",
    "    base_cols = ['high', 'low', 'close', 'open', 'vol', 'pre_close', 'pct_chg', 'turnover', 'remain_cap', 'float_share']\n",
    "    stk_cols = ['high_stk', 'low_stk', 'close_stk', 'open_stk', 'vol_stk', 'pct_chg_stk']\n",
    "    # Include 'code', 'trade_date' now they are guaranteed columns\n",
    "    all_req_cols = base_cols + stk_cols + ['code', 'trade_date']\n",
    "\n",
    "    for col in all_req_cols:\n",
    "        if col in df.columns:\n",
    "            # Don't coerce code/date if they became columns\n",
    "            if col not in ['code', 'trade_date']:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce') # Convert non-numeric to NaN\n",
    "                df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        # else: # Removed redundant check as missing cols are handled by later logic gracefully\n",
    "        #    print(f\"Warning: Column '{col}' not found in DataFrame.\")\n",
    "\n",
    "\n",
    "    # === Factor Calculations Start Here ===\n",
    "    # The rest of the code (Sections I to XVI) remains exactly the same\n",
    "    # as it now operates on a DataFrame where 'code' and 'trade_date'\n",
    "    # are guaranteed to be columns, and the data is sorted.\n",
    "    # ======================================\n",
    "\n",
    "    # === I. Basic Price & Volatility (CB) ===\n",
    "    print(\"Calculating: I. Basic Price & Volatility (CB)\")\n",
    "    df['ma_20'] = df.groupby('code')['close'].transform(lambda x: ta.SMA(x, timeperiod=20))\n",
    "    df['momentum_20'] = df.groupby('code')['close'].transform(lambda x: safe_division(x, x.shift(20)))\n",
    "    df['volatility_20'] = df.groupby('code')['close'].transform(lambda x: x.rolling(20, min_periods=10).std())\n",
    "    df['max_value'] = df.groupby('code')['close'].transform(lambda x: x.cummax().shift(1))\n",
    "    df['max_value_position'] = safe_division(df['close'], df['max_value'])\n",
    "    if 'high' in df.columns and 'low' in df.columns and 'close' in df.columns:\n",
    "        df['zhengfu'] = safe_division(df['high'] - df['low'], df['close'])\n",
    "        df['zhengfu_cha'] = safe_division(df['high'] - df['close'], (df['open'] - df['close']).abs())\n",
    "        # NATR - Needs apply which handles MultiIndex implicitly if we hadn't reset\n",
    "        # Since we reset, groupby('code').apply works fine\n",
    "        df['natr_14'] = df.groupby('code').apply(lambda x: ta.NATR(x['high'], x['low'], x['close'], timeperiod=14) if not x[['high','low','close']].isnull().all().all() else pd.Series(index=x.index, dtype=float)).reset_index(level=0, drop=True)\n",
    "        for n in [1, 3, 5, 10, 20]:\n",
    "             df[f'natr_{n}'] = df.groupby('code').apply(lambda x: ta.NATR(x['high'], x['low'], x['close'], timeperiod=n) if not x[['high','low','close']].isnull().all().all() else pd.Series(index=x.index, dtype=float)).reset_index(level=0, drop=True)\n",
    "    # Future return (Label)\n",
    "    df['aft_high1'] = df.groupby('code')['high'].shift(-1)\n",
    "    df['aft_high_cur_close'] = safe_division(df['aft_high1'] - df['close'], df['close'])\n",
    "\n",
    "\n",
    "    # === II. OBV (CB) ===\n",
    "    print(\"Calculating: II. OBV (CB)\")\n",
    "    if 'close' in df.columns and 'vol' in df.columns:\n",
    "        df['obv'] = df.groupby('code').apply(lambda x: ta.OBV(x['close'], x['vol']) if not x[['close','vol']].isnull().all().all() else pd.Series(index=x.index, dtype=float)).reset_index(level=0, drop=True)\n",
    "        df['obv_5'] = df.groupby('code')['obv'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "        df['obv_10'] = df.groupby('code')['obv'].transform(lambda x: x.rolling(10, min_periods=5).mean())\n",
    "        df['obv_ratio_5_10'] = safe_division(df['obv_5'], df['obv_10'])\n",
    "\n",
    "\n",
    "    # === III. Turnover & Cap ===\n",
    "    print(\"Calculating: III. Turnover & Cap\")\n",
    "    print(\"  - Skipping: turnover_pct, rolling_*_avg, rolling_*_to_*_avg (rank/pct based)\")\n",
    "    if 'turnover' in df.columns:\n",
    "        for win in [5, 10, 20, 60]:\n",
    "            df[f'turnover_{win}_avg'] = df.groupby('code')['turnover'].transform(lambda x: x.rolling(window=win, min_periods=int(win*0.6)).mean())\n",
    "    if all(col in df.columns for col in ['remain_cap', 'float_share', 'close_stk']):\n",
    "        df['cap_float_share_rate'] = safe_division(df['remain_cap'] * 10000, (df['float_share'] * df['close_stk']))\n",
    "\n",
    "\n",
    "    # === IV. Rolling Returns (CB & Stock) ===\n",
    "    print(\"Calculating: IV. Rolling Returns (CB & Stock)\")\n",
    "    if 'pct_chg' in df.columns:\n",
    "        for win in [3, 5, 10, 20]:\n",
    "            df[f'pct_chg_{win}'] = df.groupby('code')['pct_chg'].transform(\n",
    "                lambda x: (x + 1).rolling(win, min_periods=max(1,int(win*0.6))).apply(np.prod, raw=True) - 1\n",
    "            )\n",
    "            df[f'bond_ret_mean_{win}'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(win, min_periods=max(1,int(win*0.6))).mean())\n",
    "\n",
    "    if 'pct_chg_stk' in df.columns:\n",
    "        for win in [3, 5, 10, 20]:\n",
    "            df[f'pct_chg_stk_{win}'] = df.groupby('code')['pct_chg_stk'].transform(\n",
    "                lambda x: (x + 1).rolling(win, min_periods=max(1,int(win*0.6))).apply(np.prod, raw=True) - 1\n",
    "            )\n",
    "            df[f'stk_ret_mean_{win}'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(win, min_periods=max(1,int(win*0.6))).mean())\n",
    "\n",
    "\n",
    "    # === V. Volume Avg Ratio (CB) ===\n",
    "    print(\"Calculating: V. Volume Avg Ratio (CB)\")\n",
    "    if 'vol' in df.columns:\n",
    "        vol_windows = [3, 5, 10, 20, 30, 60]\n",
    "        for n in vol_windows:\n",
    "            df[f'vol_{n}_avg'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(n, min_periods=int(n*0.6)).mean())\n",
    "        for n in vol_windows:\n",
    "            for m in vol_windows:\n",
    "                if n < m and f'vol_{n}_avg' in df.columns and f'vol_{m}_avg' in df.columns:\n",
    "                    df[f'vol_{n}_to_{m}'] = safe_division(df[f'vol_{n}_avg'], df[f'vol_{m}_avg'])\n",
    "\n",
    "\n",
    "    # === VI. Volatility & Amplitude (CB & Stock) ===\n",
    "    print(\"Calculating: VI. Volatility & Amplitude (CB & Stock)\")\n",
    "    if 'pct_chg_stk' in df.columns:\n",
    "        for win in [5, 10, 20, 60]:\n",
    "            df[f'bodong_{win}'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(win, min_periods=int(win*0.6)).std() * (win ** 0.5))\n",
    "        if 'bodong_20' in df.columns and 'bodong_60' in df.columns:\n",
    "            df['bodong_20_to_bodong_60'] = safe_division(df['bodong_20'], df['bodong_60'])\n",
    "\n",
    "    if 'pct_chg' in df.columns:\n",
    "        for win in [5, 10, 20, 60]:\n",
    "             df[f'bodong_{win}_bd'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(win, min_periods=int(win*0.6)).std() * (win ** 0.5))\n",
    "\n",
    "    if 'zhengfu' in df.columns:\n",
    "        for win in [1, 5, 10, 20, 60]:\n",
    "            df[f'zhengfu_{win}'] = df.groupby('code')['zhengfu'].transform(lambda x: x.rolling(win, min_periods=max(1,int(win*0.6))).std())\n",
    "            df[f'zhengfu_{win}_bodong'] = df[f'zhengfu_{win}'] * (win ** 0.5)\n",
    "\n",
    "\n",
    "    # === VII. Jump & Gap (CB) ===\n",
    "    print(\"Calculating: VII. Jump & Gap (CB)\")\n",
    "    print(\"  - Skipping: high_jump_count_*_pct, low_gap_count_*_pct (pct based)\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'open', 'close', 'pre_close']):\n",
    "        # Use a temp name to avoid conflict later if needed\n",
    "        df['_high_jump_flag_temp'] = (safe_division(df['high'], df['pre_close']) - 1) > 0.025\n",
    "        df['_low_gap_flag_temp'] = (safe_division(df['low'], df['pre_close']) - 1) < -0.025\n",
    "        df['open_jump'] = (safe_division(df['open'], df['pre_close']) - 1).abs()\n",
    "        df['gap_body_ratio'] = safe_division(df['open'] - df['pre_close'], (df['close'] - df['open']))\n",
    "\n",
    "        for win in [20, 100, 250]:\n",
    "             df[f'high_jump_count_{win}'] = df.groupby('code')['_high_jump_flag_temp'].transform(lambda x: x.rolling(window=win, min_periods=int(win*0.6)).sum())\n",
    "             df[f'low_gap_count_{win}'] = df.groupby('code')['_low_gap_flag_temp'].transform(lambda x: x.rolling(window=win, min_periods=int(win*0.6)).sum())\n",
    "\n",
    "\n",
    "    # === VIII. K-Line Structure (CB) ===\n",
    "    print(\"Calculating: VIII. K-Line Structure (CB)\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'open', 'close']):\n",
    "        high_low_diff = (df['high'] - df['low']).replace(0, np.nan)\n",
    "        df['close_to_high_ratio'] = safe_division(df['close'] - df['low'], high_low_diff)\n",
    "        df['close_to_low_ratio'] = safe_division(df['high'] - df['close'], high_low_diff)\n",
    "        df['body_position'] = safe_division(df['close'] - df['open'], high_low_diff)\n",
    "        df['upper_shadow_ratio'] = safe_division(df['high'] - df[['close', 'open']].max(axis=1), high_low_diff)\n",
    "        df['lower_shadow_ratio'] = safe_division(df[['close', 'open']].min(axis=1) - df['low'], high_low_diff)\n",
    "\n",
    "\n",
    "    # === IX. Trend Reversal Alpha Factors (CB & Stock) ===\n",
    "    print(\"Calculating: IX. Trend Reversal Alpha Factors (CB & Stock)\")\n",
    "    print(\"  - Skipping: alpha6, alpha18, alpha65, alpha76, alpha99 (rank based)\")\n",
    "    df['delta_vol_1'] = df.groupby('code')['vol'].transform(lambda x: x.diff(1))\n",
    "    df['delta_close_1'] = df.groupby('code')['close'].transform(lambda x: x.diff(1))\n",
    "    df['delta_close_5'] = df.groupby('code')['close'].transform(lambda x: x.diff(5))\n",
    "    if 'vol_stk' in df.columns:\n",
    "        df['delta_vol_1_stk'] = df.groupby('code')['vol_stk'].transform(lambda x: x.diff(1))\n",
    "    if 'close_stk' in df.columns:\n",
    "        df['delta_close_1_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.diff(1))\n",
    "        df['delta_close_5_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.diff(5))\n",
    "\n",
    "    # Alpha12\n",
    "    if all(c in df.columns for c in ['delta_vol_1', 'delta_close_1']):\n",
    "        df['alpha12'] = np.sign(df['delta_vol_1']) * -1 * df['delta_close_1']\n",
    "    if all(c in df.columns for c in ['delta_vol_1_stk', 'delta_close_1_stk']):\n",
    "        df['alpha12_stk'] = np.sign(df['delta_vol_1_stk']) * -1 * df['delta_close_1_stk']\n",
    "\n",
    "    # # Alpha83 (Days since 30d high)\n",
    "    # df['alpha83'] = df.groupby('code')['close'].transform(\n",
    "    #     lambda x: x.rolling(30, min_periods=15).apply(lambda s: 29 - np.argmax(s.to_numpy()[::-1]) if not s.isnull().all() else np.nan, raw=True)\n",
    "    # )\n",
    "    # if 'close_stk' in df.columns:\n",
    "    #     df['alpha83_stk'] = df.groupby('code')['close_stk'].transform(\n",
    "    #         lambda x: x.rolling(30, min_periods=15).apply(lambda s: 29 - np.argmax(s.to_numpy()[::-1]) if not s.isnull().all() else np.nan, raw=True)\n",
    "    #     )\n",
    "\n",
    "    # === IX. Trend Reversal Alpha Factors (CB & Stock) ===\n",
    "    print(\"Calculating: IX. Trend Reversal Alpha Factors (CB & Stock)\")\n",
    "    # ... (other alpha calculations before 83) ...\n",
    "\n",
    "    # Alpha83 (Days since 30d high) - Corrected\n",
    "    print(\"  - Calculating alpha83...\")\n",
    "    df['alpha83'] = df.groupby('code')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=15).apply(\n",
    "            lambda s: (len(s) - 1) - np.nanargmax(s.to_numpy()) if not s.isnull().all() else np.nan,\n",
    "            raw=False # <--- REMOVED raw=True, forces s to be a Series\n",
    "        )\n",
    "    )\n",
    "    # Also corrected the logic: days_ago = window_size - 1 - position_of_max\n",
    "    # np.nanargmax ignores NaNs and finds the position of the first max\n",
    "\n",
    "    if 'close_stk' in df.columns:\n",
    "        print(\"  - Calculating alpha83_stk...\")\n",
    "        df['alpha83_stk'] = df.groupby('code')['close_stk'].transform(\n",
    "            lambda x: x.rolling(30, min_periods=15).apply(\n",
    "                lambda s: (len(s) - 1) - np.nanargmax(s.to_numpy()) if not s.isnull().all() else np.nan,\n",
    "                raw=False # <--- REMOVED raw=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Alpha36 (Volume-Price Correlation)\n",
    "    if all(c in df.columns for c in ['vol', 'close', 'open']):\n",
    "        # transform with rolling corr can be tricky with multiple columns, use apply carefully\n",
    "        def calc_alpha36(x):\n",
    "             corr_close = x['vol'].rolling(5, min_periods=3).corr(x['close'])\n",
    "             corr_open = x['vol'].rolling(5, min_periods=3).corr(x['open'])\n",
    "             return corr_close.add(corr_open, fill_value=0)\n",
    "        df['alpha36'] = df.groupby('code', group_keys=False).apply(calc_alpha36)\n",
    "\n",
    "    if all(c in df.columns for c in ['vol_stk', 'close_stk', 'open_stk']):\n",
    "        def calc_alpha36_stk(x):\n",
    "             corr_close = x['vol_stk'].rolling(5, min_periods=3).corr(x['close_stk'])\n",
    "             corr_open = x['vol_stk'].rolling(5, min_periods=3).corr(x['open_stk'])\n",
    "             return corr_close.add(corr_open, fill_value=0)\n",
    "        df['alpha36_stk'] = df.groupby('code', group_keys=False).apply(calc_alpha36_stk)\n",
    "\n",
    "\n",
    "    # # Alpha89 (High position / Low position)\n",
    "    # df['argmin_close_30_idx'] = df.groupby('code')['close'].transform(\n",
    "    #     lambda x: x.rolling(30, min_periods=15).apply(lambda s: 29 - np.argmin(s.to_numpy()[::-1]) if not s.isnull().all() else np.nan, raw=True)\n",
    "    # )\n",
    "    # if 'alpha83' in df.columns: # Check dependencies\n",
    "    #     df['alpha89'] = safe_division(df['alpha83'], df['argmin_close_30_idx'])\n",
    "\n",
    "    # if 'close_stk' in df.columns:\n",
    "    #     df['argmin_close_30_idx_stk'] = df.groupby('code')['close_stk'].transform(\n",
    "    #          lambda x: x.rolling(30, min_periods=15).apply(lambda s: 29 - np.argmin(s.to_numpy()[::-1]) if not s.isnull().all() else np.nan, raw=True)\n",
    "    #     )\n",
    "    #     if 'alpha83_stk' in df.columns: # Check dependencies\n",
    "    #         df['alpha89_stk'] = safe_division(df['alpha83_stk'], df['argmin_close_30_idx_stk'])\n",
    "\n",
    "    # Alpha89 (High position / Low position) - Also update this to use correct logic\n",
    "    print(\"  - Calculating alpha89 (dependent on corrected alpha83)...\")\n",
    "    df['argmin_close_30_idx_pos'] = df.groupby('code')['close'].transform( # Calculate days since min\n",
    "        lambda x: x.rolling(30, min_periods=15).apply(\n",
    "            lambda s: (len(s) - 1) - np.nanargmin(s.to_numpy()) if not s.isnull().all() else np.nan,\n",
    "            raw=False\n",
    "        )\n",
    "    )\n",
    "    if 'alpha83' in df.columns: # Check dependencies\n",
    "        # alpha89 = (days since high) / (days since low + epsilon)\n",
    "        # Smaller value means high is recent relative to low\n",
    "        df['alpha89'] = safe_division(df['alpha83'], df['argmin_close_30_idx_pos'])\n",
    "\n",
    "    if 'close_stk' in df.columns:\n",
    "        df['argmin_close_30_idx_pos_stk'] = df.groupby('code')['close_stk'].transform(\n",
    "             lambda x: x.rolling(30, min_periods=15).apply(\n",
    "                 lambda s: (len(s) - 1) - np.nanargmin(s.to_numpy()) if not s.isnull().all() else np.nan,\n",
    "                 raw=False\n",
    "             )\n",
    "        )\n",
    "        if 'alpha83_stk' in df.columns: # Check dependencies\n",
    "            df['alpha89_stk'] = safe_division(df['alpha83_stk'], df['argmin_close_30_idx_pos_stk'])\n",
    "\n",
    "    # Alpha92 (Price Change * Volume)\n",
    "    if all(c in df.columns for c in ['delta_close_5', 'close', 'vol']):\n",
    "        df['alpha92'] = safe_division(df['delta_close_5'], df['close']) * df['vol']\n",
    "    if all(c in df.columns for c in ['delta_close_5_stk', 'close_stk', 'vol_stk']):\n",
    "        df['alpha92_stk'] = safe_division(df['delta_close_5_stk'], df['close_stk']) * df['vol_stk']\n",
    "\n",
    "\n",
    "    # === X. Stock & CB Linkage ===\n",
    "    print(\"Calculating: X. Stock & CB Linkage\")\n",
    "    if 'pct_chg' in df.columns and 'pct_chg_stk' in df.columns:\n",
    "        df['stk_up_bond_flat'] = ((df['pct_chg_stk'] > 0.03) & (df['pct_chg'] < 0.01)).astype(int)\n",
    "        df['stk_down_bond_weak'] = ((df['pct_chg_stk'] < -0.03) & (df['pct_chg'] < df['pct_chg_stk'])).astype(int)\n",
    "        # Lagged vars\n",
    "        df['pct_chg_stk_lag1'] = df.groupby('code')['pct_chg_stk'].shift(1)\n",
    "        df['pct_chg_stk_lag2'] = df.groupby('code')['pct_chg_stk'].shift(2)\n",
    "        # Check if lags were created before using them\n",
    "        if 'pct_chg_stk_lag1' in df.columns:\n",
    "            df['bond_hold_stk_rebound'] = ((df['pct_chg_stk_lag1'] < -0.03) & (df['pct_chg_stk'] > 0.01) & (df['pct_chg'] > 0.005)).astype(int)\n",
    "        if 'pct_chg_stk_lag2' in df.columns:\n",
    "            df['stk_down_then_up'] = ((df['pct_chg_stk_lag2'] < -0.03) & (df['pct_chg_stk'] > 0.02)).astype(int)\n",
    "        df['bond_rebound'] = (df['pct_chg'] > 0.01).astype(int)\n",
    "        if 'stk_down_then_up' in df.columns: # Check dependency\n",
    "            df['bond_follow_stk_rebound'] = ((df['stk_down_then_up'] == 1) & (df['bond_rebound'] == 1)).astype(int)\n",
    "        # Multi-day linkage\n",
    "        if all(c in df.columns for c in ['stk_ret_mean_3', 'bond_ret_mean_3']):\n",
    "             df['stk_up_bond_flat_3'] = ((df['stk_ret_mean_3'] > 0.01) & (df['bond_ret_mean_3'] < 0.003)).astype(int)\n",
    "        if all(c in df.columns for c in ['stk_ret_mean_5', 'bond_ret_mean_5']):\n",
    "             df['stk_up_bond_flat_5'] = ((df['stk_ret_mean_5'] > 0.015) & (df['bond_ret_mean_5'] < 0.005)).astype(int)\n",
    "\n",
    "\n",
    "    # === XI. Horizontal & Vertical Deviation ===\n",
    "    print(\"Calculating: XI. Horizontal & Vertical Deviation\")\n",
    "    print(\"  - Skipping: cb_vs_stk_ret_rank_diff (rank based)\")\n",
    "    for win in [3, 5, 10]:\n",
    "        if f'bond_ret_mean_{win}' in df.columns and f'stk_ret_mean_{win}' in df.columns:\n",
    "            df[f'dev_bond_vs_stk_{win}'] = df[f'bond_ret_mean_{win}'] - df[f'stk_ret_mean_{win}']\n",
    "\n",
    "    # Vertical requires longer term means calculated here\n",
    "    if 'pct_chg' in df.columns:\n",
    "        df['bond_ret_mean_20'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        df['bond_ret_mean_30'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(30, min_periods=18).mean())\n",
    "        if 'bond_ret_mean_3' in df.columns and 'bond_ret_mean_20' in df.columns:\n",
    "            df['dev_bond_short3_long20'] = df['bond_ret_mean_3'] - df['bond_ret_mean_20']\n",
    "        if 'bond_ret_mean_5' in df.columns and 'bond_ret_mean_30' in df.columns:\n",
    "            df['dev_bond_short5_long30'] = df['bond_ret_mean_5'] - df['bond_ret_mean_30']\n",
    "\n",
    "    if 'pct_chg_stk' in df.columns:\n",
    "        df['stk_ret_mean_20'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        df['stk_ret_mean_30'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(30, min_periods=18).mean())\n",
    "        if 'stk_ret_mean_3' in df.columns and 'stk_ret_mean_20' in df.columns:\n",
    "            df['dev_stk_short3_long20'] = df['stk_ret_mean_3'] - df['stk_ret_mean_20']\n",
    "        if 'stk_ret_mean_5' in df.columns and 'stk_ret_mean_30' in df.columns:\n",
    "            df['dev_stk_short5_long30'] = df['stk_ret_mean_5'] - df['stk_ret_mean_30']\n",
    "\n",
    "\n",
    "    # === XII. Risk & Drawdown (CB) ===\n",
    "    print(\"Calculating: XII. Risk & Drawdown (CB)\")\n",
    "    df['cb_low_5'] = df.groupby('code')['close'].transform(lambda x: x.rolling(5, min_periods=3).min())\n",
    "    df['cb_dev_from_low_5'] = safe_division(df['close'] - df['cb_low_5'], df['cb_low_5'])\n",
    "    df['cb_close_std_5'] = df.groupby('code')['close'].transform(lambda x: x.rolling(5, min_periods=3).std())\n",
    "    df['cb_high_5'] = df.groupby('code')['close'].transform(lambda x: x.rolling(5, min_periods=3).max())\n",
    "    df['cb_drawdown_5'] = safe_division(df['close'] - df['cb_high_5'], df['cb_high_5'])\n",
    "    # cb_dd_prob_estimate moved to XV where its components are calculated\n",
    "\n",
    "\n",
    "    # === XIII. Consolidation (CB) ===\n",
    "    print(\"Calculating: XIII. Consolidation (CB)\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'close', 'open', 'pre_close']):\n",
    "        df['range_hl'] = df['high'] - df['low']\n",
    "        df['atr_5'] = df.groupby('code')['range_hl'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "        df['atr_10'] = df.groupby('code')['range_hl'].transform(lambda x: x.rolling(10, min_periods=6).mean())\n",
    "        df['atr_20'] = df.groupby('code')['range_hl'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        df['atr_decay_5_10'] = safe_division(df['atr_5'], df['atr_10'])\n",
    "        df['atr_decay_5_20'] = safe_division(df['atr_5'], df['atr_20'])\n",
    "\n",
    "        df['close_std_10'] = df.groupby('code')['close'].transform(lambda x: x.rolling(10, min_periods=6).std())\n",
    "        if 'cb_close_std_5' in df.columns: # Dependency check\n",
    "             df['vol_shrink_ratio'] = safe_division(df['cb_close_std_5'], df['close_std_10'])\n",
    "\n",
    "        df['body_abs'] = (df['close'] - df['open']).abs()\n",
    "        df['body_pct'] = safe_division(df['body_abs'], df['pre_close'])\n",
    "        df['body_pct_mean_5'] = df.groupby('code')['body_pct'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "        df['shadow'] = df['range_hl'] - df['body_abs']\n",
    "        df['shadow_ratio'] = safe_division(df['shadow'], df['pre_close'])\n",
    "        df['shadow_mean_5'] = df.groupby('code')['shadow_ratio'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "        df['small_body_shadow_ratio'] = safe_division(df['shadow'], df['body_abs'], default=100)\n",
    "\n",
    "        df['is_doji'] = safe_division(df['body_abs'], df['range_hl']) < 0.15\n",
    "        df['doji_ratio_5'] = df.groupby('code')['is_doji'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "\n",
    "\n",
    "    # === XIV. Impulse & Momentum (CB) ===\n",
    "    print(\"Calculating: XIV. Impulse & Momentum (CB)\")\n",
    "    print(\"  - Skipping: score_high_jump_* (score based)\")\n",
    "    if all(c in df.columns for c in ['high', 'pre_close', 'pct_chg']):\n",
    "        thresholds = [0.015, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "        windows = [20, 120, 250, 500]\n",
    "        grouped_pct_chg = df.groupby('code')['pct_chg'] # Pre-group for efficiency\n",
    "\n",
    "        for thres in thresholds:\n",
    "            thres_name = int(thres*1000)\n",
    "            df[f'high_jump_{thres_name}_flag'] = (safe_division(df['high'], df['pre_close']) - 1) > thres\n",
    "            grouped_flag = df.groupby('code')[f'high_jump_{thres_name}_flag']\n",
    "\n",
    "            for win in windows:\n",
    "                print(f\"  - Calculating high_jump stats for thres={thres}, win={win}...\")\n",
    "                # Use helper function via transform if possible, otherwise apply\n",
    "                # Count is easy with transform\n",
    "                df[f'hj_count_{thres_name}_{win}'] = grouped_flag.transform(lambda x: x.rolling(win, min_periods=max(1, int(win*0.6))).sum().fillna(0))\n",
    "\n",
    "                # Mean and Std require apply because they condition on the flag\n",
    "                def calc_hj_mean_std(group):\n",
    "                    flag = group[f'high_jump_{thres_name}_flag']\n",
    "                    pct = group['pct_chg']\n",
    "                    _, mean_s, std_s = rolling_high_jump_stats(flag, pct, win)\n",
    "                    return pd.DataFrame({f'hj_mean_{thres_name}_{win}': mean_s, f'hj_std_{thres_name}_{win}': std_s})\n",
    "\n",
    "                # Apply and join back - ensure index is handled correctly\n",
    "                stats_df = df.groupby('code', group_keys=False).apply(calc_hj_mean_std)\n",
    "                df = df.join(stats_df) # Join based on index (which includes code, trade_date after reset)\n",
    "\n",
    "\n",
    "    if 'open_jump' in df.columns:\n",
    "        for n in [5, 10]:\n",
    "            df[f'open_gap_mean_{n}'] = df.groupby('code')['open_jump'].transform(lambda x: x.rolling(n, min_periods=int(n*0.6)).mean())\n",
    "            df[f'open_gap_max_{n}'] = df.groupby('code')['open_jump'].transform(lambda x: x.rolling(n, min_periods=int(n*0.6)).max())\n",
    "\n",
    "    if 'high' in df.columns and 'close' in df.columns:\n",
    "        for n in [3, 5, 10]:\n",
    "            close_mean_n = df.groupby('code')['close'].transform(lambda x: x.rolling(n, min_periods=max(1,int(n*0.6))).mean())\n",
    "            close_std_n = df.groupby('code')['close'].transform(lambda x: x.rolling(n, min_periods=max(1,int(n*0.6))).std())\n",
    "            df[f'jump_atr_{n}'] = safe_division(df['high'] - close_mean_n, close_std_n)\n",
    "\n",
    "    if 'pct_chg' in df.columns:\n",
    "        pct_mean_20 = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        pct_std_20 = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(20, min_periods=12).std())\n",
    "        df['zscore_pctchg_20'] = safe_division(df['pct_chg'] - pct_mean_20, pct_std_20)\n",
    "\n",
    "    if 'vol' in df.columns:\n",
    "        df['vol_ma20'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        df['vol_spike_ratio'] = safe_division(df['vol'], df['vol_ma20'], default=1.0)\n",
    "        df['vol_std_5'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(5, min_periods=3).std())\n",
    "        df['vol_std_20'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(20, min_periods=12).std())\n",
    "        df['vol_std_decay'] = safe_division(df['vol_std_5'], df['vol_std_20'])\n",
    "\n",
    "    if 'range_hl' in df.columns and 'atr_5' in df.columns:\n",
    "        df['range_today'] = df['range_hl']\n",
    "        df['range_atr_5'] = safe_division(df['range_today'], df['atr_5'])\n",
    "        df['range_jump_potential'] = (df['range_atr_5'] > 1.5).astype(int)\n",
    "\n",
    "    if all(c in df.columns for c in ['open', 'pre_close', 'close']):\n",
    "        df['gap_and_go_flag'] = ((safe_division(df['open'], df['pre_close']) - 1 > 0.01) & (df['close'] > df['open'])).astype(int)\n",
    "\n",
    "\n",
    "    # === XV. Downside Resilience (CB) ===\n",
    "    print(\"Calculating: XV. Downside Resilience (CB)\")\n",
    "    print(\"  - Skipping: no_fall_score_* (score based)\")\n",
    "    if 'pct_chg' in df.columns:\n",
    "        windows = [20, 60, 120, 250]\n",
    "        grouped_pct_chg = df.groupby('code')['pct_chg'] # Pre-group\n",
    "\n",
    "        for win in windows:\n",
    "            print(f\"  - Calculating downside stats for win={win}...\")\n",
    "\n",
    "            def calc_downside(group):\n",
    "                 pct = group['pct_chg']\n",
    "                 freq_s, mean_s, std_s = rolling_downside_stats(pct, win)\n",
    "                 return pd.DataFrame({\n",
    "                     f'down_freq_{win}': freq_s,\n",
    "                     f'down_amp_mean_{win}': mean_s,\n",
    "                     f'down_amp_std_{win}': std_s\n",
    "                 })\n",
    "\n",
    "            stats_df = df.groupby('code', group_keys=False).apply(calc_downside)\n",
    "            df = df.join(stats_df) # Join based on index\n",
    "\n",
    "        # Original cb_dd_prob_estimate (10d lag1 based)\n",
    "        df['cb_ret_lag1'] = df.groupby('code')['pct_chg'].shift(1)\n",
    "        if 'cb_ret_lag1' in df.columns:\n",
    "            df['cb_fall_flag'] = (df['cb_ret_lag1'] < 0).astype(int)\n",
    "            df['cb_fall_freq_10'] = df.groupby('code')['cb_fall_flag'].transform(lambda x: x.rolling(10, min_periods=6).mean())\n",
    "            df['cb_fall_amp_10'] = df.groupby('code')['cb_ret_lag1'].transform(\n",
    "                lambda x: x.rolling(10, min_periods=6).apply(lambda s: s[s < 0].mean() if (s < 0).any() else 0, raw=True)\n",
    "            )\n",
    "            df['cb_dd_prob_estimate'] = df['cb_fall_freq_10'] * df['cb_fall_amp_10']\n",
    "\n",
    "\n",
    "    # === XVI. K-Line Structure Continuity ===\n",
    "    print(\"Calculating: XVI. K-Line Structure Continuity\")\n",
    "    if all(c in df.columns for c in ['close', 'open']):\n",
    "        df['kline_direction'] = np.sign(df['close'] - df['open'])\n",
    "        df['kline_direction_shift1'] = df.groupby('code')['kline_direction'].shift(1)\n",
    "        if 'kline_direction_shift1' in df.columns: # Check dependency\n",
    "            df['kline_flip'] = (df['kline_direction'] * df['kline_direction_shift1'] < 0).astype(int)\n",
    "            df['kline_flip_ratio_5'] = df.groupby('code')['kline_flip'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "\n",
    "\n",
    "    # --- Final Cleanup & Optional Index Restore ---\n",
    "    # Drop temporary columns if any (like _high_jump_flag_temp)\n",
    "    temp_cols = [col for col in df.columns if col.startswith('_') and col.endswith('_temp')]\n",
    "    df = df.drop(columns=temp_cols, errors='ignore')\n",
    "\n",
    "    if restore_multiindex:\n",
    "        print(\"Restoring MultiIndex ['code', 'trade_date']...\")\n",
    "        df = df.set_index(['code', 'trade_date'])\n",
    "\n",
    "    print(\"Factor calculation finished.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "def safe_division(numerator, denominator, default=np.nan):\n",
    "    \"\"\"Performs division, returning default value if denominator is zero, NaN, or invalid.\"\"\"\n",
    "    try:\n",
    "        # Ensure inputs are numeric if they are series/arrays\n",
    "        if hasattr(numerator, '__iter__'):\n",
    "            numerator = pd.to_numeric(numerator, errors='coerce')\n",
    "        if hasattr(denominator, '__iter__'):\n",
    "            denominator = pd.to_numeric(denominator, errors='coerce')\n",
    "            denominator = denominator.replace(0, np.nan)\n",
    "        elif isinstance(denominator, (int, float)) and denominator == 0:\n",
    "            denominator = np.nan\n",
    "\n",
    "        result = numerator / denominator\n",
    "        if hasattr(result, '__iter__'):\n",
    "             # Replace inf/-inf that might result from large numbers / small numbers\n",
    "             result = result.replace([np.inf, -np.inf], np.nan)\n",
    "             return result.fillna(default)\n",
    "        elif np.isinf(result) or np.isnan(result):\n",
    "             return default\n",
    "        else:\n",
    "             return result\n",
    "\n",
    "    except (TypeError, ValueError):\n",
    "        # Handle cases where inputs cannot be converted to numeric\n",
    "        if hasattr(numerator, 'shape'):\n",
    "             return pd.Series(default, index=getattr(numerator, 'index', None), dtype=float)\n",
    "        elif hasattr(denominator, 'shape'):\n",
    "             return pd.Series(default, index=getattr(denominator, 'index', None), dtype=float)\n",
    "        else:\n",
    "             return default\n",
    "\n",
    "def ts_rank(series, window):\n",
    "    \"\"\"Calculates the rank of the last value in a rolling window.\"\"\"\n",
    "    if series.isnull().all(): # Handle all NaN window\n",
    "        return np.nan\n",
    "    # Rank within the window, get rank of the last element (-1 index)\n",
    "    # pct=True gives rank from 0 to 1\n",
    "    return series.rank(pct=True).iloc[-1]\n",
    "\n",
    "# Rolling correlation helper\n",
    "def rolling_corr(x_series, y_series, window, min_periods):\n",
    "    \"\"\"Safely compute rolling correlation\"\"\"\n",
    "    return x_series.rolling(window=window, min_periods=min_periods).corr(y_series)\n",
    "\n",
    "# Rolling covariance helper\n",
    "def rolling_cov(x_series, y_series, window, min_periods):\n",
    "    \"\"\"Safely compute rolling covariance\"\"\"\n",
    "    return x_series.rolling(window=window, min_periods=min_periods).cov(y_series)\n",
    "\n",
    "# Rolling rank helper (needed for Alpha 65, 99 inner rank)\n",
    "def rolling_series_rank(series, window, min_periods):\n",
    "     # Note: This ranks *within* the rolling window, might not be the same as daily rank\n",
    "     # For Alpha 65/99, the rank is applied *before* rolling.\n",
    "     # This helper is more for concept, usually rank is cross-sectional first.\n",
    "     # We will apply rank cross-sectionally before rolling for alpha factors.\n",
    "     # Keeping this placeholder in case needed for other rolling rank concepts.\n",
    "     # return series.rolling(window=window, min_periods=min_periods).apply(lambda x: x.rank().iloc[-1], raw=False)\n",
    "     pass # Not directly used for the current Alphas as rank is cross-sectional\n",
    "\n",
    "# Assume 'natr' function uses TA-Lib's NATR if not provided externally\n",
    "def apply_natr(group, n):\n",
    "     \"\"\"Applies TA-Lib NATR safely within a group.\"\"\"\n",
    "     if group[['high', 'low', 'close']].isnull().all().all() or len(group) < n:\n",
    "         return pd.Series(np.nan, index=group.index)\n",
    "     # Ensure float type for TA-Lib\n",
    "     high = group['high'].astype(float)\n",
    "     low = group['low'].astype(float)\n",
    "     close = group['close'].astype(float)\n",
    "     return ta.NATR(high, low, close, timeperiod=n)\n",
    "\n",
    "# --- Main Factor Calculation Function ---\n",
    "def calculate_factors(df, restore_multiindex=False):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å¯è½¬å€ºåŠå…¶å¯¹åº”æ­£è‚¡çš„è¡ç”Ÿå› å­ (åŒ…å«åŸºäºæ’åçš„Alphaå› å­)ã€‚\n",
    "    Handles DataFrame with 'code' and 'trade_date' as columns OR MultiIndex levels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): è¾“å…¥DataFrame.\n",
    "        restore_multiindex (bool): è‹¥ä¸ºTrue, åœ¨æœ«å°¾å°† ['code', 'trade_date'] è®¾å›ç´¢å¼•.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: æ·»åŠ äº†å› å­åˆ—çš„DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"å¼€å§‹å› å­è®¡ç®—...\")\n",
    "\n",
    "    # --- è¾“å…¥éªŒè¯å’Œç´¢å¼•å¤„ç† ---\n",
    "    original_index = df.index # Store original index if needed\n",
    "    if isinstance(df.index, pd.MultiIndex) and all(name in df.index.names for name in ['code', 'trade_date']):\n",
    "        print(\"æ£€æµ‹åˆ° 'code' å’Œ 'trade_date' åœ¨ MultiIndex ä¸­ï¼Œæ­£åœ¨é‡ç½®ç´¢å¼•...\")\n",
    "        df = df.reset_index()\n",
    "        is_multiindex_input = True\n",
    "    elif all(col in df.columns for col in ['code', 'trade_date']):\n",
    "        print(\"æ£€æµ‹åˆ° 'code' å’Œ 'trade_date' åœ¨åˆ—ä¸­ã€‚\")\n",
    "        is_multiindex_input = False\n",
    "    else:\n",
    "        raise ValueError(\"DataFrame å¿…é¡»åŒ…å« 'code' å’Œ 'trade_date'ï¼Œå¯ä»¥æ˜¯åœ¨åˆ—ä¸­æˆ–ä½œä¸º MultiIndex çš„å±‚çº§ã€‚\")\n",
    "\n",
    "    # ç¡®ä¿æ’åº\n",
    "    df = df.sort_values(by=['code', 'trade_date']).copy()\n",
    "    # --- ç»“æŸç´¢å¼•å¤„ç† ---\n",
    "\n",
    "\n",
    "    # 0. æ•°æ®ç±»å‹å‡†å¤‡ä¸å®‰å…¨æ£€æŸ¥\n",
    "    print(\"æ­¥éª¤ 0: å‡†å¤‡æ•°æ®ç±»å‹...\")\n",
    "    numeric_cols = ['high', 'low', 'close', 'open', 'vol', 'pre_close', 'pct_chg', 'turnover', 'remain_cap', 'float_share',\n",
    "                    'high_stk', 'low_stk', 'close_stk', 'open_stk', 'vol_stk', 'pct_chg_stk']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        # else:\n",
    "            # print(f\"  è­¦å‘Š: åˆ— '{col}' ä¸å­˜åœ¨.\")\n",
    "\n",
    "\n",
    "    # === I. åŸºæœ¬ä»·æ ¼ä¸æ³¢åŠ¨ç±»å› å­ï¼ˆè½¬å€ºæœ¬èº«ï¼‰ ===\n",
    "    print(\"è®¡ç®—: I. åŸºæœ¬ä»·æ ¼ä¸æ³¢åŠ¨ç±»å› å­ï¼ˆè½¬å€ºæœ¬èº«ï¼‰\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'close']):\n",
    "        # NATR\n",
    "        print(\"  - è®¡ç®— NATR...\")\n",
    "        df['natr_14'] = df.groupby('code', group_keys=False).apply(apply_natr, n=14)\n",
    "        for n in [1, 3, 5, 10, 20]:\n",
    "            df[f'natr_{n}'] = df.groupby('code', group_keys=False).apply(apply_natr, n=n)\n",
    "\n",
    "        # æŒ¯å¹…\n",
    "        df['zhengfu'] = safe_division(df['high'] - df['low'], df['close'])\n",
    "        if 'open' in df.columns:\n",
    "             df['zhengfu_cha'] = safe_division(df['high'] - df['close'], (df['open'] - df['close']).abs())\n",
    "\n",
    "    if 'close' in df.columns:\n",
    "        # MA, Momentum, Volatility\n",
    "        print(\"  - è®¡ç®— MA, Momentum, Volatility...\")\n",
    "        df['ma_20'] = df.groupby('code')['close'].transform(lambda x: ta.SMA(x.astype(float), timeperiod=20))\n",
    "        df['momentum_20'] = df.groupby('code')['close'].transform(lambda x: safe_division(x, x.shift(20)))\n",
    "        df['volatility_20'] = df.groupby('code')['close'].transform(lambda x: x.rolling(20, min_periods=10).std())\n",
    "        # Max Value\n",
    "        df['max_value'] = df.groupby('code')['close'].transform(lambda x: x.cummax().shift(1))\n",
    "        df['max_value_position'] = safe_division(df['close'], df['max_value'])\n",
    "\n",
    "    # æ¬¡æ—¥æ­¢ç›ˆç‰¹å¾ (æ ‡ç­¾)\n",
    "    if 'high' in df.columns and 'close' in df.columns:\n",
    "        print(\"  - è®¡ç®—æ¬¡æ—¥æ­¢ç›ˆç‰¹å¾...\")\n",
    "        df['aft_high1'] = df.groupby('code')['high'].shift(-1)\n",
    "        df['aft_high_cur_close'] = safe_division(df['aft_high1'] - df['close'], df['close'])\n",
    "\n",
    "\n",
    "    # === II. OBVé‡èƒ½æŒ‡æ ‡ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: II. OBVé‡èƒ½æŒ‡æ ‡ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if all(c in df.columns for c in ['close', 'vol']):\n",
    "        df['obv'] = df.groupby('code').apply(\n",
    "             lambda x: ta.OBV(x['close'].astype(float), x['vol'].astype(float)) if not x[['close','vol']].isnull().all().all() else pd.Series(index=x.index, dtype=float)\n",
    "        ).reset_index(level=0, drop=True) # Retain reset_index as used in original for apply\n",
    "        if 'obv' in df.columns:\n",
    "             df['obv_5'] = df.groupby('code')['obv'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "             df['obv_10'] = df.groupby('code')['obv'].transform(lambda x: x.rolling(10, min_periods=5).mean())\n",
    "             df['obv_ratio_5_10'] = safe_division(df['obv_5'], df['obv_10'])\n",
    "\n",
    "\n",
    "    # === III. æ¢æ‰‹ä¸å¸‚å€¼ç±»å› å­ ===\n",
    "    print(\"è®¡ç®—: III. æ¢æ‰‹ä¸å¸‚å€¼ç±»å› å­\")\n",
    "    if 'turnover' in df.columns:\n",
    "        print(\"  - è®¡ç®— turnover ç›¸å…³å› å­...\")\n",
    "        # Calculate turnover_pct as intermediate step (needed for rolling_avg)\n",
    "        df['turnover_pct_temp'] = df.groupby('trade_date')['turnover'].rank(pct=True)\n",
    "        # å‡å€¼æ¢æ‰‹ç‡\n",
    "        for win in [5, 10, 20, 60]:\n",
    "            df[f'turnover_{win}_avg'] = df.groupby('code')['turnover'].transform(lambda x: x.rolling(window=win, min_periods=int(win*0.6)).mean())\n",
    "        # åˆ†ä½æ¢æ‰‹ç‡å‡å€¼ (ä¾èµ– turnover_pct_temp)\n",
    "        if 'turnover_pct_temp' in df.columns:\n",
    "            for win in [1, 5, 20, 50]:\n",
    "                 df[f'rolling_{win}_avg'] = df.groupby('code')['turnover_pct_temp'].transform(lambda x: x.rolling(window=win, min_periods=max(1,int(win*0.6))).mean())\n",
    "            # åˆ†ä½æ¢æ‰‹ç‡æ¯”ç‡\n",
    "            if all(c in df.columns for c in ['rolling_1_avg', 'rolling_5_avg', 'rolling_20_avg', 'rolling_50_avg']):\n",
    "                 df['rolling_1_to_5_avg'] = safe_division(df['rolling_1_avg'], df['rolling_5_avg'])\n",
    "                 df['rolling_5_to_20_avg'] = safe_division(df['rolling_5_avg'], df['rolling_20_avg'])\n",
    "                 df['rolling_20_to_50_avg'] = safe_division(df['rolling_20_avg'], df['rolling_50_avg'])\n",
    "            # Drop intermediate temp column\n",
    "            df = df.drop(columns=['turnover_pct_temp'])\n",
    "        else:\n",
    "            print(\"  è­¦å‘Š: æ— æ³•è®¡ç®— rolling_avg ç­‰å› å­ï¼Œå› ä¸º turnover_pct_temp æœªæˆåŠŸè®¡ç®—ã€‚\")\n",
    "\n",
    "    if all(col in df.columns for col in ['remain_cap', 'float_share', 'close_stk']):\n",
    "        print(\"  - è®¡ç®— cap_float_share_rate...\")\n",
    "        df['cap_float_share_rate'] = safe_division(df['remain_cap'] * 10000, (df['float_share'] * df['close_stk']))\n",
    "\n",
    "\n",
    "    # === IV. åŒºé—´æ”¶ç›Šç‡ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰ ===\n",
    "    print(\"è®¡ç®—: IV. åŒºé—´æ”¶ç›Šç‡ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\")\n",
    "    # Use mean return naming consistently for deviation factors later\n",
    "    windows_ret = [3, 5, 10, 20]\n",
    "    if 'pct_chg' in df.columns:\n",
    "        print(\"  - è®¡ç®—è½¬å€ºåŒºé—´æ”¶ç›Šç‡...\")\n",
    "        for win in windows_ret:\n",
    "            # Cumulative Product Return\n",
    "            df[f'pct_chg_{win}'] = df.groupby('code')['pct_chg'].transform(\n",
    "                lambda x: (x + 1).rolling(win, min_periods=max(1,int(win*0.6))).apply(np.prod, raw=True) - 1\n",
    "            )\n",
    "            # Mean Arithmetic Return\n",
    "            df[f'bond_ret_mean_{win}'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(win, min_periods=max(1,int(win*0.6))).mean())\n",
    "\n",
    "    if 'pct_chg_stk' in df.columns:\n",
    "        print(\"  - è®¡ç®—è‚¡ç¥¨åŒºé—´æ”¶ç›Šç‡...\")\n",
    "        for win in windows_ret:\n",
    "            df[f'pct_chg_stk_{win}'] = df.groupby('code')['pct_chg_stk'].transform(\n",
    "                lambda x: (x + 1).rolling(win, min_periods=max(1,int(win*0.6))).apply(np.prod, raw=True) - 1\n",
    "            )\n",
    "            df[f'stk_ret_mean_{win}'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(win, min_periods=max(1,int(win*0.6))).mean())\n",
    "\n",
    "\n",
    "    # === V. æˆäº¤é‡å‡å€¼æ¯”å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: V. æˆäº¤é‡å‡å€¼æ¯”å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if 'vol' in df.columns:\n",
    "        vol_windows = [3, 5, 10, 20, 30, 60]\n",
    "        print(\"  - è®¡ç®—å‡é‡...\")\n",
    "        for n in vol_windows:\n",
    "            df[f'vol_{n}_avg'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(n, min_periods=int(n*0.6)).mean())\n",
    "        print(\"  - è®¡ç®—é‡æ¯”...\")\n",
    "        for n in vol_windows:\n",
    "            for m in vol_windows:\n",
    "                if n < m and f'vol_{n}_avg' in df.columns and f'vol_{m}_avg' in df.columns:\n",
    "                    df[f'vol_{n}_to_{m}'] = safe_division(df[f'vol_{n}_avg'], df[f'vol_{m}_avg'])\n",
    "\n",
    "\n",
    "    # === VI. æ³¢åŠ¨ç‡ä¸æŒ¯å¹…ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰ ===\n",
    "    print(\"è®¡ç®—: VI. æ³¢åŠ¨ç‡ä¸æŒ¯å¹…ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\")\n",
    "    bodong_windows = [5, 10, 20, 60]\n",
    "    if 'pct_chg_stk' in df.columns:\n",
    "        print(\"  - è®¡ç®—è‚¡ç¥¨æ³¢åŠ¨ç‡...\")\n",
    "        for win in bodong_windows:\n",
    "            df[f'bodong_{win}'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(win, min_periods=int(win*0.6)).std() * (win ** 0.5))\n",
    "        if all(c in df.columns for c in ['bodong_20', 'bodong_60']):\n",
    "            df['bodong_20_to_bodong_60'] = safe_division(df['bodong_20'], df['bodong_60'])\n",
    "\n",
    "    if 'pct_chg' in df.columns:\n",
    "        print(\"  - è®¡ç®—è½¬å€ºæ³¢åŠ¨ç‡...\")\n",
    "        for win in bodong_windows:\n",
    "             df[f'bodong_{win}_bd'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(win, min_periods=int(win*0.6)).std() * (win ** 0.5))\n",
    "\n",
    "    if 'zhengfu' in df.columns:\n",
    "        print(\"  - è®¡ç®—æŒ¯å¹…æ³¢åŠ¨...\")\n",
    "        for win in [1, 5, 10, 20, 60]:\n",
    "            df[f'zhengfu_{win}'] = df.groupby('code')['zhengfu'].transform(lambda x: x.rolling(win, min_periods=max(1,int(win*0.6))).std())\n",
    "            df[f'zhengfu_{win}_bodong'] = df[f'zhengfu_{win}'] * (win ** 0.5)\n",
    "\n",
    "\n",
    "    # === VII. è·³ç©ºä¸ç¼ºå£ç±»å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: VII. è·³ç©ºä¸ç¼ºå£ç±»å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'open', 'close', 'pre_close']):\n",
    "        print(\"  - è®¡ç®—åŸºç¡€è·³ç©º/ç¼ºå£æŒ‡æ ‡...\")\n",
    "        df['high_jump'] = (safe_division(df['high'], df['pre_close']) - 1) > 0.025 # Used in count\n",
    "        df['low_gap'] = (safe_division(df['low'], df['pre_close']) - 1) < -0.025   # Used in count\n",
    "        df['open_jump'] = (safe_division(df['open'], df['pre_close']) - 1).abs()\n",
    "        df['gap_body_ratio'] = safe_division(df['open'] - df['pre_close'], (df['close'] - df['open']))\n",
    "\n",
    "        jump_windows = [20, 100, 250] # Windows from original code\n",
    "        print(\"  - è®¡ç®—è·³ç©º/ç¼ºå£ç»Ÿè®¡...\")\n",
    "        if 'high_jump' in df.columns:\n",
    "            for win in jump_windows:\n",
    "                 df[f'high_jump_count_{win}'] = df.groupby('code')['high_jump'].transform(lambda x: x.rolling(window=win, min_periods=int(win*0.6)).sum())\n",
    "                 # Calculate pct rank based on count\n",
    "                 df[f'high_jump_count_{win}_pct'] = df.groupby('trade_date')[f'high_jump_count_{win}'].rank(pct=True)\n",
    "        if 'low_gap' in df.columns:\n",
    "            for win in jump_windows:\n",
    "                 df[f'low_gap_count_{win}'] = df.groupby('code')['low_gap'].transform(lambda x: x.rolling(window=win, min_periods=int(win*0.6)).sum())\n",
    "                 df[f'low_gap_count_{win}_pct'] = df.groupby('trade_date')[f'low_gap_count_{win}'].rank(pct=True)\n",
    "\n",
    "\n",
    "    # === VIII. Kçº¿ç»“æ„å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: VIII. Kçº¿ç»“æ„å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'open', 'close']):\n",
    "        high_low_diff = safe_division(1.0, df['high'] - df['low']) # Precompute inverse for safety/efficiency\n",
    "        df['close_to_high_ratio'] = (df['close'] - df['low']) * high_low_diff\n",
    "        df['close_to_low_ratio'] = (df['high'] - df['close']) * high_low_diff\n",
    "        df['body_position'] = (df['close'] - df['open']) * high_low_diff\n",
    "        df['upper_shadow_ratio'] = (df['high'] - df[['close', 'open']].max(axis=1)) * high_low_diff\n",
    "        df['lower_shadow_ratio'] = (df[['close', 'open']].min(axis=1) - df['low']) * high_low_diff\n",
    "\n",
    "\n",
    "    # === IX. è¶‹åŠ¿åè½¬ç±»Alphaå› å­ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰ ===\n",
    "    print(\"è®¡ç®—: IX. è¶‹åŠ¿åè½¬ç±»Alphaå› å­ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\")\n",
    "    # --- Prerequisites ---\n",
    "    print(\"  - è®¡ç®— Alpha å› å­å‰ç½®æ•°æ®...\")\n",
    "    df['delta_close_1'] = df.groupby('code')['close'].transform(lambda x: x.diff(1))\n",
    "    df['delta_vol_1'] = df.groupby('code')['vol'].transform(lambda x: x.diff(1))\n",
    "    df['delta_close_5'] = df.groupby('code')['close'].transform(lambda x: x.diff(5))\n",
    "    df['delta_close_10'] = df.groupby('code')['close'].transform(lambda x: x.diff(10))\n",
    "    df['mean_close_20'] = df.groupby('code')['close'].transform(lambda x: x.rolling(20, min_periods=10).mean())\n",
    "\n",
    "    if 'close_stk' in df.columns:\n",
    "        df['delta_close_1_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.diff(1))\n",
    "        df['delta_close_5_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.diff(5))\n",
    "        df['delta_close_10_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.diff(10))\n",
    "        df['mean_close_20_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.rolling(20, min_periods=10).mean())\n",
    "    if 'vol_stk' in df.columns:\n",
    "        df['delta_vol_1_stk'] = df.groupby('code')['vol_stk'].transform(lambda x: x.diff(1))\n",
    "\n",
    "    # --- Cross-sectional Ranks (can be slow) ---\n",
    "    print(\"  - è®¡ç®—æˆªé¢æ’å (å¯èƒ½è¾ƒæ…¢)...\")\n",
    "    if 'delta_close_10' in df.columns:\n",
    "        df['rank_delta_close_10'] = df.groupby('trade_date')['delta_close_10'].rank()\n",
    "    if 'vol' in df.columns:\n",
    "        df['rank_vol'] = df.groupby('trade_date')['vol'].rank()\n",
    "    if 'mean_close_20' in df.columns:\n",
    "        df['rank_mean_close_20'] = df.groupby('trade_date')['mean_close_20'].rank()\n",
    "    if 'close' in df.columns: # Rank close needed for Alpha65, 99\n",
    "         df['rank_close'] = df.groupby('trade_date')['close'].rank()\n",
    "\n",
    "    if 'delta_close_10_stk' in df.columns:\n",
    "        df['rank_delta_close_10_stk'] = df.groupby('trade_date')['delta_close_10_stk'].rank()\n",
    "    if 'vol_stk' in df.columns:\n",
    "        df['rank_vol_stk'] = df.groupby('trade_date')['vol_stk'].rank()\n",
    "    if 'mean_close_20_stk' in df.columns:\n",
    "        df['rank_mean_close_20_stk'] = df.groupby('trade_date')['mean_close_20_stk'].rank()\n",
    "    if 'close_stk' in df.columns: # Rank close_stk needed for Alpha65_stk, 99_stk\n",
    "         df['rank_close_stk'] = df.groupby('trade_date')['close_stk'].rank()\n",
    "\n",
    "    # --- Alpha Calculations ---\n",
    "    # Alpha6: -corr(rank(delta(close, 10)), rank(vol), 10)\n",
    "    print(\"  - è®¡ç®— Alpha6...\")\n",
    "    if all(c in df.columns for c in ['rank_delta_close_10', 'rank_vol']):\n",
    "         df['alpha6'] = df.groupby('code').apply(\n",
    "             lambda x: rolling_corr(x['rank_delta_close_10'], x['rank_vol'], 10, 6) * -1\n",
    "         ).reset_index(level=0, drop=True)\n",
    "    if all(c in df.columns for c in ['rank_delta_close_10_stk', 'rank_vol_stk']):\n",
    "         df['alpha6_stk'] = df.groupby('code').apply(\n",
    "             lambda x: rolling_corr(x['rank_delta_close_10_stk'], x['rank_vol_stk'], 10, 6) * -1\n",
    "         ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Alpha12: sign(delta(vol, 1)) * -1 * delta(close, 1)\n",
    "    print(\"  - è®¡ç®— Alpha12...\")\n",
    "    if all(c in df.columns for c in ['delta_vol_1', 'delta_close_1']):\n",
    "        df['alpha12'] = np.sign(df['delta_vol_1']) * -1 * df['delta_close_1']\n",
    "    if all(c in df.columns for c in ['delta_vol_1_stk', 'delta_close_1_stk']):\n",
    "        df['alpha12_stk'] = np.sign(df['delta_vol_1_stk']) * -1 * df['delta_close_1_stk']\n",
    "\n",
    "    # Alpha83: Days since 30d high (Corrected)\n",
    "    print(\"  - è®¡ç®— Alpha83...\")\n",
    "    df['alpha83'] = df.groupby('code')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=15).apply(\n",
    "            lambda s: (len(s) - 1) - np.nanargmax(s.to_numpy()) if not s.isnull().all() else np.nan, raw=False\n",
    "        )\n",
    "    )\n",
    "    if 'close_stk' in df.columns:\n",
    "        df['alpha83_stk'] = df.groupby('code')['close_stk'].transform(\n",
    "            lambda x: x.rolling(30, min_periods=15).apply(\n",
    "                lambda s: (len(s) - 1) - np.nanargmax(s.to_numpy()) if not s.isnull().all() else np.nan, raw=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Alpha18: close / rank(mean(close, 20))\n",
    "    print(\"  - è®¡ç®— Alpha18...\")\n",
    "    if all(c in df.columns for c in ['close', 'rank_mean_close_20']):\n",
    "        df['alpha18'] = safe_division(df['close'], df['rank_mean_close_20'])\n",
    "    if all(c in df.columns for c in ['close_stk', 'rank_mean_close_20_stk']):\n",
    "        df['alpha18_stk'] = safe_division(df['close_stk'], df['rank_mean_close_20_stk'])\n",
    "\n",
    "    # Alpha36: (correlation(vol, close, 5)) + (correlation(vol, open, 5))\n",
    "    print(\"  - è®¡ç®— Alpha36...\")\n",
    "    if all(c in df.columns for c in ['vol', 'close', 'open']):\n",
    "        def calc_alpha36(x):\n",
    "             corr_close = rolling_corr(x['vol'], x['close'], 5, 3)\n",
    "             corr_open = rolling_corr(x['vol'], x['open'], 5, 3)\n",
    "             return corr_close.add(corr_open, fill_value=0) # Handle potential NaNs\n",
    "        df['alpha36'] = df.groupby('code', group_keys=False).apply(calc_alpha36)\n",
    "    if all(c in df.columns for c in ['vol_stk', 'close_stk', 'open_stk']):\n",
    "        def calc_alpha36_stk(x):\n",
    "             corr_close = rolling_corr(x['vol_stk'], x['close_stk'], 5, 3)\n",
    "             corr_open = rolling_corr(x['vol_stk'], x['open_stk'], 5, 3)\n",
    "             return corr_close.add(corr_open, fill_value=0)\n",
    "        df['alpha36_stk'] = df.groupby('code', group_keys=False).apply(calc_alpha36_stk)\n",
    "\n",
    "    # Alpha89: (days since high) / (days since low + eps) (Corrected)\n",
    "    print(\"  - è®¡ç®— Alpha89...\")\n",
    "    df['argmin_close_30_idx_pos'] = df.groupby('code')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=15).apply(\n",
    "            lambda s: (len(s) - 1) - np.nanargmin(s.to_numpy()) if not s.isnull().all() else np.nan, raw=False\n",
    "        )\n",
    "    )\n",
    "    if 'alpha83' in df.columns: # Check dependencies\n",
    "        df['alpha89'] = safe_division(df['alpha83'], df['argmin_close_30_idx_pos'])\n",
    "    if 'close_stk' in df.columns:\n",
    "        df['argmin_close_30_idx_pos_stk'] = df.groupby('code')['close_stk'].transform(\n",
    "             lambda x: x.rolling(30, min_periods=15).apply(\n",
    "                 lambda s: (len(s) - 1) - np.nanargmin(s.to_numpy()) if not s.isnull().all() else np.nan, raw=False\n",
    "             )\n",
    "        )\n",
    "        if 'alpha83_stk' in df.columns: # Check dependencies\n",
    "            df['alpha89_stk'] = safe_division(df['alpha83_stk'], df['argmin_close_30_idx_pos_stk'])\n",
    "\n",
    "    # Alpha65: correlation(rank(close), rank(vol), 6)\n",
    "    print(\"  - è®¡ç®— Alpha65...\")\n",
    "    if all(c in df.columns for c in ['rank_close', 'rank_vol']):\n",
    "         df['alpha65'] = df.groupby('code').apply(\n",
    "             lambda x: rolling_corr(x['rank_close'], x['rank_vol'], 6, 4)\n",
    "         ).reset_index(level=0, drop=True)\n",
    "    if all(c in df.columns for c in ['rank_close_stk', 'rank_vol_stk']):\n",
    "         df['alpha65_stk'] = df.groupby('code').apply(\n",
    "             lambda x: rolling_corr(x['rank_close_stk'], x['rank_vol_stk'], 6, 4)\n",
    "         ).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Alpha76: -1 * ts_rank(correlation(close, vol, 10), 10)\n",
    "    print(\"  - è®¡ç®— Alpha76...\")\n",
    "    if all(c in df.columns for c in ['close', 'vol']):\n",
    "        df['corr_close_vol_10'] = df.groupby('code').apply(\n",
    "            lambda x: rolling_corr(x['close'], x['vol'], 10, 6)\n",
    "        ).reset_index(level=0, drop=True)\n",
    "        # Apply ts_rank using rolling apply\n",
    "        df['alpha76'] = df.groupby('code')['corr_close_vol_10'].transform(\n",
    "             lambda x: -1 * x.rolling(10, min_periods=6).apply(ts_rank, raw=False, args=(10,))\n",
    "        )\n",
    "    if all(c in df.columns for c in ['close_stk', 'vol_stk']):\n",
    "        df['corr_close_vol_10_stk'] = df.groupby('code').apply(\n",
    "            lambda x: rolling_corr(x['close_stk'], x['vol_stk'], 10, 6)\n",
    "        ).reset_index(level=0, drop=True)\n",
    "        df['alpha76_stk'] = df.groupby('code')['corr_close_vol_10_stk'].transform(\n",
    "             lambda x: -1 * x.rolling(10, min_periods=6).apply(ts_rank, raw=False, args=(10,))\n",
    "        )\n",
    "\n",
    "    # Alpha92: (delta(close, 5)/close) * vol\n",
    "    print(\"  - è®¡ç®— Alpha92...\")\n",
    "    if all(c in df.columns for c in ['delta_close_5', 'close', 'vol']):\n",
    "        df['alpha92'] = safe_division(df['delta_close_5'], df['close']) * df['vol']\n",
    "    if all(c in df.columns for c in ['delta_close_5_stk', 'close_stk', 'vol_stk']):\n",
    "        df['alpha92_stk'] = safe_division(df['delta_close_5_stk'], df['close_stk']) * df['vol_stk']\n",
    "\n",
    "    # Alpha99: -1 * ts_rank(cov(rank(close), rank(vol), 5), 5)\n",
    "    print(\"  - è®¡ç®— Alpha99...\")\n",
    "    if all(c in df.columns for c in ['rank_close', 'rank_vol']):\n",
    "         df['cov_rank_close_vol_5'] = df.groupby('code').apply(\n",
    "             lambda x: rolling_cov(x['rank_close'], x['rank_vol'], 5, 3)\n",
    "         ).reset_index(level=0, drop=True)\n",
    "         df['alpha99'] = df.groupby('code')['cov_rank_close_vol_5'].transform(\n",
    "              lambda x: -1 * x.rolling(5, min_periods=3).apply(ts_rank, raw=False, args=(5,))\n",
    "         )\n",
    "    if all(c in df.columns for c in ['rank_close_stk', 'rank_vol_stk']):\n",
    "         df['cov_rank_close_vol_5_stk'] = df.groupby('code').apply(\n",
    "             lambda x: rolling_cov(x['rank_close_stk'], x['rank_vol_stk'], 5, 3)\n",
    "         ).reset_index(level=0, drop=True)\n",
    "         df['alpha99_stk'] = df.groupby('code')['cov_rank_close_vol_5_stk'].transform(\n",
    "              lambda x: -1 * x.rolling(5, min_periods=3).apply(ts_rank, raw=False, args=(5,))\n",
    "         )\n",
    "\n",
    "\n",
    "    # === X. è‚¡ç¥¨ä¸è½¬å€ºè”åŠ¨å› å­ ===\n",
    "    print(\"è®¡ç®—: X. è‚¡ç¥¨ä¸è½¬å€ºè”åŠ¨å› å­\")\n",
    "    if 'pct_chg' in df.columns and 'pct_chg_stk' in df.columns:\n",
    "        print(\"  - è®¡ç®—æ—¥å†…è”åŠ¨...\")\n",
    "        df['stk_up_bond_flat'] = ((df['pct_chg_stk'] > 0.03) & (df['pct_chg'] < 0.01)).astype(int)\n",
    "        df['stk_down_bond_weak'] = ((df['pct_chg_stk'] < -0.03) & (df['pct_chg'] < df['pct_chg_stk'])).astype(int)\n",
    "        # Lagged vars\n",
    "        df['pct_chg_stk_lag1'] = df.groupby('code')['pct_chg_stk'].shift(1)\n",
    "        df['pct_chg_stk_lag2'] = df.groupby('code')['pct_chg_stk'].shift(2)\n",
    "        if 'pct_chg_stk_lag1' in df.columns:\n",
    "            df['bond_hold_stk_rebound'] = ((df['pct_chg_stk_lag1'] < -0.03) & (df['pct_chg_stk'] > 0.01) & (df['pct_chg'] > 0.005)).astype(int)\n",
    "        if 'pct_chg_stk_lag2' in df.columns:\n",
    "            df['stk_down_then_up'] = ((df['pct_chg_stk_lag2'] < -0.03) & (df['pct_chg_stk'] > 0.02)).astype(int)\n",
    "        df['bond_rebound'] = (df['pct_chg'] > 0.01).astype(int)\n",
    "        if 'stk_down_then_up' in df.columns: # Check dependency\n",
    "            df['bond_follow_stk_rebound'] = ((df['stk_down_then_up'] == 1) & (df['bond_rebound'] == 1)).astype(int)\n",
    "\n",
    "        print(\"  - è®¡ç®—å¤šæ—¥è”åŠ¨ (æ»æ¶¨)...\")\n",
    "        # Multi-day linkage (using mean returns calculated in section IV)\n",
    "        # Naming stk_chg_N/bond_chg_N based on original code, points to mean returns\n",
    "        df['stk_chg_3'] = df['stk_ret_mean_3'] if 'stk_ret_mean_3' in df.columns else np.nan\n",
    "        df['bond_chg_3'] = df['bond_ret_mean_3'] if 'bond_ret_mean_3' in df.columns else np.nan\n",
    "        df['stk_chg_5'] = df['stk_ret_mean_5'] if 'stk_ret_mean_5' in df.columns else np.nan\n",
    "        df['bond_chg_5'] = df['bond_ret_mean_5'] if 'bond_ret_mean_5' in df.columns else np.nan\n",
    "\n",
    "        if all(c in df.columns for c in ['stk_chg_3', 'bond_chg_3']):\n",
    "             df['stk_up_bond_flat_3'] = ((df['stk_chg_3'] > 0.03) & (df['bond_chg_3'] < 0.01)).astype(int)\n",
    "        if all(c in df.columns for c in ['stk_chg_5', 'bond_chg_5']):\n",
    "             df['stk_up_bond_flat_5'] = ((df['stk_chg_5'] > 0.05) & (df['bond_chg_5'] < 0.01)).astype(int)\n",
    "\n",
    "\n",
    "    # === XI. æ¨ªçºµå‘èƒŒç¦»å› å­ï¼ˆè‚¡ç¥¨ä¸è½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: XI. æ¨ªçºµå‘èƒŒç¦»å› å­ï¼ˆè‚¡ç¥¨ä¸è½¬å€ºï¼‰\")\n",
    "    print(\"  - è®¡ç®—æ¨ªå‘èƒŒç¦»...\")\n",
    "    for win in [3, 5, 10]:\n",
    "        # Deviation using mean returns\n",
    "        if f'bond_ret_mean_{win}' in df.columns and f'stk_ret_mean_{win}' in df.columns:\n",
    "            df[f'dev_bond_vs_stk_{win}'] = df[f'bond_ret_mean_{win}'] - df[f'stk_ret_mean_{win}']\n",
    "        # Rank difference (requires returns calculated in IV)\n",
    "        if f'pct_chg_{win}' in df.columns :\n",
    "             df[f'cb_ret_rank_{win}'] = df.groupby('trade_date')[f'pct_chg_{win}'].rank() # Rank based on cumulative return\n",
    "        if f'pct_chg_stk_{win}' in df.columns:\n",
    "             df[f'stk_ret_rank_{win}'] = df.groupby('trade_date')[f'pct_chg_stk_{win}'].rank()\n",
    "        if f'cb_ret_rank_{win}' in df.columns and f'stk_ret_rank_{win}' in df.columns:\n",
    "             df[f'cb_vs_stk_ret_rank_diff_{win}'] = df[f'cb_ret_rank_{win}'] - df[f'stk_ret_rank_{win}']\n",
    "\n",
    "    print(\"  - è®¡ç®—çºµå‘èƒŒç¦»...\")\n",
    "    # Longer term means needed\n",
    "    if 'pct_chg' in df.columns:\n",
    "        df['bond_ret_mean_20'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        df['bond_ret_mean_30'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(30, min_periods=18).mean())\n",
    "        if 'bond_ret_mean_3' in df.columns and 'bond_ret_mean_20' in df.columns:\n",
    "            df['dev_bond_short3_long20'] = df['bond_ret_mean_3'] - df['bond_ret_mean_20']\n",
    "        if 'bond_ret_mean_5' in df.columns and 'bond_ret_mean_30' in df.columns:\n",
    "            df['dev_bond_short5_long30'] = df['bond_ret_mean_5'] - df['bond_ret_mean_30']\n",
    "\n",
    "    if 'pct_chg_stk' in df.columns:\n",
    "        df['stk_ret_mean_20'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        df['stk_ret_mean_30'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(30, min_periods=18).mean())\n",
    "        if 'stk_ret_mean_3' in df.columns and 'stk_ret_mean_20' in df.columns:\n",
    "            df['dev_stk_short3_long20'] = df['stk_ret_mean_3'] - df['stk_ret_mean_20']\n",
    "        if 'stk_ret_mean_5' in df.columns and 'stk_ret_mean_30' in df.columns:\n",
    "            df['dev_stk_short5_long30'] = df['stk_ret_mean_5'] - df['stk_ret_mean_30']\n",
    "\n",
    "\n",
    "    # === XII. é£é™©ä¸å›æ’¤ç›¸å…³å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: XII. é£é™©ä¸å›æ’¤ç›¸å…³å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if 'close' in df.columns:\n",
    "        print(\"  - è®¡ç®—ä½ç‚¹è·ç¦»/æ ‡å‡†å·®/å›æ’¤...\")\n",
    "        df['cb_low_5'] = df.groupby('code')['close'].transform(lambda x: x.rolling(5, min_periods=3).min())\n",
    "        df['cb_dev_from_low_5'] = safe_division(df['close'] - df['cb_low_5'], df['cb_low_5'])\n",
    "        df['cb_close_std_5'] = df.groupby('code')['close'].transform(lambda x: x.rolling(5, min_periods=3).std())\n",
    "        df['cb_high_5'] = df.groupby('code')['close'].transform(lambda x: x.rolling(5, min_periods=3).max())\n",
    "        df['cb_drawdown_5'] = safe_division(df['close'] - df['cb_high_5'], df['cb_high_5'])\n",
    "\n",
    "    if 'pct_chg' in df.columns:\n",
    "        print(\"  - è®¡ç®—ä¸‹è·Œé£é™©é¢„ä¼°...\")\n",
    "        df['cb_ret_1'] = df.groupby('code')['pct_chg'].shift(1) # Renamed from original cb_ret_1\n",
    "        df['cb_fall_flag'] = (df['cb_ret_1'] < 0).astype(int)\n",
    "        df['cb_fall_freq_10'] = df.groupby('code')['cb_fall_flag'].transform(lambda x: x.rolling(10, min_periods=6).mean())\n",
    "        df['cb_fall_amp_10'] = df.groupby('code')['cb_ret_1'].transform(\n",
    "            lambda x: x.rolling(10, min_periods=6).apply(lambda s: s[s < 0].mean() if (s < 0).any() else 0, raw=True)\n",
    "        )\n",
    "        df['cb_dd_prob_estimate'] = df['cb_fall_freq_10'] * df['cb_fall_amp_10']\n",
    "\n",
    "\n",
    "    # === XIII. éœ‡è¡æ”¶æ•›ç±»å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: XIII. éœ‡è¡æ”¶æ•›ç±»å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'close', 'open', 'pre_close']):\n",
    "        print(\"  - è®¡ç®— ATR/æŒ¯å¹…/ä»·æ ¼æ³¢åŠ¨ æ”¶æ•›...\")\n",
    "        df['range_hl'] = df['high'] - df['low'] # Reusable range\n",
    "        df['atr_5'] = df.groupby('code')['range_hl'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "        df['atr_10'] = df.groupby('code')['range_hl'].transform(lambda x: x.rolling(10, min_periods=6).mean()) # Needed in one snippet\n",
    "        df['atr_20'] = df.groupby('code')['range_hl'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        df['atr_5_decay'] = safe_division(df['atr_5'], df['atr_20']) # Based on snippet\n",
    "        df['atr_decay_5_10'] = safe_division(df['atr_5'], df['atr_10']) # Based on snippet\n",
    "\n",
    "        # Zhengfu decay / Range ratio (similar to atr decay)\n",
    "        # df['zhengfu_5'] = df['atr_5'] # Redundant if atr_5 exists\n",
    "        # df['zhengfu_20'] = df['atr_20']\n",
    "        df['zhengfu_decay_5_20'] = safe_division(df['atr_5'], df['atr_20']) # Reusing ATR calc\n",
    "        df['range_ratio_5_20'] = safe_division(df['atr_5'], df['atr_20']) # Reusing ATR calc\n",
    "\n",
    "        # Close std deviation shrink\n",
    "        df['close_std_10'] = df.groupby('code')['close'].transform(lambda x: x.rolling(10, min_periods=6).std())\n",
    "        if 'cb_close_std_5' in df.columns: # Dependency from Sec XII\n",
    "             df['vol_shrink_ratio'] = safe_division(df['cb_close_std_5'], df['close_std_10'])\n",
    "\n",
    "        print(\"  - è®¡ç®— Kçº¿å®ä½“/å½±çº¿/åå­—æ˜Ÿ ç‰¹å¾...\")\n",
    "        df['body_abs'] = (df['close'] - df['open']).abs()\n",
    "        df['body_pct'] = safe_division(df['body_abs'], df['pre_close'])\n",
    "        df['body_pct_mean_5'] = df.groupby('code')['body_pct'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "\n",
    "        df['shadow'] = df['range_hl'] - df['body_abs']\n",
    "        df['shadow_ratio'] = safe_division(df['shadow'], df['pre_close'])\n",
    "        df['shadow_mean_5'] = df.groupby('code')['shadow_ratio'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "        df['small_body_shadow_ratio'] = safe_division(df['shadow'], df['body_abs'], default=100) # Assign large number if body is zero\n",
    "\n",
    "        df['is_doji'] = safe_division(df['body_abs'], df['range_hl']) < 0.15\n",
    "        df['doji_ratio_5'] = df.groupby('code')['is_doji'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "\n",
    "\n",
    "    # === XIV. è„‰å†²ä¸åŠ¨èƒ½å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: XIV. è„‰å†²ä¸åŠ¨èƒ½å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if all(c in df.columns for c in ['high', 'pre_close', 'pct_chg', 'vol', 'close', 'low']):\n",
    "        print(\"  - è®¡ç®—é«˜è„‰å†²ç»Ÿè®¡ (count, mean, score)...\")\n",
    "        thresholds = [0.015, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "        pulse_window = 20 # Window used for score in original code\n",
    "        for thres in thresholds:\n",
    "            thres_name = int(thres*1000)\n",
    "            # Flag\n",
    "            df[f'high_jump_{thres_name}'] = (safe_division(df['high'], df['pre_close']) - 1) > thres\n",
    "            # Count (rolling sum of flags)\n",
    "            df[f'count_high_jump_{thres_name}_{pulse_window}'] = df.groupby('code')[f'high_jump_{thres_name}'].transform(\n",
    "                lambda x: x.rolling(pulse_window, min_periods=int(pulse_window*0.6)).sum()\n",
    "            )\n",
    "            # Mean return on jump days\n",
    "            df[f'mean_high_jump_{thres_name}_{pulse_window}'] = df.groupby('code').apply(\n",
    "                 lambda x: x['pct_chg'].where(x[f'high_jump_{thres_name}']).rolling(pulse_window, min_periods=1).mean() # Need at least 1 jump day for mean\n",
    "            ).reset_index(level=0, drop=True)\n",
    "            # Score\n",
    "            if f'count_high_jump_{thres_name}_{pulse_window}' in df.columns and f'mean_high_jump_{thres_name}_{pulse_window}' in df.columns:\n",
    "                 df[f'score_high_jump_{thres_name}_{pulse_window}'] = df[f'count_high_jump_{thres_name}_{pulse_window}'] * df[f'mean_high_jump_{thres_name}_{pulse_window}']\n",
    "\n",
    "        print(\"  - è®¡ç®—å…¶ä»–è„‰å†²æŒ‡æ ‡...\")\n",
    "        # Z-score\n",
    "        pct_mean_20 = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        pct_std_20 = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(20, min_periods=12).std())\n",
    "        df['zscore_pctchg_20'] = safe_division(df['pct_chg'] - pct_mean_20, pct_std_20)\n",
    "\n",
    "        # Volume spike & decay\n",
    "        df['vol_ma20'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(20, min_periods=12).mean())\n",
    "        df['vol_spike_ratio'] = safe_division(df['vol'], df['vol_ma20'], default=1.0)\n",
    "        df['vol_std_5'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(5, min_periods=3).std())\n",
    "        df['vol_std_20'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(20, min_periods=12).std())\n",
    "        df['vol_std_decay'] = safe_division(df['vol_std_5'], df['vol_std_20'])\n",
    "\n",
    "        # Open gap stats\n",
    "        if 'open_jump' in df.columns: # Calculated in Sec VII\n",
    "             for n in [5, 10]:\n",
    "                 df[f'open_gap_mean_{n}'] = df.groupby('code')['open_jump'].transform(lambda x: x.rolling(n, min_periods=int(n*0.6)).mean())\n",
    "                 df[f'open_gap_max_{n}'] = df.groupby('code')['open_jump'].transform(lambda x: x.rolling(n, min_periods=int(n*0.6)).max())\n",
    "\n",
    "        # Jump ATR\n",
    "        for n in [3, 5, 10]:\n",
    "            close_mean_n = df.groupby('code')['close'].transform(lambda x: x.rolling(n, min_periods=max(1,int(n*0.6))).mean())\n",
    "            close_std_n = df.groupby('code')['close'].transform(lambda x: x.rolling(n, min_periods=max(1,int(n*0.6))).std())\n",
    "            df[f'jump_atr_{n}'] = safe_division(df['high'] - close_mean_n, close_std_n)\n",
    "\n",
    "        # Range jump potential\n",
    "        if 'range_hl' in df.columns and 'atr_5' in df.columns: # Calculated in XIII\n",
    "            df['range_today'] = df['range_hl'] # Alias for clarity\n",
    "            df['range_atr_5'] = safe_division(df['range_today'], df['atr_5'])\n",
    "            df['range_jump_potential'] = (df['range_atr_5'] > 1.5).astype(int)\n",
    "\n",
    "        # Gap and Go flag\n",
    "        if 'open' in df.columns and 'pre_close' in df.columns:\n",
    "            df['gap_and_go_flag'] = ((safe_division(df['open'], df['pre_close']) - 1 > 0.02) & (df['close'] > df['open'])).astype(int) # Using 2% threshold from snippet\n",
    "\n",
    "\n",
    "    # === XV. è·Œä¸åŠ¨å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: XV. è·Œä¸åŠ¨å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if 'pct_chg' in df.columns:\n",
    "        print(\"  - è®¡ç®—ä¸‹è·Œé¢‘ç‡/å¹…åº¦/è¯„åˆ†...\")\n",
    "        for win in [5, 10]: # Windows from original code\n",
    "            # Down frequency\n",
    "            df[f'down_freq_{win}'] = df.groupby('code')['pct_chg'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=int(win*0.6)).apply(lambda s: (s < 0).mean(), raw=True)\n",
    "            )\n",
    "            # Down amplitude (mean of negative returns)\n",
    "            df[f'down_amp_{win}'] = df.groupby('code')['pct_chg'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=int(win*0.6)).apply(lambda s: s[s < 0].mean() if (s < 0).any() else 0, raw=True)\n",
    "            )\n",
    "            # Score\n",
    "            df[f'no_fall_score_{win}'] = (1 - df[f'down_freq_{win}']) * (-df[f'down_amp_{win}'])\n",
    "\n",
    "\n",
    "    # === XVI. Kçº¿ç»“æ„è¿ç»­æ€§ ===\n",
    "    print(\"è®¡ç®—: XVI. Kçº¿ç»“æ„è¿ç»­æ€§\")\n",
    "    if all(c in df.columns for c in ['close', 'open']):\n",
    "        print(\"  - è®¡ç®—Kçº¿æ–¹å‘åè½¬ç‡...\")\n",
    "        df['kline_direction'] = np.sign(df['close'] - df['open'])\n",
    "        df['kline_direction_shift1'] = df.groupby('code')['kline_direction'].shift(1)\n",
    "        if 'kline_direction_shift1' in df.columns: # Check dependency\n",
    "            df['kline_flip'] = (df['kline_direction'] * df['kline_direction_shift1'] < 0).astype(int)\n",
    "            df['kline_flip_ratio_5'] = df.groupby('code')['kline_flip'].transform(lambda x: x.rolling(5, min_periods=3).mean())\n",
    "\n",
    "\n",
    "    # --- Final Cleanup & Optional Index Restore ---\n",
    "    print(\"æ­¥éª¤ XVII: æ¸…ç†ä¸´æ—¶åˆ—å’Œæ¢å¤ç´¢å¼• (å¦‚æœéœ€è¦)...\")\n",
    "    # Drop intermediate columns used only for calculation (if any)\n",
    "    cols_to_drop = [\n",
    "        'rank_delta_close_10', 'rank_vol', 'rank_mean_close_20', 'rank_close',\n",
    "        'rank_delta_close_10_stk', 'rank_vol_stk', 'rank_mean_close_20_stk', 'rank_close_stk',\n",
    "        'corr_close_vol_10', 'corr_close_vol_10_stk',\n",
    "        'cov_rank_close_vol_5', 'cov_rank_close_vol_5_stk',\n",
    "        'argmin_close_30_idx_pos', 'argmin_close_30_idx_pos_stk',\n",
    "        # Add other intermediate columns if created e.g. '_high_jump_flag_temp' if used\n",
    "    ]\n",
    "    # Check if columns exist before dropping\n",
    "    cols_exist = [col for col in cols_to_drop if col in df.columns]\n",
    "    if cols_exist:\n",
    "         df = df.drop(columns=cols_exist)\n",
    "\n",
    "    if restore_multiindex and is_multiindex_input:\n",
    "        print(\"  - æ¢å¤ MultiIndex ['code', 'trade_date']...\")\n",
    "        df = df.set_index(['code', 'trade_date'])\n",
    "    elif restore_multiindex and not is_multiindex_input:\n",
    "        print(\"  - è­¦å‘Š: åŸå§‹è¾“å…¥æ²¡æœ‰ MultiIndexï¼Œæ— æ³•æ¢å¤ã€‚\")\n",
    "\n",
    "\n",
    "    print(\"å› å­è®¡ç®—å®Œæˆã€‚\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib as ta # ç¡®ä¿å·²å®‰è£… TA-Lib: pip install TA-Lib\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def safe_division(numerator, denominator, default=np.nan):\n",
    "    \"\"\"\n",
    "    æ‰§è¡Œå®‰å…¨é™¤æ³•ï¼Œåˆ†æ¯ä¸ºé›¶ã€NaNæˆ–æ— æ•ˆæ—¶è¿”å›é»˜è®¤å€¼ã€‚\n",
    "    Handles potential inf/-inf results and type errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure inputs are numeric if they are series/arrays\n",
    "        if hasattr(numerator, '__iter__'):\n",
    "            numerator = pd.to_numeric(numerator, errors='coerce')\n",
    "        if hasattr(denominator, '__iter__'):\n",
    "            denominator = pd.to_numeric(denominator, errors='coerce')\n",
    "            denominator = denominator.replace(0, np.nan)\n",
    "        elif isinstance(denominator, (int, float)) and denominator == 0:\n",
    "            denominator = np.nan\n",
    "\n",
    "        result = numerator / denominator\n",
    "        if hasattr(result, '__iter__'):\n",
    "             # Replace inf/-inf that might result from large numbers / small numbers\n",
    "             result = result.replace([np.inf, -np.inf], np.nan)\n",
    "             return result.fillna(default)\n",
    "        elif np.isinf(result) or np.isnan(result):\n",
    "             return default\n",
    "        else:\n",
    "             return result\n",
    "\n",
    "    except (TypeError, ValueError):\n",
    "        # Handle cases where inputs cannot be converted to numeric\n",
    "        if hasattr(numerator, 'shape'):\n",
    "             return pd.Series(default, index=getattr(numerator, 'index', None), dtype=float)\n",
    "        elif hasattr(denominator, 'shape'):\n",
    "             return pd.Series(default, index=getattr(denominator, 'index', None), dtype=float)\n",
    "        else:\n",
    "             return default\n",
    "\n",
    "def rolling_downside_stats(series, window):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ»šåŠ¨çª—å£å†…çš„ä¸‹è·Œé¢‘ç‡ã€å¹³å‡ä¸‹è·Œå¹…åº¦ã€ä¸‹è·Œå¹…åº¦æ ‡å‡†å·®ã€‚\n",
    "    Calculates downside frequency, mean amplitude, and std amplitude.\n",
    "    \"\"\"\n",
    "    min_p = max(1, int(window * 0.6))\n",
    "    min_p_std = max(2, int(window * 0.6)) # Std requires at least 2 points\n",
    "\n",
    "    is_down = series < 0\n",
    "    freq = is_down.rolling(window, min_periods=min_p).mean().fillna(0)\n",
    "\n",
    "    down_series = series.where(is_down)\n",
    "    # Use np.nanmean and np.nanstd for robustness if needed, but rolling handles skipna\n",
    "    mean_amp = down_series.rolling(window, min_periods=min_p).mean().fillna(0)\n",
    "    std_amp = down_series.rolling(window, min_periods=min_p_std).std().fillna(0)\n",
    "    return freq, mean_amp, std_amp\n",
    "\n",
    "def rolling_high_jump_stats(jump_flag, pct_chg, window):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ»šåŠ¨çª—å£å†…çš„é«˜è„‰å†²æ¬¡æ•°ã€å¹³å‡è„‰å†²æ—¥æ”¶ç›Šç‡ã€è„‰å†²æ—¥æ”¶ç›Šç‡æ ‡å‡†å·®ã€‚\n",
    "    Calculates high jump count, mean jump return, and std jump return.\n",
    "    \"\"\"\n",
    "    min_p = max(1, int(window * 0.6))\n",
    "    min_p_std = max(2, int(window * 0.6)) # Std requires at least 2 points\n",
    "\n",
    "    # Ensure jump_flag is boolean or 0/1\n",
    "    jump_flag_bool = jump_flag.astype(bool)\n",
    "\n",
    "    count = jump_flag_bool.rolling(window, min_periods=min_p).sum().fillna(0)\n",
    "\n",
    "    jump_returns = pct_chg.where(jump_flag_bool)\n",
    "    mean_ret = jump_returns.rolling(window, min_periods=min_p).mean().fillna(0)\n",
    "    std_ret = jump_returns.rolling(window, min_periods=min_p_std).std().fillna(0)\n",
    "    return count, mean_ret, std_ret\n",
    "\n",
    "def ts_rank(series, window):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ»šåŠ¨çª—å£ä¸­æœ€åä¸€ä¸ªå€¼çš„ç™¾åˆ†ä½æ’åã€‚\n",
    "    Calculates the rank of the last value in a rolling window (as percentage).\n",
    "    \"\"\"\n",
    "    if series.isnull().all(): # Handle all NaN window\n",
    "        return np.nan\n",
    "    # Rank within the window, get rank of the last element (-1 index)\n",
    "    # pct=True gives rank from 0 to 1\n",
    "    try:\n",
    "        ranks = series.rank(pct=True)\n",
    "        return ranks.iloc[-1] if not ranks.empty else np.nan\n",
    "    except IndexError: # Handle empty series edge case\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def rolling_corr(x_series, y_series, window, min_periods):\n",
    "    \"\"\"å®‰å…¨åœ°è®¡ç®—æ»šåŠ¨ç›¸å…³ç³»æ•° (Safely compute rolling correlation)\"\"\"\n",
    "    # Ensure consistent indexing before rolling operation if needed, though groupby handles it\n",
    "    return x_series.rolling(window=window, min_periods=min_periods).corr(y_series)\n",
    "\n",
    "def rolling_cov(x_series, y_series, window, min_periods):\n",
    "    \"\"\"å®‰å…¨åœ°è®¡ç®—æ»šåŠ¨åæ–¹å·® (Safely compute rolling covariance)\"\"\"\n",
    "    return x_series.rolling(window=window, min_periods=min_periods).cov(y_series)\n",
    "\n",
    "def apply_natr(group, n):\n",
    "     \"\"\"åœ¨åˆ†ç»„å†…å®‰å…¨åœ°åº”ç”¨ TA-Lib NATR (Applies TA-Lib NATR safely within a group)\"\"\"\n",
    "     # Check for sufficient non-null data\n",
    "     required_cols = ['high', 'low', 'close']\n",
    "     if group[required_cols].isnull().all().all() or len(group.dropna(subset=required_cols)) < n:\n",
    "         return pd.Series(np.nan, index=group.index)\n",
    "     # Ensure float type for TA-Lib\n",
    "     high = group['high'].astype(float)\n",
    "     low = group['low'].astype(float)\n",
    "     close = group['close'].astype(float)\n",
    "     try:\n",
    "         # Add min_periods check internally for TA-Lib\n",
    "         if len(high.dropna()) >= n and len(low.dropna()) >= n and len(close.dropna()) >= n:\n",
    "             return ta.NATR(high, low, close, timeperiod=n)\n",
    "         else:\n",
    "              return pd.Series(np.nan, index=group.index)\n",
    "     except Exception: # Catch potential TA-Lib errors\n",
    "         return pd.Series(np.nan, index=group.index)\n",
    "\n",
    "# --- Main Factor Calculation Function ---\n",
    "def calculate_factors_merged_v3(df, restore_multiindex=False):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å¯è½¬å€ºåŠå…¶å¯¹åº”æ­£è‚¡çš„è¡ç”Ÿå› å­ (åˆå¹¶ç‰ˆæœ¬ V3 - åŒ…å«ä¸‰ä»½ä»£ç å› å­)ã€‚\n",
    "    Calculates convertible bond and stock factors (Merged V3 - incorporating factors from three scripts).\n",
    "    Handles DataFrame with 'code' and 'trade_date' as columns OR MultiIndex levels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): è¾“å…¥DataFrame (Input DataFrame).\n",
    "        restore_multiindex (bool): è‹¥ä¸ºTrue, åœ¨æœ«å°¾å°† ['code', 'trade_date'] è®¾å›ç´¢å¼• (If True, sets ['code', 'trade_date'] back as index at the end).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: æ·»åŠ äº†å› å­åˆ—çš„DataFrame (DataFrame with added factor columns).\n",
    "    \"\"\"\n",
    "    print(\"å¼€å§‹åˆå¹¶åçš„å› å­è®¡ç®— (V3)...\")\n",
    "\n",
    "    # --- è¾“å…¥éªŒè¯å’Œç´¢å¼•å¤„ç† (Input Validation and Index Handling) ---\n",
    "    original_index = df.index # Store original index if needed\n",
    "    is_multiindex_input = isinstance(df.index, pd.MultiIndex) and all(name in df.index.names for name in ['code', 'trade_date'])\n",
    "\n",
    "    if is_multiindex_input:\n",
    "        print(\"æ£€æµ‹åˆ° 'code' å’Œ 'trade_date' åœ¨ MultiIndex ä¸­ï¼Œæ­£åœ¨é‡ç½®ç´¢å¼•...\")\n",
    "        df = df.reset_index()\n",
    "    elif all(col in df.columns for col in ['code', 'trade_date']):\n",
    "        print(\"æ£€æµ‹åˆ° 'code' å’Œ 'trade_date' åœ¨åˆ—ä¸­ã€‚\")\n",
    "    else:\n",
    "        raise ValueError(\"DataFrame å¿…é¡»åŒ…å« 'code' å’Œ 'trade_date'ï¼Œå¯ä»¥æ˜¯åœ¨åˆ—ä¸­æˆ–ä½œä¸º MultiIndex çš„å±‚çº§ã€‚\")\n",
    "\n",
    "    # ç¡®ä¿æ’åº (Ensure sorting)\n",
    "    df = df.sort_values(by=['code', 'trade_date']).copy()\n",
    "    # --- ç»“æŸç´¢å¼•å¤„ç† ---\n",
    "\n",
    "\n",
    "    # 0. æ•°æ®ç±»å‹å‡†å¤‡ä¸å®‰å…¨æ£€æŸ¥ (Data Type Preparation & Safety Check)\n",
    "    print(\"æ­¥éª¤ 0: å‡†å¤‡æ•°æ®ç±»å‹...\")\n",
    "    numeric_cols = [\n",
    "        'high', 'low', 'close', 'open', 'vol', 'pre_close', 'pct_chg', 'turnover',\n",
    "        'remain_cap', 'float_share',\n",
    "        'high_stk', 'low_stk', 'close_stk', 'open_stk', 'vol_stk', 'pct_chg_stk'\n",
    "    ]\n",
    "    # Include additional columns if Script 3 introduced them and they need numeric conversion\n",
    "    # Example: 'turnover_premium_rate' etc., if they exist\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        # else:\n",
    "            # print(f\"  è­¦å‘Š: åˆ— '{col}' ä¸å­˜åœ¨.\") # Optional warning\n",
    "\n",
    "    # Define minimum periods factor based on window size\n",
    "    def get_min_periods(window, factor=0.6, min_required=1):\n",
    "        return max(min_required, int(window * factor))\n",
    "\n",
    "    def get_min_periods_std(window, factor=0.6):\n",
    "        return max(2, int(window * factor)) # Std needs at least 2 periods\n",
    "\n",
    "    # === I. åŸºæœ¬ä»·æ ¼ä¸æ³¢åŠ¨ç±»å› å­ï¼ˆè½¬å€ºæœ¬èº«ï¼‰ ===\n",
    "    print(\"è®¡ç®—: I. åŸºæœ¬ä»·æ ¼ä¸æ³¢åŠ¨ç±»å› å­ï¼ˆè½¬å€ºæœ¬èº«ï¼‰\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'close']):\n",
    "        # NATR (Normalized Average True Range) - æ ‡å‡†åŒ–å¹³å‡çœŸå®æ³¢å¹…\n",
    "        print(\"  - è®¡ç®— NATR...\")\n",
    "        df['natr_14'] = df.groupby('code', group_keys=False).apply(apply_natr, n=14)\n",
    "        # å¤šå‘¨æœŸNATR (Multi-period NATR)\n",
    "        for n in [1, 3, 5, 10, 20]:\n",
    "            df[f'natr_{n}'] = df.groupby('code', group_keys=False).apply(apply_natr, n=n)\n",
    "\n",
    "        # æŒ¯å¹… (Amplitude)\n",
    "        df['zhengfu'] = safe_division(df['high'] - df['low'], df['close'])\n",
    "        if 'open' in df.columns:\n",
    "             # æŒ¯å¹…å·® (Amplitude Difference related to open/close)\n",
    "             df['zhengfu_cha'] = safe_division(df['high'] - df['close'], (df['open'] - df['close']).abs())\n",
    "\n",
    "    if 'close' in df.columns:\n",
    "        # MA (Moving Average) - ç§»åŠ¨å¹³å‡çº¿\n",
    "        print(\"  - è®¡ç®— MA, Momentum, Volatility...\")\n",
    "        df['ma_20'] = df.groupby('code')['close'].transform(lambda x: ta.SMA(x.astype(float), timeperiod=20))\n",
    "        # Momentum - åŠ¨é‡\n",
    "        df['momentum_20'] = df.groupby('code')['close'].transform(lambda x: safe_division(x, x.shift(20)))\n",
    "        # Volatility (Rolling Standard Deviation) - æ»šåŠ¨æ ‡å‡†å·®/å†å²æ³¢åŠ¨ç‡\n",
    "        df['volatility_20'] = df.groupby('code')['close'].transform(lambda x: x.rolling(20, min_periods=get_min_periods_std(20)).std())\n",
    "        # Max Value (Historical High) - å†å²é«˜ç‚¹\n",
    "        df['max_value'] = df.groupby('code')['close'].transform(lambda x: x.cummax().shift(1))\n",
    "        # Max Value Position (Ratio to Historical High) - å½“å‰ä»·æ ¼ç›¸å¯¹å†å²é«˜ç‚¹ä½ç½®\n",
    "        df['max_value_position'] = safe_division(df['close'], df['max_value'])\n",
    "\n",
    "    # æ¬¡æ—¥æ­¢ç›ˆç‰¹å¾ (Next Day Profit-Taking Feature / Label)\n",
    "    if 'high' in df.columns and 'close' in df.columns:\n",
    "        print(\"  - è®¡ç®—æ¬¡æ—¥æ­¢ç›ˆç‰¹å¾...\")\n",
    "        df['aft_high1'] = df.groupby('code')['high'].shift(-1)\n",
    "        df['aft_high_cur_close'] = safe_division(df['aft_high1'] - df['close'], df['close'])\n",
    "\n",
    "\n",
    "    # === II. OBVé‡èƒ½æŒ‡æ ‡ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: II. OBVé‡èƒ½æŒ‡æ ‡ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if all(c in df.columns for c in ['close', 'vol']):\n",
    "        # OBV (On Balance Volume) - èƒ½é‡æ½®æŒ‡æ ‡\n",
    "        # TA-Lib's OBV doesn't need explicit grouping if data is sorted, but apply safer for complex cases\n",
    "        df['obv'] = df.groupby('code', group_keys=False).apply(\n",
    "             lambda x: ta.OBV(x['close'].astype(float), x['vol'].astype(float)) if not x[['close','vol']].isnull().all().all() else pd.Series(index=x.index, dtype=float)\n",
    "        )\n",
    "        if 'obv' in df.columns:\n",
    "             # OBV ç§»åŠ¨å¹³å‡ (OBV Moving Average)\n",
    "             df['obv_5'] = df.groupby('code')['obv'].transform(lambda x: x.rolling(5, min_periods=get_min_periods(5)).mean())\n",
    "             df['obv_10'] = df.groupby('code')['obv'].transform(lambda x: x.rolling(10, min_periods=get_min_periods(10)).mean())\n",
    "             # OBV å‡çº¿æ¯”ç‡ (OBV Moving Average Ratio)\n",
    "             df['obv_ratio_5_10'] = safe_division(df['obv_5'], df['obv_10'])\n",
    "\n",
    "\n",
    "    # === III. æ¢æ‰‹ä¸å¸‚å€¼ç±»å› å­ ===\n",
    "    print(\"è®¡ç®—: III. æ¢æ‰‹ä¸å¸‚å€¼ç±»å› å­\")\n",
    "    if 'turnover' in df.columns:\n",
    "        print(\"  - è®¡ç®— turnover ç›¸å…³å› å­...\")\n",
    "        # æ¢æ‰‹ç‡æ—¥å†…ç™¾åˆ†ä½æ’å (Daily Turnover Rank Percentage)\n",
    "        df['turnover_pct'] = df.groupby('trade_date')['turnover'].rank(pct=True)\n",
    "        # å‡å€¼æ¢æ‰‹ç‡ (Average Turnover Rate)\n",
    "        for win in [5, 10, 20, 60]:\n",
    "            df[f'turnover_{win}_avg'] = df.groupby('code')['turnover'].transform(lambda x: x.rolling(window=win, min_periods=get_min_periods(win)).mean())\n",
    "        # åˆ†ä½æ¢æ‰‹ç‡å‡å€¼ (Rolling Mean of Turnover Rank Pct)\n",
    "        if 'turnover_pct' in df.columns:\n",
    "            for win in [1, 5, 20, 50]:\n",
    "                 df[f'rolling_{win}_avg'] = df.groupby('code')['turnover_pct'].transform(lambda x: x.rolling(window=win, min_periods=get_min_periods(win, min_required=1)).mean()) # Rank avg needs min_periods=1\n",
    "            # åˆ†ä½æ¢æ‰‹ç‡æ¯”ç‡ (Ratio of Rolling Turnover Rank Pct Means)\n",
    "            if all(c in df.columns for c in ['rolling_1_avg', 'rolling_5_avg', 'rolling_20_avg', 'rolling_50_avg']):\n",
    "                 df['rolling_1_to_5_avg'] = safe_division(df['rolling_1_avg'], df['rolling_5_avg'])\n",
    "                 df['rolling_5_to_20_avg'] = safe_division(df['rolling_5_avg'], df['rolling_20_avg'])\n",
    "                 df['rolling_20_to_50_avg'] = safe_division(df['rolling_20_avg'], df['rolling_50_avg'])\n",
    "        else:\n",
    "            print(\"  è­¦å‘Š: æ— æ³•è®¡ç®— rolling_avg ç­‰å› å­ï¼Œå› ä¸º turnover_pct æœªæˆåŠŸè®¡ç®—ã€‚\")\n",
    "\n",
    "    if all(col in df.columns for col in ['remain_cap', 'float_share', 'close_stk']):\n",
    "        print(\"  - è®¡ç®—æµé€šå¸‚å€¼å æ¯” (Cap / Float Share Value)...\")\n",
    "        # è½¬å€ºä½™é¢ / (æµé€šè‚¡ * è‚¡ä»·) - è¡¡é‡è½¬å€ºç›¸å¯¹æµé€šç›˜çš„å¤§å°\n",
    "        df['cap_float_share_rate'] = safe_division(df['remain_cap'] * 10000, (df['float_share'] * df['close_stk']))\n",
    "\n",
    "\n",
    "    # === IV. åŒºé—´æ”¶ç›Šç‡ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰ ===\n",
    "    print(\"è®¡ç®—: IV. åŒºé—´æ”¶ç›Šç‡ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\")\n",
    "    windows_ret = [3, 5, 10, 20] # Use comprehensive windows\n",
    "    if 'pct_chg' in df.columns:\n",
    "        print(\"  - è®¡ç®—è½¬å€ºåŒºé—´æ”¶ç›Šç‡...\")\n",
    "        df['pct_chg_plus_1'] = df['pct_chg'] + 1 # Temporary column for product calculation\n",
    "        for win in windows_ret:\n",
    "            # ç´¯è®¡æ”¶ç›Šç‡ (Cumulative Product Return)\n",
    "            # min_periods=1 is appropriate for cumulative product over short periods\n",
    "            df[f'pct_chg_{win}'] = df.groupby('code')['pct_chg_plus_1'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=1).apply(np.prod, raw=True)\n",
    "            ) - 1\n",
    "            # ç®—æœ¯å¹³å‡æ”¶ç›Šç‡ (Mean Arithmetic Return)\n",
    "            df[f'bond_ret_mean_{win}'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(win, min_periods=get_min_periods(win)).mean())\n",
    "        del df['pct_chg_plus_1'] # Clean up temporary column\n",
    "\n",
    "    if 'pct_chg_stk' in df.columns:\n",
    "        print(\"  - è®¡ç®—è‚¡ç¥¨åŒºé—´æ”¶ç›Šç‡...\")\n",
    "        df['pct_chg_stk_plus_1'] = df['pct_chg_stk'] + 1 # Temporary column\n",
    "        for win in windows_ret:\n",
    "            df[f'pct_chg_stk_{win}'] = df.groupby('code')['pct_chg_stk_plus_1'].transform(\n",
    "                lambda x: (x).rolling(win, min_periods=1).apply(np.prod, raw=True)\n",
    "            ) - 1\n",
    "            df[f'stk_ret_mean_{win}'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(win, min_periods=get_min_periods(win)).mean())\n",
    "        del df['pct_chg_stk_plus_1'] # Clean up\n",
    "\n",
    "\n",
    "    # === V. æˆäº¤é‡å‡å€¼æ¯”å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: V. æˆäº¤é‡å‡å€¼æ¯”å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if 'vol' in df.columns:\n",
    "        vol_windows = [3, 5, 10, 20, 30, 60]\n",
    "        print(\"  - è®¡ç®—å‡é‡...\")\n",
    "        for n in vol_windows:\n",
    "            df[f'vol_{n}_avg'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(n, min_periods=get_min_periods(n)).mean())\n",
    "        print(\"  - è®¡ç®—é‡æ¯”...\")\n",
    "        # è‡ªåŠ¨ç”Ÿæˆå¤šç»„ N:M é‡èƒ½æ¯” (Auto-generate N:M volume ratios)\n",
    "        for n in vol_windows:\n",
    "            for m in vol_windows:\n",
    "                if n < m and f'vol_{n}_avg' in df.columns and f'vol_{m}_avg' in df.columns:\n",
    "                    df[f'vol_{n}_to_{m}'] = safe_division(df[f'vol_{n}_avg'], df[f'vol_{m}_avg'])\n",
    "\n",
    "\n",
    "    # === VI. æ³¢åŠ¨ç‡ä¸æŒ¯å¹…ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰ ===\n",
    "   # === VI. æ³¢åŠ¨ç‡ä¸æŒ¯å¹…ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰ ===\n",
    "    print(\"è®¡ç®—: VI. æ³¢åŠ¨ç‡ä¸æŒ¯å¹…ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\")\n",
    "    bodong_windows = [5, 10, 20, 60]\n",
    "    if 'pct_chg_stk' in df.columns:\n",
    "        print(\"  - è®¡ç®—è‚¡ç¥¨å¹´åŒ–æ³¢åŠ¨ç‡...\")\n",
    "        for win in bodong_windows:\n",
    "            conceptual_min_periods = get_min_periods_std(win)\n",
    "            actual_min_periods = min(win, conceptual_min_periods) # Ensure min_periods <= window\n",
    "            df[f'bodong_{win}'] = df.groupby('code')['pct_chg_stk'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=actual_min_periods).std() * (win ** 0.5)\n",
    "            )\n",
    "        if all(c in df.columns for c in ['bodong_20', 'bodong_60']):\n",
    "            df['bodong_20_to_bodong_60'] = safe_division(df['bodong_20'], df['bodong_60'])\n",
    "\n",
    "    if 'pct_chg' in df.columns:\n",
    "        print(\"  - è®¡ç®—è½¬å€ºå¹´åŒ–æ³¢åŠ¨ç‡...\")\n",
    "        for win in bodong_windows:\n",
    "            conceptual_min_periods = get_min_periods_std(win)\n",
    "            actual_min_periods = min(win, conceptual_min_periods) # Ensure min_periods <= window\n",
    "            df[f'bodong_{win}_bd'] = df.groupby('code')['pct_chg'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=actual_min_periods).std() * (win ** 0.5)\n",
    "            )\n",
    "\n",
    "    if 'zhengfu' in df.columns:\n",
    "        print(\"  - è®¡ç®—æŒ¯å¹…æ»šåŠ¨æ ‡å‡†å·® & æŒ¯å¹…æ³¢åŠ¨...\")\n",
    "        for win in [1, 5, 10, 20, 60]:\n",
    "            # æŒ¯å¹…æ ‡å‡†å·® (Rolling Std of Amplitude)\n",
    "            # è·å–æ ‡å‡†å·®é€šå¸¸éœ€è¦çš„æœ€å°å‘¨æœŸ (é€šå¸¸æ˜¯2)\n",
    "            conceptual_min_periods = get_min_periods_std(win)\n",
    "            # å®é™…åº”ç”¨çš„æœ€å°å‘¨æœŸä¸èƒ½è¶…è¿‡çª—å£å¤§å°\n",
    "            actual_min_periods = min(win, conceptual_min_periods)\n",
    "            # ç‰¹åˆ«æ³¨æ„ï¼šå½“ win=1 æ—¶, actual_min_periods=1ã€‚std() å¯¹å•ä¸ªå€¼é€šå¸¸è¿”å› NaN (ddof=1é»˜è®¤) æˆ– 0 (ddof=0)ã€‚\n",
    "            df[f'zhengfu_{win}'] = df.groupby('code')['zhengfu'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=actual_min_periods).std()\n",
    "            )\n",
    "            # æŒ¯å¹…æ³¢åŠ¨ = æŒ¯å¹…æ ‡å‡†å·® * sqrt(å‘¨æœŸ) (Amplitude Volatility)\n",
    "            df[f'zhengfu_{win}_bodong'] = df[f'zhengfu_{win}'] * (win ** 0.5)\n",
    "\n",
    "\n",
    "    # === VII. è·³ç©ºä¸ç¼ºå£ç±»å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: VII. è·³ç©ºä¸ç¼ºå£ç±»å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'open', 'close', 'pre_close']):\n",
    "        print(\"  - è®¡ç®—åŸºç¡€è·³ç©º/ç¼ºå£æŒ‡æ ‡...\")\n",
    "        # é«˜å¼€è·³ç©ºæ ‡å¿— (High Jump Flag > 2.5%)\n",
    "        df['high_jump_flag'] = (safe_division(df['high'], df['pre_close']) - 1) > 0.025\n",
    "        # ä½å¼€ç¼ºå£æ ‡å¿— (Low Gap Flag < -2.5%)\n",
    "        df['low_gap_flag'] = (safe_division(df['low'], df['pre_close']) - 1) < -0.025\n",
    "        # å¼€ç›˜è·³ç©ºå¹…åº¦ç»å¯¹å€¼ (Absolute Open Jump Percentage)\n",
    "        df['open_jump'] = (safe_division(df['open'], df['pre_close']) - 1).abs()\n",
    "        # è·³ç©ºç¼ºå£å®ä½“æ¯” (Gap to Body Ratio)\n",
    "        df['gap_body_ratio'] = safe_division(df['open'] - df['pre_close'], (df['close'] - df['open']))\n",
    "\n",
    "        jump_windows = [20, 100, 250] # Windows from original code\n",
    "        print(\"  - è®¡ç®—è·³ç©º/ç¼ºå£ç»Ÿè®¡ (è®¡æ•°ä¸ç™¾åˆ†ä½)...\")\n",
    "        if 'high_jump_flag' in df.columns:\n",
    "            for win in jump_windows:\n",
    "                 # è·³ç©ºæ¬¡æ•° (High Jump Count)\n",
    "                 df[f'high_jump_count_{win}'] = df.groupby('code')['high_jump_flag'].transform(lambda x: x.rolling(window=win, min_periods=get_min_periods(win, min_required=1)).sum())\n",
    "                 # è·³ç©ºæ¬¡æ•°æ—¥å†…ç™¾åˆ†ä½ (Daily Rank Pct of High Jump Count)\n",
    "                 df[f'high_jump_count_{win}_pct'] = df.groupby('trade_date')[f'high_jump_count_{win}'].rank(pct=True)\n",
    "                 # Example filter from Script 3 (can be used downstream)\n",
    "                 # df.loc[df[f'high_jump_count_{win}_pct'] < 0.1, 'filter_low_jump_pct'] = True\n",
    "        if 'low_gap_flag' in df.columns:\n",
    "            for win in jump_windows:\n",
    "                 # ç¼ºå£æ¬¡æ•° (Low Gap Count)\n",
    "                 df[f'low_gap_count_{win}'] = df.groupby('code')['low_gap_flag'].transform(lambda x: x.rolling(window=win, min_periods=get_min_periods(win, min_required=1)).sum())\n",
    "                 # ç¼ºå£æ¬¡æ•°æ—¥å†…ç™¾åˆ†ä½ (Daily Rank Pct of Low Gap Count)\n",
    "                 df[f'low_gap_count_{win}_pct'] = df.groupby('trade_date')[f'low_gap_count_{win}'].rank(pct=True)\n",
    "\n",
    "\n",
    "    # === VIII. Kçº¿ç»“æ„å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: VIII. Kçº¿ç»“æ„å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'open', 'close']):\n",
    "        high_low_diff = df['high'] - df['low']\n",
    "        # Use safe_division for ratios\n",
    "        # æ”¶ç›˜ä»·åœ¨ K çº¿ä¸­çš„ç›¸å¯¹ä½ç½® (Close relative to High/Low)\n",
    "        df['close_to_high_ratio'] = safe_division(df['close'] - df['low'], high_low_diff) # è¶Šæ¥è¿‘1ï¼Œæ”¶ç›˜è¶Šé è¿‘æœ€é«˜ä»·\n",
    "        df['close_to_low_ratio'] = safe_division(df['high'] - df['close'], high_low_diff) # è¶Šæ¥è¿‘0ï¼Œæ”¶ç›˜è¶Šé è¿‘æœ€é«˜ä»·\n",
    "        # Kçº¿å®ä½“ç›¸å¯¹ä½ç½® (Body position relative to High/Low)\n",
    "        df['body_position'] = safe_division(df['close'] - df['open'], high_low_diff) # æ­£æ•°è¡¨ç¤ºé˜³çº¿ä½ç½®ï¼Œè´Ÿæ•°è¡¨ç¤ºé˜´çº¿ä½ç½®\n",
    "        # ä¸Šå½±çº¿æ¯”ä¾‹ (Upper Shadow Ratio)\n",
    "        df['upper_shadow_ratio'] = safe_division(df['high'] - df[['close', 'open']].max(axis=1), high_low_diff) # è¶Šå°ä¸Šå½±çº¿è¶ŠçŸ­\n",
    "        # ä¸‹å½±çº¿æ¯”ä¾‹ (Lower Shadow Ratio)\n",
    "        df['lower_shadow_ratio'] = safe_division(df[['close', 'open']].min(axis=1) - df['low'], high_low_diff) # è¶Šå°ä¸‹å½±çº¿è¶ŠçŸ­\n",
    "\n",
    "\n",
    "    # === IX. è¶‹åŠ¿åè½¬ç±»Alphaå› å­ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰ ===\n",
    "    print(\"è®¡ç®—: IX. è¶‹åŠ¿åè½¬ç±»Alphaå› å­ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\")\n",
    "    # --- Prerequisites (å‰ç½®è®¡ç®—) ---\n",
    "    print(\"  - è®¡ç®— Alpha å› å­å‰ç½®æ•°æ®...\")\n",
    "    df['delta_close_1'] = df.groupby('code')['close'].transform(lambda x: x.diff(1))\n",
    "    df['delta_vol_1'] = df.groupby('code')['vol'].transform(lambda x: x.diff(1))\n",
    "    df['delta_close_5'] = df.groupby('code')['close'].transform(lambda x: x.diff(5))\n",
    "    df['delta_close_10'] = df.groupby('code')['close'].transform(lambda x: x.diff(10))\n",
    "    df['mean_close_20'] = df.groupby('code')['close'].transform(lambda x: x.rolling(20, min_periods=get_min_periods(20)).mean())\n",
    "\n",
    "    if 'close_stk' in df.columns:\n",
    "        df['delta_close_1_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.diff(1))\n",
    "        df['delta_close_5_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.diff(5))\n",
    "        df['delta_close_10_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.diff(10))\n",
    "        df['mean_close_20_stk'] = df.groupby('code')['close_stk'].transform(lambda x: x.rolling(20, min_periods=get_min_periods(20)).mean())\n",
    "    if 'vol_stk' in df.columns:\n",
    "        df['delta_vol_1_stk'] = df.groupby('code')['vol_stk'].transform(lambda x: x.diff(1))\n",
    "\n",
    "    # --- Cross-sectional Ranks (æˆªé¢æ’å - å¯èƒ½è¾ƒæ…¢) ---\n",
    "    print(\"  - è®¡ç®—æˆªé¢æ’å (å¯èƒ½è¾ƒæ…¢)...\")\n",
    "    rank_cols_input = {\n",
    "        'delta_close_10': 'rank_delta_close_10', 'vol': 'rank_vol',\n",
    "        'mean_close_20': 'rank_mean_close_20', 'close': 'rank_close'\n",
    "    }\n",
    "    rank_cols_input_stk = {\n",
    "        'delta_close_10_stk': 'rank_delta_close_10_stk', 'vol_stk': 'rank_vol_stk',\n",
    "        'mean_close_20_stk': 'rank_mean_close_20_stk', 'close_stk': 'rank_close_stk'\n",
    "    }\n",
    "    for col, rank_col in rank_cols_input.items():\n",
    "        if col in df.columns: df[rank_col] = df.groupby('trade_date')[col].rank()\n",
    "    for col, rank_col in rank_cols_input_stk.items():\n",
    "        if col in df.columns: df[rank_col] = df.groupby('trade_date')[col].rank()\n",
    "\n",
    "    # --- Alpha Calculations (Alphaå› å­è®¡ç®—) ---\n",
    "    # Alpha6: -corr(rank(delta(close, 10)), rank(vol), 10) | è¶‹åŠ¿ä¸æˆäº¤é‡ç›¸å…³æ€§ï¼Œåè½¬ä¿¡å·\n",
    "    print(\"  - è®¡ç®— Alpha6...\")\n",
    "    if all(c in df.columns for c in ['rank_delta_close_10', 'rank_vol']):\n",
    "         df['alpha6'] = df.groupby('code', group_keys=False).apply(\n",
    "             lambda x: rolling_corr(x['rank_delta_close_10'], x['rank_vol'], 10, get_min_periods(10)) * -1\n",
    "         )\n",
    "    if all(c in df.columns for c in ['rank_delta_close_10_stk', 'rank_vol_stk']):\n",
    "         df['alpha6_stk'] = df.groupby('code', group_keys=False).apply(\n",
    "             lambda x: rolling_corr(x['rank_delta_close_10_stk'], x['rank_vol_stk'], 10, get_min_periods(10)) * -1\n",
    "         )\n",
    "\n",
    "    # Alpha12: sign(delta(vol, 1)) * -1 * delta(close, 1) | æˆäº¤é‡å˜åŠ¨çš„åå‘åŠ¨é‡ä¿¡å·\n",
    "    print(\"  - è®¡ç®— Alpha12...\")\n",
    "    if all(c in df.columns for c in ['delta_vol_1', 'delta_close_1']):\n",
    "        df['alpha12'] = np.sign(df['delta_vol_1']) * -1 * df['delta_close_1']\n",
    "    if all(c in df.columns for c in ['delta_vol_1_stk', 'delta_close_1_stk']):\n",
    "        df['alpha12_stk'] = np.sign(df['delta_vol_1_stk']) * -1 * df['delta_close_1_stk']\n",
    "\n",
    "    # Alpha83: Days since 30d high (Corrected) | è¿‘30æ—¥é«˜ç‚¹å‡ºç°æ—¶é—´ (å¤©æ•°)ï¼Œæ•°å€¼è¶Šå°è¶Šå¼º\n",
    "    # ä½¿ç”¨ä¿®æ­£åçš„ nanargmax é€»è¾‘!\n",
    "    print(\"  - è®¡ç®— Alpha83 (ä¿®æ­£ç‰ˆ)...\")\n",
    "    df['alpha83'] = df.groupby('code')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=get_min_periods(30, factor=0.5)).apply( # Factor 0.5 for 15 min periods\n",
    "            lambda s: (len(s) - 1) - np.nanargmax(s.to_numpy()) if not s.isnull().all() else np.nan, raw=False\n",
    "        )\n",
    "    )\n",
    "    if 'close_stk' in df.columns:\n",
    "        df['alpha83_stk'] = df.groupby('code')['close_stk'].transform(\n",
    "            lambda x: x.rolling(30, min_periods=get_min_periods(30, factor=0.5)).apply(\n",
    "                lambda s: (len(s) - 1) - np.nanargmax(s.to_numpy()) if not s.isnull().all() else np.nan, raw=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Alpha18: close / rank(mean(close, 20)) | åŠ¨é‡å‡å€¼åç¦»ï¼ˆè¶Šå¤§è¶Šå¼ºï¼‰\n",
    "    print(\"  - è®¡ç®— Alpha18...\")\n",
    "    if all(c in df.columns for c in ['close', 'rank_mean_close_20']):\n",
    "        df['alpha18'] = safe_division(df['close'], df['rank_mean_close_20'])\n",
    "    if all(c in df.columns for c in ['close_stk', 'rank_mean_close_20_stk']):\n",
    "        df['alpha18_stk'] = safe_division(df['close_stk'], df['rank_mean_close_20_stk'])\n",
    "\n",
    "    # Alpha36: correlation(vol, close, 5) + correlation(vol, open, 5) | é‡ä»·ç›¸å…³æ€§\n",
    "    print(\"  - è®¡ç®— Alpha36...\")\n",
    "    if all(c in df.columns for c in ['vol', 'close', 'open']):\n",
    "        def calc_alpha36(x):\n",
    "             corr_close = rolling_corr(x['vol'], x['close'], 5, get_min_periods(5))\n",
    "             corr_open = rolling_corr(x['vol'], x['open'], 5, get_min_periods(5))\n",
    "             return corr_close.add(corr_open, fill_value=0) # Handle potential NaNs\n",
    "        df['alpha36'] = df.groupby('code', group_keys=False).apply(calc_alpha36)\n",
    "    if all(c in df.columns for c in ['vol_stk', 'close_stk', 'open_stk']):\n",
    "        def calc_alpha36_stk(x):\n",
    "             corr_close = rolling_corr(x['vol_stk'], x['close_stk'], 5, get_min_periods(5))\n",
    "             corr_open = rolling_corr(x['vol_stk'], x['open_stk'], 5, get_min_periods(5))\n",
    "             return corr_close.add(corr_open, fill_value=0)\n",
    "        df['alpha36_stk'] = df.groupby('code', group_keys=False).apply(calc_alpha36_stk)\n",
    "\n",
    "    # Alpha89: (days since high) / (days since low + eps) (Corrected) | åè½¬æ—¶æœº (é«˜ç‚¹/ä½ç‚¹è¿œè¿‘å¯¹æ¯”)\n",
    "    # ä½¿ç”¨ä¿®æ­£åçš„ nanargmin é€»è¾‘!\n",
    "    print(\"  - è®¡ç®— Alpha89 (ä¿®æ­£ç‰ˆ)...\")\n",
    "    df['argmin_close_30_idx_pos'] = df.groupby('code')['close'].transform(\n",
    "        lambda x: x.rolling(30, min_periods=get_min_periods(30, factor=0.5)).apply(\n",
    "            lambda s: (len(s) - 1) - np.nanargmin(s.to_numpy()) if not s.isnull().all() else np.nan, raw=False\n",
    "        )\n",
    "    )\n",
    "    if 'alpha83' in df.columns: # Check dependencies\n",
    "        df['alpha89'] = safe_division(df['alpha83'], df['argmin_close_30_idx_pos']) # Add epsilon in division? safe_division handles 0 denom\n",
    "    if 'close_stk' in df.columns:\n",
    "        df['argmin_close_30_idx_pos_stk'] = df.groupby('code')['close_stk'].transform(\n",
    "             lambda x: x.rolling(30, min_periods=get_min_periods(30, factor=0.5)).apply(\n",
    "                 lambda s: (len(s) - 1) - np.nanargmin(s.to_numpy()) if not s.isnull().all() else np.nan, raw=False\n",
    "             )\n",
    "        )\n",
    "        if 'alpha83_stk' in df.columns: # Check dependencies\n",
    "            df['alpha89_stk'] = safe_division(df['alpha83_stk'], df['argmin_close_30_idx_pos_stk'])\n",
    "\n",
    "    # Alpha65: correlation(rank(close), rank(vol), 6) | è¶‹åŠ¿ä¼´éšæ”¾é‡ï¼ˆæ­£ç›¸å…³ä¸ºå¼ºï¼‰\n",
    "    print(\"  - è®¡ç®— Alpha65...\")\n",
    "    if all(c in df.columns for c in ['rank_close', 'rank_vol']):\n",
    "         df['alpha65'] = df.groupby('code', group_keys=False).apply(\n",
    "             lambda x: rolling_corr(x['rank_close'], x['rank_vol'], 6, get_min_periods(6))\n",
    "         )\n",
    "    if all(c in df.columns for c in ['rank_close_stk', 'rank_vol_stk']):\n",
    "         df['alpha65_stk'] = df.groupby('code', group_keys=False).apply(\n",
    "             lambda x: rolling_corr(x['rank_close_stk'], x['rank_vol_stk'], 6, get_min_periods(6))\n",
    "         )\n",
    "\n",
    "    # Alpha76: -1 * ts_rank(correlation(close, vol, 10), 10) | é‡ä»·ç›¸å…³æ€§çš„æ—¶åºæ’å\n",
    "    print(\"  - è®¡ç®— Alpha76...\")\n",
    "    if all(c in df.columns for c in ['close', 'vol']):\n",
    "        # Calculate intermediate correlation first\n",
    "        df['corr_close_vol_10'] = df.groupby('code', group_keys=False).apply(\n",
    "            lambda x: rolling_corr(x['close'], x['vol'], 10, get_min_periods(10))\n",
    "        )\n",
    "        # Apply ts_rank using rolling apply with the helper function\n",
    "        df['alpha76'] = df.groupby('code')['corr_close_vol_10'].transform(\n",
    "             lambda x: -1 * x.rolling(10, min_periods=get_min_periods(10)).apply(ts_rank, raw=False, args=(10,)) # Pass window to helper\n",
    "        )\n",
    "    if all(c in df.columns for c in ['close_stk', 'vol_stk']):\n",
    "        df['corr_close_vol_10_stk'] = df.groupby('code', group_keys=False).apply(\n",
    "            lambda x: rolling_corr(x['close_stk'], x['vol_stk'], 10, get_min_periods(10))\n",
    "        )\n",
    "        df['alpha76_stk'] = df.groupby('code')['corr_close_vol_10_stk'].transform(\n",
    "             lambda x: -1 * x.rolling(10, min_periods=get_min_periods(10)).apply(ts_rank, raw=False, args=(10,))\n",
    "        )\n",
    "\n",
    "    # Alpha92: (delta(close, 5)/close) * vol | å›è°ƒå¹…åº¦ä¸é‡èƒ½ç»“åˆåˆ¤æ–­æ´—ç›˜/åè½¬\n",
    "    print(\"  - è®¡ç®— Alpha92...\")\n",
    "    if all(c in df.columns for c in ['delta_close_5', 'close', 'vol']):\n",
    "        df['alpha92'] = safe_division(df['delta_close_5'], df['close']) * df['vol']\n",
    "    if all(c in df.columns for c in ['delta_close_5_stk', 'close_stk', 'vol_stk']):\n",
    "        df['alpha92_stk'] = safe_division(df['delta_close_5_stk'], df['close_stk']) * df['vol_stk']\n",
    "\n",
    "    # Alpha99: -1 * ts_rank(cov(rank(close), rank(vol), 5), 5) | é‡ä»·åæ–¹å·®çš„æ—¶åºæ’å\n",
    "    print(\"  - è®¡ç®— Alpha99...\")\n",
    "    if all(c in df.columns for c in ['rank_close', 'rank_vol']):\n",
    "         df['cov_rank_close_vol_5'] = df.groupby('code', group_keys=False).apply(\n",
    "             lambda x: rolling_cov(x['rank_close'], x['rank_vol'], 5, get_min_periods(5))\n",
    "         )\n",
    "         df['alpha99'] = df.groupby('code')['cov_rank_close_vol_5'].transform(\n",
    "              lambda x: -1 * x.rolling(5, min_periods=get_min_periods(5)).apply(ts_rank, raw=False, args=(5,))\n",
    "         )\n",
    "    if all(c in df.columns for c in ['rank_close_stk', 'rank_vol_stk']):\n",
    "         df['cov_rank_close_vol_5_stk'] = df.groupby('code', group_keys=False).apply(\n",
    "             lambda x: rolling_cov(x['rank_close_stk'], x['rank_vol_stk'], 5, get_min_periods(5))\n",
    "         )\n",
    "         df['alpha99_stk'] = df.groupby('code')['cov_rank_close_vol_5_stk'].transform(\n",
    "              lambda x: -1 * x.rolling(5, min_periods=get_min_periods(5)).apply(ts_rank, raw=False, args=(5,))\n",
    "         )\n",
    "\n",
    "\n",
    "    # === X. è‚¡ç¥¨ä¸è½¬å€ºè”åŠ¨å› å­ ===\n",
    "    print(\"è®¡ç®—: X. è‚¡ç¥¨ä¸è½¬å€ºè”åŠ¨å› å­\")\n",
    "    if 'pct_chg' in df.columns and 'pct_chg_stk' in df.columns:\n",
    "        print(\"  - è®¡ç®—æ—¥å†…è”åŠ¨...\")\n",
    "        # è‚¡ç¥¨æ¶¨ï¼Œè½¬å€ºå¹³ (Stock Up, Bond Flat)\n",
    "        df['stk_up_bond_flat'] = ((df['pct_chg_stk'] > 0.03) & (df['pct_chg'] < 0.01)).astype(int)\n",
    "        # è‚¡ç¥¨è·Œï¼Œè½¬å€ºå¼± (Stock Down, Bond Weak)\n",
    "        df['stk_down_bond_weak'] = ((df['pct_chg_stk'] < -0.03) & (df['pct_chg'] < df['pct_chg_stk'])).astype(int)\n",
    "\n",
    "        # æ»åå˜é‡ (Lagged variables)\n",
    "        df['pct_chg_stk_lag1'] = df.groupby('code')['pct_chg_stk'].shift(1)\n",
    "        df['pct_chg_stk_lag2'] = df.groupby('code')['pct_chg_stk'].shift(2)\n",
    "\n",
    "        # è½¬å€ºæŒæœ‰ï¼Œè‚¡ç¥¨åå¼¹ (Bond Hold, Stock Rebound)\n",
    "        if 'pct_chg_stk_lag1' in df.columns:\n",
    "            df['bond_hold_stk_rebound'] = ((df['pct_chg_stk_lag1'] < -0.03) & (df['pct_chg_stk'] > 0.01) & (df['pct_chg'] > 0.005)).astype(int)\n",
    "        # è‚¡ç¥¨å…ˆè·Œåæ¶¨ (Stock Down then Up)\n",
    "        if 'pct_chg_stk_lag2' in df.columns:\n",
    "            df['stk_down_then_up'] = ((df['pct_chg_stk_lag2'] < -0.03) & (df['pct_chg_stk'] > 0.02)).astype(int)\n",
    "\n",
    "        # è½¬å€ºåå¼¹æ ‡å¿— (Bond Rebound Flag)\n",
    "        df['bond_rebound'] = (df['pct_chg'] > 0.01).astype(int)\n",
    "        # è½¬å€ºè·Ÿéšè‚¡ç¥¨åå¼¹ (Bond Follows Stock Rebound)\n",
    "        if 'stk_down_then_up' in df.columns: # Check dependency\n",
    "            df['bond_follow_stk_rebound'] = ((df['stk_down_then_up'] == 1) & (df['bond_rebound'] == 1)).astype(int)\n",
    "\n",
    "        print(\"  - è®¡ç®—å¤šæ—¥è”åŠ¨ (æ»æ¶¨)...\")\n",
    "        # Multi-day linkage (using mean returns calculated in section IV)\n",
    "        # æ»æ¶¨å› å­ï¼šè‚¡ç¥¨æ¶¨ï¼Œè½¬å€ºä¸æ¶¨ï¼ˆè¡¥æ¶¨æ½œåŠ›ï¼‰(Stock up, Bond flat - multi-day)\n",
    "        # Using Script 3 / 2 thresholds - Document the choice\n",
    "        if all(c in df.columns for c in ['stk_ret_mean_3', 'bond_ret_mean_3']):\n",
    "             df['stk_up_bond_flat_3'] = ((df['stk_ret_mean_3'] > 0.03) & (df['bond_ret_mean_3'] < 0.01)).astype(int) # Using 0.03/0.01 threshold\n",
    "        if all(c in df.columns for c in ['stk_ret_mean_5', 'bond_ret_mean_5']):\n",
    "             df['stk_up_bond_flat_5'] = ((df['stk_ret_mean_5'] > 0.05) & (df['bond_ret_mean_5'] < 0.01)).astype(int) # Using 0.05/0.01 threshold\n",
    "\n",
    "\n",
    "    # === XI. æ¨ªçºµå‘èƒŒç¦»å› å­ï¼ˆè‚¡ç¥¨ä¸è½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: XI. æ¨ªçºµå‘èƒŒç¦»å› å­ï¼ˆè‚¡ç¥¨ä¸è½¬å€ºï¼‰\")\n",
    "    print(\"  - è®¡ç®—æ¨ªå‘èƒŒç¦» (è‚¡ç¥¨ vs è½¬å€º)...\")\n",
    "     # Horizontal Deviation: Bond Return vs Stock Return (Mean)\n",
    "    for win in [3, 5, 10]:\n",
    "        # Deviation using mean returns\n",
    "        if f'bond_ret_mean_{win}' in df.columns and f'stk_ret_mean_{win}' in df.columns:\n",
    "             # æ¨ªå‘èƒŒç¦»å€¼ (Bond mean return - Stock mean return)\n",
    "            df[f'dev_bond_vs_stk_{win}'] = df[f'bond_ret_mean_{win}'] - df[f'stk_ret_mean_{win}']\n",
    "        # Rank difference (requires cumulative returns calculated in IV)\n",
    "        # å¯è½¬å€º vs è‚¡ç¥¨ æ”¶ç›Šå¼ºå¼±æ¨ªå‘å¯¹æ¯”ï¼ˆç›¸å¯¹å¼ºåº¦æ’åï¼‰\n",
    "        if f'pct_chg_{win}' in df.columns :\n",
    "             df[f'cb_ret_rank_{win}'] = df.groupby('trade_date')[f'pct_chg_{win}'].rank() # Rank based on cumulative return\n",
    "        if f'pct_chg_stk_{win}' in df.columns:\n",
    "             df[f'stk_ret_rank_{win}'] = df.groupby('trade_date')[f'pct_chg_stk_{win}'].rank()\n",
    "        if f'cb_ret_rank_{win}' in df.columns and f'stk_ret_rank_{win}' in df.columns:\n",
    "             # æ’åå·®å€¼ (Rank difference)\n",
    "             df[f'cb_vs_stk_ret_rank_diff_{win}'] = df[f'cb_ret_rank_{win}'] - df[f'stk_ret_rank_{win}']\n",
    "\n",
    "    print(\"  - è®¡ç®—çºµå‘èƒŒç¦» (è‡ªèº«å†å²å¯¹æ¯”)...\")\n",
    "    # Vertical Deviation: Short-term vs Long-term Mean Return\n",
    "    # Longer term means needed\n",
    "    if 'pct_chg' in df.columns:\n",
    "        df['bond_ret_mean_20'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(20, min_periods=get_min_periods(20)).mean())\n",
    "        df['bond_ret_mean_30'] = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(30, min_periods=get_min_periods(30)).mean())\n",
    "        # è½¬å€ºè‡ªèº«å†å²åç¦»ï¼ˆçºµå‘ï¼‰ï¼šè¿‘æœŸè¡¨ç° vs é•¿æœŸå‡å€¼\n",
    "        if 'bond_ret_mean_3' in df.columns and 'bond_ret_mean_20' in df.columns:\n",
    "            df['dev_bond_short3_long20'] = df['bond_ret_mean_3'] - df['bond_ret_mean_20']\n",
    "        if 'bond_ret_mean_5' in df.columns and 'bond_ret_mean_30' in df.columns:\n",
    "            df['dev_bond_short5_long30'] = df['bond_ret_mean_5'] - df['bond_ret_mean_30']\n",
    "\n",
    "    if 'pct_chg_stk' in df.columns:\n",
    "        df['stk_ret_mean_20'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(20, min_periods=get_min_periods(20)).mean())\n",
    "        df['stk_ret_mean_30'] = df.groupby('code')['pct_chg_stk'].transform(lambda x: x.rolling(30, min_periods=get_min_periods(30)).mean())\n",
    "        # æ­£è‚¡è‡ªèº«å†å²åç¦»ï¼ˆçºµå‘ï¼‰ï¼šæœ€è¿‘å‡ å¤©è¡¨ç° vs è‡ªèº«é•¿æœŸå‡å€¼\n",
    "        if 'stk_ret_mean_3' in df.columns and 'stk_ret_mean_20' in df.columns:\n",
    "            df['dev_stk_short3_long20'] = df['stk_ret_mean_3'] - df['stk_ret_mean_20']\n",
    "        if 'stk_ret_mean_5' in df.columns and 'stk_ret_mean_30' in df.columns:\n",
    "            df['dev_stk_short5_long30'] = df['stk_ret_mean_5'] - df['stk_ret_mean_30']\n",
    "\n",
    "\n",
    "    # === XII. é£é™©ä¸å›æ’¤ç›¸å…³å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: XII. é£é™©ä¸å›æ’¤ç›¸å…³å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if 'close' in df.columns:\n",
    "        print(\"  - è®¡ç®—ä½ç‚¹è·ç¦»/æ ‡å‡†å·®/å›æ’¤...\")\n",
    "        # å¯è½¬å€ºè·è¿‘æœŸä½ç‚¹è·ç¦» (Distance from recent low)\n",
    "        df['cb_low_5'] = df.groupby('code')['close'].transform(lambda x: x.rolling(5, min_periods=get_min_periods(5)).min())\n",
    "        # è·ç¦»ä½ç‚¹å¹…åº¦ (Deviation from 5-day low)\n",
    "        df['cb_dev_from_low_5'] = safe_division(df['close'] - df['cb_low_5'], df['cb_low_5']) # è¶Šå¤§è¯´æ˜å·²åå¼¹\n",
    "        # å¯è½¬å€ºä»·æ ¼æ³¢åŠ¨æ€§ï¼ˆæ ‡å‡†å·®ï¼‰(Price Volatility - std dev)\n",
    "        df['cb_close_std_5'] = df.groupby('code')['close'].transform(lambda x: x.rolling(5, min_periods=get_min_periods_std(5)).std())\n",
    "        # è¿‘æœŸé«˜ç‚¹ (Recent High)\n",
    "        df['cb_high_5'] = df.groupby('code')['close'].transform(lambda x: x.rolling(5, min_periods=get_min_periods(5)).max())\n",
    "        # å¯è½¬å€ºè¿‘5æ—¥æœ€å¤§å›æ’¤ (Max Drawdown from 5-day high)\n",
    "        df['cb_drawdown_5'] = safe_division(df['close'] - df['cb_high_5'], df['cb_high_5']) # è¶Šè´Ÿè¯´æ˜é£é™©é‡Šæ”¾\n",
    "\n",
    "    if 'pct_chg' in df.columns:\n",
    "        print(\"  - è®¡ç®—ä¸‹è·Œé£é™©é¢„ä¼°...\")\n",
    "        # ä¸‹è·Œé£é™©é¢„ä¼°ï¼ˆå†å²æ¦‚ç‡ Ã— å¹…åº¦ï¼‰(Downside Risk Estimate: Prob * Amplitude)\n",
    "        df['cb_ret_lag1'] = df.groupby('code')['pct_chg'].shift(1) # Lagged return\n",
    "        df['cb_fall_flag'] = (df['cb_ret_lag1'] < 0).astype(int) # Fall flag\n",
    "        # è¿‘10æ—¥ä¸‹è·Œé¢‘ç‡ (Fall Frequency - 10 days)\n",
    "        df['cb_fall_freq_10'] = df.groupby('code')['cb_fall_flag'].transform(lambda x: x.rolling(10, min_periods=get_min_periods(10)).mean())\n",
    "        # è¿‘10æ—¥å¹³å‡ä¸‹è·Œå¹…åº¦ (Mean Fall Amplitude - 10 days)\n",
    "        df['cb_fall_amp_10'] = df.groupby('code')['cb_ret_lag1'].transform(\n",
    "            lambda x: x.rolling(10, min_periods=get_min_periods(10)).apply(lambda s: s[s < 0].mean() if (s < 0).any() else 0, raw=True)\n",
    "        )\n",
    "        # ä¸‹è·Œé£é™©ä¼°è®¡å€¼ (Estimated Drawdown Probability * Amplitude)\n",
    "        df['cb_dd_prob_estimate'] = df['cb_fall_freq_10'] * df['cb_fall_amp_10'] # è¶Šè´Ÿé£é™©è¶Šå¤§\n",
    "\n",
    "\n",
    "    # === XIII. éœ‡è¡æ”¶æ•›ç±»å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: XIII. éœ‡è¡æ”¶æ•›ç±»å› å­ï¼ˆè½¬å€ºï¼‰ - æ¶¨ä¸åŠ¨/è·Œä¸åŠ¨\")\n",
    "    if all(c in df.columns for c in ['high', 'low', 'close', 'open', 'pre_close']):\n",
    "        print(\"  - è®¡ç®— ATR/æŒ¯å¹…/ä»·æ ¼æ³¢åŠ¨ æ”¶æ•›...\")\n",
    "        df['range_hl'] = df['high'] - df['low'] # Reusable range\n",
    "        # ATR (Average True Range) - å¹³å‡çœŸå®æ³¢å¹…\n",
    "        df['atr_5'] = df.groupby('code')['range_hl'].transform(lambda x: x.rolling(5, min_periods=get_min_periods(5)).mean())\n",
    "        df['atr_10'] = df.groupby('code')['range_hl'].transform(lambda x: x.rolling(10, min_periods=get_min_periods(10)).mean())\n",
    "        df['atr_20'] = df.groupby('code')['range_hl'].transform(lambda x: x.rolling(20, min_periods=get_min_periods(20)).mean())\n",
    "        # ATR è¡°å‡ç‡ (ATR Decay Ratio 5/20)\n",
    "        df['atr_decay_5_20'] = safe_division(df['atr_5'], df['atr_20']) # è¿œå°äº1ä¸ºæ”¶æ•›\n",
    "        # ATR è¡°å‡ç‡ (ATR Decay Ratio 5/10)\n",
    "        df['atr_decay_5_10'] = safe_division(df['atr_5'], df['atr_10']) # è¶‹è¿‘ 1 ä¸ºéœ‡è¡ï¼Œè¿œå°äº 1 ä¸ºæ”¶æ•›\n",
    "\n",
    "        # # æŒ¯å¹…è¡°å‡ï¼ˆç­‰åŒ ATR è¡°å‡ï¼‰ - Redundant if using ATR\n",
    "        # df['zhengfu_decay_5_20'] = safe_division(df['atr_5'], df['atr_20'])\n",
    "        # # é«˜ä½ä»·å·®æ¯”å‡å€¼ ï¼ˆç­‰åŒ ATR è¡°å‡ï¼‰- Redundant if using ATR\n",
    "        # df['range_ratio_5_20'] = safe_division(df['atr_5'], df['atr_20'])\n",
    "\n",
    "        # æ”¶ç›˜ä»·æ³¢åŠ¨ç‡ç¼©å°ï¼ˆæ ‡å‡†å·®ä¸‹é™ï¼‰(Close Std Dev Shrink Ratio)\n",
    "        # cb_close_std_5 calculated in Sec XII\n",
    "        df['close_std_10'] = df.groupby('code')['close'].transform(lambda x: x.rolling(10, min_periods=get_min_periods_std(10)).std())\n",
    "        if 'cb_close_std_5' in df.columns and 'close_std_10' in df.columns:\n",
    "             df['vol_shrink_ratio'] = safe_division(df['cb_close_std_5'], df['close_std_10']) # å°äº1è¯´æ˜éœ‡è¡æ”¶æ•›\n",
    "\n",
    "        print(\"  - è®¡ç®— Kçº¿å®ä½“/å½±çº¿/åå­—æ˜Ÿ ç‰¹å¾...\")\n",
    "        df['body_abs'] = (df['close'] - df['open']).abs() # Kçº¿å®ä½“ç»å¯¹å€¼\n",
    "        # Kçº¿å®ä½“ç›¸å¯¹æ˜¨æ—¥æ”¶ç›˜ä»·æ¯”ä¾‹ (Body size relative to previous close)\n",
    "        df['body_pct'] = safe_division(df['body_abs'], df['pre_close'])\n",
    "        # è¿‘5æ—¥å¹³å‡å®ä½“æ¯”ä¾‹ (Mean body pct over 5 days)\n",
    "        df['body_pct_mean_5'] = df.groupby('code')['body_pct'].transform(lambda x: x.rolling(5, min_periods=get_min_periods(5)).mean())\n",
    "\n",
    "        df['shadow'] = df['range_hl'] - df['body_abs'] # ä¸Šä¸‹å½±çº¿æ€»é•¿åº¦\n",
    "        # å½±çº¿ç›¸å¯¹æ˜¨æ—¥æ”¶ç›˜ä»·æ¯”ä¾‹ (Shadow size relative to previous close)\n",
    "        df['shadow_ratio'] = safe_division(df['shadow'], df['pre_close'])\n",
    "        # è¿‘5æ—¥å¹³å‡å½±çº¿æ¯”ä¾‹ (Mean shadow ratio over 5 days)\n",
    "        df['shadow_mean_5'] = df.groupby('code')['shadow_ratio'].transform(lambda x: x.rolling(5, min_periods=get_min_periods(5)).mean())\n",
    "\n",
    "        # æå°å®ä½“ + é•¿å½±çº¿ç»“æ„ (Small Body to Shadow Ratio)\n",
    "        df['small_body_shadow_ratio'] = safe_division(df['shadow'], df['body_abs'], default=100) # Assign large number if body is zero\n",
    "\n",
    "        # åå­—æ˜Ÿåˆ¤æ–­ (Doji Flag)\n",
    "        df['is_doji'] = safe_division(df['body_abs'], df['range_hl']) < 0.15 # å®ä½“å°äºæŒ¯å¹…15%\n",
    "        # è¿‘5æ—¥åå­—æ˜Ÿé¢‘ç‡ (Doji Frequency over 5 days)\n",
    "        df['doji_ratio_5'] = df.groupby('code')['is_doji'].transform(lambda x: x.rolling(5, min_periods=get_min_periods(5)).mean())\n",
    "\n",
    "\n",
    "    # === XIV. è„‰å†²ä¸åŠ¨èƒ½å› å­ï¼ˆè½¬å€ºï¼‰ ===\n",
    "    print(\"è®¡ç®—: XIV. è„‰å†²ä¸åŠ¨èƒ½å› å­ï¼ˆè½¬å€ºï¼‰\")\n",
    "    if all(c in df.columns for c in ['high', 'pre_close', 'pct_chg', 'vol', 'close', 'low', 'open']):\n",
    "        # --- Combine High Jump calculations ---\n",
    "        print(\"  - è®¡ç®—é«˜è„‰å†²ç»Ÿè®¡ (å¤šé˜ˆå€¼/å¤šçª—å£)...\")\n",
    "        thresholds = [0.015, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "        windows_s1 = [20, 120, 250, 500] # Windows for count/mean/std (from Script 1)\n",
    "        window_s2_score = 20           # Window for score (from Script 2/3)\n",
    "\n",
    "        for thres in thresholds:\n",
    "            thres_name = int(thres*1000)\n",
    "            # Flag (use clean name from Script 3, consistent bool/int)\n",
    "            flag_col = f'high_jump_{thres_name}_flag' # Renamed from Script 3's non-flag name\n",
    "            df[flag_col] = ((safe_division(df['high'], df['pre_close']) - 1) > thres) # Boolean initially\n",
    "\n",
    "            # Calculate count, mean, std using Script 1's helper for its windows\n",
    "            for win in windows_s1:\n",
    "                 # Apply helper function using transform or apply\n",
    "                 def calc_hj_stats_s1(group):\n",
    "                     flag = group[flag_col]\n",
    "                     pct = group['pct_chg']\n",
    "                     count_s, mean_s, std_s = rolling_high_jump_stats(flag, pct, win) # Helper returns calculated series\n",
    "                     return pd.DataFrame({\n",
    "                         f'hj_count_{thres_name}_{win}': count_s,      # é«˜è„‰å†²æ¬¡æ•°\n",
    "                         f'hj_mean_{thres_name}_{win}': mean_s,       # å¹³å‡è„‰å†²å¹…åº¦\n",
    "                         f'hj_std_{thres_name}_{win}': std_s         # è„‰å†²å¹…åº¦æ ‡å‡†å·®\n",
    "                     })\n",
    "                 # Use groupby().apply() as the helper returns a DataFrame\n",
    "                 stats_df_s1 = df.groupby('code', group_keys=False).apply(calc_hj_stats_s1)\n",
    "                 df = df.join(stats_df_s1) # Join results back\n",
    "\n",
    "            # Calculate score using Script 2/3's logic for its window (win=20)\n",
    "            mean_col_s2 = f'hj_mean_{thres_name}_{window_s2_score}'\n",
    "            count_col_s2 = f'hj_count_{thres_name}_{window_s2_score}' # Should exist from loop above\n",
    "\n",
    "            if count_col_s2 in df.columns and mean_col_s2 in df.columns:\n",
    "                 # è„‰å†²å¾—åˆ† = æ¬¡æ•° * å¹³å‡å¹…åº¦ (High Jump Score = Count * Mean Amplitude)\n",
    "                 df[f'score_high_jump_{thres_name}_{window_s2_score}'] = df[count_col_s2] * df[mean_col_s2]\n",
    "            else:\n",
    "                 # This might happen if window_s2_score is not in windows_s1\n",
    "                 # Recalculate count/mean for window_s2_score if necessary\n",
    "                 if window_s2_score not in windows_s1:\n",
    "                     print(f\"  - Calculating intermediate count/mean for score window {window_s2_score}...\")\n",
    "                     def calc_hj_stats_s2_score(group):\n",
    "                         flag = group[flag_col]\n",
    "                         pct = group['pct_chg']\n",
    "                         count_s, mean_s, _ = rolling_high_jump_stats(flag, pct, window_s2_score)\n",
    "                         return pd.DataFrame({ count_col_s2: count_s, mean_col_s2: mean_s })\n",
    "                     stats_df_s2 = df.groupby('code', group_keys=False).apply(calc_hj_stats_s2_score)\n",
    "                     df = df.join(stats_df_s2)\n",
    "                     # Try calculating score again\n",
    "                     if count_col_s2 in df.columns and mean_col_s2 in df.columns:\n",
    "                         df[f'score_high_jump_{thres_name}_{window_s2_score}'] = df[count_col_s2] * df[mean_col_s2]\n",
    "                     else:\n",
    "                          print(f\"  è­¦å‘Š: æ— æ³•è®¡ç®— score_high_jump_{thres_name}_{window_s2_score}ï¼Œç¼ºå°‘ç»„ä»¶ã€‚\")\n",
    "                 else:\n",
    "                      print(f\"  è­¦å‘Š: æ— æ³•è®¡ç®— score_high_jump_{thres_name}_{window_s2_score}ï¼Œç»„ä»¶ {count_col_s2} æˆ– {mean_col_s2} ä¸å­˜åœ¨ã€‚\")\n",
    "\n",
    "            # Convert flag back to int if needed for other factors, or keep bool\n",
    "            df[flag_col] = df[flag_col].astype(int)\n",
    "\n",
    "        # --- Calculate other impulse factors (combine sources) ---\n",
    "        print(\"  - è®¡ç®—å…¶ä»–è„‰å†²æŒ‡æ ‡...\")\n",
    "        # Z-score (æ¶¨å¹… Z-score)\n",
    "        pct_mean_20 = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(20, min_periods=get_min_periods(20)).mean())\n",
    "        pct_std_20 = df.groupby('code')['pct_chg'].transform(lambda x: x.rolling(20, min_periods=get_min_periods_std(20)).std())\n",
    "        df['zscore_pctchg_20'] = safe_division(df['pct_chg'] - pct_mean_20, pct_std_20)\n",
    "\n",
    "        # Volume spike & decay (æˆäº¤é‡è„‰å†²ä¸æ³¢åŠ¨æ”¶æ•›)\n",
    "        df['vol_ma20'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(20, min_periods=get_min_periods(20)).mean())\n",
    "        df['vol_spike_ratio'] = safe_division(df['vol'], df['vol_ma20'], default=1.0) # é‡æ¯” (å½“æ—¥/20æ—¥å‡é‡)\n",
    "        df['vol_std_5'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(5, min_periods=get_min_periods_std(5)).std())\n",
    "        df['vol_std_20'] = df.groupby('code')['vol'].transform(lambda x: x.rolling(20, min_periods=get_min_periods_std(20)).std())\n",
    "        df['vol_std_decay'] = safe_division(df['vol_std_5'], df['vol_std_20']) # æˆäº¤é‡æ³¢åŠ¨æ”¶æ•›\n",
    "\n",
    "        # Open gap stats (è·³ç©ºå¹…åº¦ç»Ÿè®¡)\n",
    "        if 'open_jump' in df.columns: # Calculated in Sec VII\n",
    "             for n in [5, 10]:\n",
    "                 df[f'open_gap_mean_{n}'] = df.groupby('code')['open_jump'].transform(lambda x: x.rolling(n, min_periods=get_min_periods(n)).mean())\n",
    "                 df[f'open_gap_max_{n}'] = df.groupby('code')['open_jump'].transform(lambda x: x.rolling(n, min_periods=get_min_periods(n)).max())\n",
    "\n",
    "        # Jump ATR (N æ—¥è„‰å†² ATRï¼šé«˜ç‚¹è¿œç¦»å‡å€¼)\n",
    "        for n in [3, 5, 10]:\n",
    "            close_mean_n = df.groupby('code')['close'].transform(lambda x: x.rolling(n, min_periods=get_min_periods(n)).mean())\n",
    "            close_std_n = df.groupby('code')['close'].transform(lambda x: x.rolling(n, min_periods=get_min_periods_std(n)).std())\n",
    "            df[f'jump_atr_{n}'] = safe_division(df['high'] - close_mean_n, close_std_n)\n",
    "\n",
    "        # Range jump potential (æ—¥å†…æŒ¯å¹…ç›¸å¯¹ ATR æ½œåŠ›)\n",
    "        if 'range_hl' in df.columns and 'atr_5' in df.columns: # Calculated in XIII\n",
    "            df['range_today'] = df['range_hl'] # Alias for clarity\n",
    "            df['range_atr_5'] = safe_division(df['range_today'], df['atr_5']) # å½“æ—¥æŒ¯å¹… / 5æ—¥å‡å¹…\n",
    "            df['range_jump_potential'] = (df['range_atr_5'] > 1.5).astype(int) # æŒ¯å¹…æ”¾å¤§è¶…è¿‡1.5å€\n",
    "\n",
    "        # Gap and Go flag (è·³ç©ºé«˜å¼€é«˜èµ°)\n",
    "        # Use 2% threshold from Script 3/2\n",
    "        df['gap_and_go_flag'] = ((safe_division(df['open'], df['pre_close']) - 1 > 0.02) & (df['close'] > df['open'])).astype(int)\n",
    "\n",
    "\n",
    "    # === XV. ä¸‹è·Œç»Ÿè®¡ä¸éŸ§æ€§å› å­ï¼ˆè½¬å€ºï¼‰ - è·Œä¸åŠ¨ ===\n",
    "    print(\"è®¡ç®—: XV. ä¸‹è·Œç»Ÿè®¡ä¸éŸ§æ€§å› å­ï¼ˆè½¬å€ºï¼‰ - è·Œä¸åŠ¨\")\n",
    "    if 'pct_chg' in df.columns:\n",
    "        # --- Combine Downside calculations ---\n",
    "        print(\"  - è®¡ç®—é•¿çª—å£ä¸‹è·Œç»Ÿè®¡ (é¢‘ç‡/å¹…åº¦/æ ‡å‡†å·®)...\")\n",
    "        windows_s1_down = [20, 60, 120, 250] # From Script 1\n",
    "        for win in windows_s1_down:\n",
    "            def calc_downside_s1(group):\n",
    "                 pct = group['pct_chg']\n",
    "                 freq_s, mean_s, std_s = rolling_downside_stats(pct, win) # Use helper\n",
    "                 return pd.DataFrame({\n",
    "                     f'down_freq_{win}': freq_s,         # ä¸‹è·Œé¢‘ç‡\n",
    "                     f'down_amp_mean_{win}': mean_s,    # å¹³å‡ä¸‹è·Œå¹…åº¦\n",
    "                     f'down_amp_std_{win}': std_s      # ä¸‹è·Œå¹…åº¦æ ‡å‡†å·®\n",
    "                 })\n",
    "            # Use groupby().apply() as helper returns DataFrame\n",
    "            stats_df_down_s1 = df.groupby('code', group_keys=False).apply(calc_downside_s1)\n",
    "            df = df.join(stats_df_down_s1)\n",
    "\n",
    "        print(\"  - è®¡ç®—çŸ­çª—å£'è·Œä¸åŠ¨'è¯„åˆ†...\")\n",
    "        windows_s23_down = [5, 10] # From Script 2/3\n",
    "        for win in windows_s23_down:\n",
    "            # çŸ­æœŸä¸‹è·Œé¢‘ç‡ (Short-term Down Frequency)\n",
    "            df[f'short_down_freq_{win}'] = df.groupby('code')['pct_chg'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=get_min_periods(win)).apply(lambda s: (s < 0).mean(), raw=True)\n",
    "            )\n",
    "            # çŸ­æœŸå¹³å‡ä¸‹è·Œå¹…åº¦ (Short-term Mean Down Amplitude)\n",
    "            df[f'short_down_amp_{win}'] = df.groupby('code')['pct_chg'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=get_min_periods(win)).apply(lambda s: s[s < 0].mean() if (s < 0).any() else 0, raw=True)\n",
    "            )\n",
    "            # è·Œä¸åŠ¨è¯„åˆ† = (1 - ä¸‹è·Œé¢‘ç‡) * (-ä¸‹è·Œå¹…åº¦) (No-Fall Score)\n",
    "            df[f'no_fall_score_{win}'] = (1 - df[f'short_down_freq_{win}']) * (-df[f'short_down_amp_{win}']) # è¶Šå¤§è¶Šâ€œè·Œä¸åŠ¨â€\n",
    "\n",
    "        # Note: cb_dd_prob_estimate (using lag1) is calculated in Section XII.\n",
    "\n",
    "\n",
    "    # === XVI. Kçº¿ç»“æ„è¿ç»­æ€§ ===\n",
    "    print(\"è®¡ç®—: XVI. Kçº¿ç»“æ„è¿ç»­æ€§\")\n",
    "    if all(c in df.columns for c in ['close', 'open']):\n",
    "        print(\"  - è®¡ç®—Kçº¿æ–¹å‘åè½¬ç‡...\")\n",
    "        # Kçº¿æ–¹å‘ (-1: é˜´çº¿, 0: åå­—, 1: é˜³çº¿) (K-line Direction)\n",
    "        df['kline_direction'] = np.sign(df['close'] - df['open'])\n",
    "        # ä¸Šä¸€æ—¥Kçº¿æ–¹å‘ (Previous Day K-line Direction)\n",
    "        df['kline_direction_shift1'] = df.groupby('code')['kline_direction'].shift(1)\n",
    "        if 'kline_direction_shift1' in df.columns: # Check dependency\n",
    "            # Kçº¿æ–¹å‘æ˜¯å¦åè½¬ (Did K-line direction flip?)\n",
    "            df['kline_flip'] = (df['kline_direction'] * df['kline_direction_shift1'] < 0).astype(int)\n",
    "             # è¿‘5æ—¥Kçº¿åè½¬é¢‘ç‡ (K-line Flip Ratio over 5 days)\n",
    "            df['kline_flip_ratio_5'] = df.groupby('code')['kline_flip'].transform(lambda x: x.rolling(5, min_periods=get_min_periods(5)).mean()) # å¤šä¸º 0 åˆ™è¶‹åŠ¿ç¨³å®š\n",
    "\n",
    "\n",
    "    # --- Final Cleanup & Optional Index Restore ---\n",
    "    print(\"æ­¥éª¤ XVII: æ¸…ç†ä¸´æ—¶åˆ—å’Œæ¢å¤ç´¢å¼• (å¦‚æœéœ€è¦)...\")\n",
    "    # Define columns to drop (intermediate calculations, ranks, etc.)\n",
    "    cols_to_drop = [\n",
    "        # Ranks from Alpha calculations\n",
    "        'rank_delta_close_10', 'rank_vol', 'rank_mean_close_20', 'rank_close',\n",
    "        'rank_delta_close_10_stk', 'rank_vol_stk', 'rank_mean_close_20_stk', 'rank_close_stk',\n",
    "        # Intermediate correlations/covariances for Alphas\n",
    "        'corr_close_vol_10', 'corr_close_vol_10_stk',\n",
    "        'cov_rank_close_vol_5', 'cov_rank_close_vol_5_stk',\n",
    "        # Intermediate position index for Alpha89\n",
    "        'argmin_close_30_idx_pos', 'argmin_close_30_idx_pos_stk',\n",
    "        # Intermediate turnover percent rank\n",
    "        'turnover_pct',\n",
    "         # Intermediate flags if not desired as final output (keep if used in strategy)\n",
    "        # 'high_jump_flag', 'low_gap_flag', 'is_doji', 'kline_flip', 'cb_fall_flag',\n",
    "        # *[f'high_jump_{int(t*1000)}_flag' for t in thresholds], # Keep flags?\n",
    "        # Temporary diff columns\n",
    "        'delta_close_1', 'delta_vol_1', 'delta_close_5', 'delta_close_10',\n",
    "        'delta_close_1_stk', 'delta_vol_1_stk', 'delta_close_5_stk', 'delta_close_10_stk',\n",
    "        # Temporary mean columns if only used for rank\n",
    "        'mean_close_20', 'mean_close_20_stk',\n",
    "        # Temporary lag columns\n",
    "        'pct_chg_stk_lag1', 'pct_chg_stk_lag2', 'cb_ret_lag1', 'kline_direction_shift1',\n",
    "        # Temporary range column\n",
    "        'range_hl', 'range_today',\n",
    "        # Temporary body/shadow columns if ratios are enough\n",
    "        'body_abs', 'shadow',\n",
    "         # Temporary K-line direction if only flip ratio is needed\n",
    "        'kline_direction'\n",
    "    ]\n",
    "    # Check which columns actually exist before attempting to drop\n",
    "    cols_exist = [col for col in cols_to_drop if col in df.columns]\n",
    "    if cols_exist:\n",
    "         print(f\"  - Dropping intermediate columns: {cols_exist}\")\n",
    "         df = df.drop(columns=cols_exist, errors='ignore') # Use errors='ignore' for safety\n",
    "\n",
    "    # Restore index if original was MultiIndex and requested\n",
    "    if restore_multiindex and is_multiindex_input:\n",
    "        print(\"  - æ¢å¤ MultiIndex ['code', 'trade_date']...\")\n",
    "        df = df.set_index(['code', 'trade_date'])\n",
    "    elif restore_multiindex and not is_multiindex_input:\n",
    "        print(\"  - è­¦å‘Š: åŸå§‹è¾“å…¥æ²¡æœ‰ MultiIndexï¼Œæ— æ³•æ¢å¤ã€‚\")\n",
    "\n",
    "\n",
    "    print(\"å› å­è®¡ç®—å®Œæˆã€‚\")\n",
    "    return df\n",
    "\n",
    "# --- Example Usage (éœ€è¦ä½ çš„æ•°æ®åŠ è½½é€»è¾‘) ---\n",
    "# print(\"Loading data...\")\n",
    "# # Replace with your actual data loading method\n",
    "# # df_raw = pd.read_parquet('your_data.parquet')\n",
    "# # OR use the structure from Script 1/2 if df is already loaded\n",
    "# df_raw = pd.DataFrame({ # Dummy Data Example\n",
    "#     'trade_date': pd.to_datetime(['2023-01-03', '2023-01-04'] * 2),\n",
    "#     'code': ['110001', '110001', '110002', '110002'],\n",
    "#     'open': [110, 112, 120, 119],\n",
    "#     'high': [113, 114, 122, 121],\n",
    "#     'low': [109, 111, 118, 118],\n",
    "#     'close': [112, 113, 121, 120],\n",
    "#     'pre_close': [109.5, 112, 119.5, 121],\n",
    "#     'vol': [10000, 12000, 8000, 9000],\n",
    "#     'pct_chg': [0.0228, 0.0089, 0.0126, -0.0083],\n",
    "#     'turnover': [0.5, 0.6, 0.4, 0.45],\n",
    "#     'remain_cap': [5.0, 5.0, 8.0, 8.0],\n",
    "#     'float_share': [1.0, 1.0, 1.5, 1.5],\n",
    "#     'close_stk': [10.0, 10.1, 12.5, 12.4],\n",
    "#     'open_stk': [9.9, 10.0, 12.4, 12.5],\n",
    "#     'high_stk': [10.2, 10.2, 12.6, 12.5],\n",
    "#     'low_stk': [9.8, 9.9, 12.3, 12.3],\n",
    "#     'vol_stk': [100000, 110000, 90000, 85000],\n",
    "#     'pct_chg_stk': [0.0100, 0.0099, 0.0080, -0.0080]\n",
    "# })\n",
    "# # Ensure enough data for rolling calculations in a real scenario\n",
    "# # ... add more rows/codes ...\n",
    "\n",
    "# print(\"Calculating factors...\")\n",
    "# df_factors = calculate_factors_merged_v3(df_raw.copy(), restore_multiindex=False) # Use copy\n",
    "\n",
    "# print(\"Factor calculation complete. Result shape:\", df_factors.shape)\n",
    "# print(\"Sample output:\")\n",
    "# print(df_factors.tail())\n",
    "# print(\"\\nColumns:\")\n",
    "# print(df_factors.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions (from previous code, ensure they are defined) ---\n",
    "def safe_division(numerator, denominator, default=np.nan):\n",
    "    \"\"\"Performs division, returning default value if denominator is zero, NaN, or invalid.\"\"\"\n",
    "    try:\n",
    "        if hasattr(numerator, '__iter__'): numerator = pd.to_numeric(numerator, errors='coerce')\n",
    "        if hasattr(denominator, '__iter__'):\n",
    "            denominator = pd.to_numeric(denominator, errors='coerce')\n",
    "            denominator = denominator.replace(0, np.nan)\n",
    "        elif isinstance(denominator, (int, float)) and denominator == 0:\n",
    "            denominator = np.nan\n",
    "\n",
    "        result = numerator / denominator\n",
    "\n",
    "        if hasattr(result, '__iter__'):\n",
    "            result = result.replace([np.inf, -np.inf], np.nan)\n",
    "            return result.fillna(default)\n",
    "        elif np.isinf(result) or np.isnan(result):\n",
    "            return default\n",
    "        else:\n",
    "            return result\n",
    "    except (TypeError, ValueError):\n",
    "        shape = getattr(numerator, 'shape', getattr(denominator, 'shape', None))\n",
    "        index = getattr(numerator, 'index', getattr(denominator, 'index', None))\n",
    "        if shape is not None:\n",
    "            return pd.Series(default, index=index, dtype=float)\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "def apply_ta_func(func, group, required_cols, **kwargs):\n",
    "    \"\"\"Safely applies a TA-Lib function to a group.\"\"\"\n",
    "    if group[required_cols].isnull().all().all() or len(group) < kwargs.get('timeperiod', 1)*1.5: # Basic check\n",
    "        return pd.Series(np.nan, index=group.index)\n",
    "    try:\n",
    "        # Prepare arguments for TA-Lib function\n",
    "        args = {col: group[col].astype(float) for col in required_cols}\n",
    "        return func(**args, **kwargs)\n",
    "    except Exception as e:\n",
    "        # print(f\"Error applying {func.__name__} to group: {e}\") # Optional: for debugging\n",
    "        return pd.Series(np.nan, index=group.index)\n",
    "\n",
    "# --- New Advanced Factor Calculation Function ---\n",
    "\n",
    "def apply_ta_func(group, func, required_cols, **kwargs): # group FIRST, then func, required_cols, **kwargs\n",
    "    \"\"\"Safely applies a TA-Lib function to a group.\"\"\"\n",
    "    min_len_needed = 1\n",
    "    if 'timeperiod' in kwargs:\n",
    "        min_len_needed = kwargs['timeperiod']\n",
    "    # Add extra buffer, e.g., 1.5 times the timeperiod, minimum 5 periods for robustness\n",
    "    min_len_needed = max(5, int(min_len_needed * 1.5))\n",
    "\n",
    "    # Check for sufficient non-null data points in required columns\n",
    "    sufficient_data = True\n",
    "    if len(group) < min_len_needed:\n",
    "        sufficient_data = False\n",
    "    else:\n",
    "        # Ensure enough *non-null* values exist in the rolling window equivalent\n",
    "        # This is a proxy check; the actual rolling window might have NaNs internally\n",
    "        non_null_counts = group[required_cols].iloc[-min_len_needed:].notnull().sum()\n",
    "        if any(count < kwargs.get('timeperiod', 1) for count in non_null_counts): # Check if any col has < timeperiod non-nulls\n",
    "             sufficient_data = False\n",
    "\n",
    "    # Handle cases with insufficient or all-NaN data\n",
    "    if not sufficient_data or group[required_cols].isnull().all().all():\n",
    "        return pd.Series(np.nan, index=group.index, dtype=float)\n",
    "\n",
    "    try:\n",
    "        # Prepare arguments for TA-Lib function - ensure they are float arrays\n",
    "        # TA-Lib functions generally expect numpy arrays of float64\n",
    "        args = {col: group[col].astype(float).to_numpy() for col in required_cols}\n",
    "\n",
    "        # Call the TA-Lib function using keyword arguments\n",
    "        result_array = func(**args, **kwargs)\n",
    "\n",
    "        # Return as a pandas Series aligned with the group's index\n",
    "        return pd.Series(result_array, index=group.index, dtype=float)\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error applying {func.__name__} to group {group.name if hasattr(group, 'name') else 'N/A'}: {e}\") # Debugging\n",
    "        return pd.Series(np.nan, index=group.index, dtype=float)\n",
    "\n",
    "def calculate_advanced_factors(df, restore_multiindex=False):\n",
    "    \"\"\"\n",
    "    è®¡ç®—è¡¥å……çš„é«˜çº§å¯è½¬å€ºè½®åŠ¨å› å­ï¼Œå‡è®¾åŸºç¡€å› å­å·²é€šè¿‡ calculate_factors è®¡ç®—ã€‚\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): åŒ…å«åŸºç¡€å› å­çš„ DataFrame (ç”± calculate_factors è¾“å‡º)ã€‚\n",
    "        restore_multiindex (bool): è‹¥ä¸ºTrue, åœ¨æœ«å°¾å°† ['code', 'trade_date'] è®¾å›ç´¢å¼•.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: æ·»åŠ äº†é«˜çº§å› å­åˆ—çš„DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"å¼€å§‹è®¡ç®—é«˜çº§å› å­...\")\n",
    "\n",
    "    required_base_cols = ['code', 'trade_date', 'close', 'high', 'low', 'open', 'pct_chg'] # Minimum required\n",
    "    if not all(col in df.columns or col in df.index.names for col in required_base_cols):\n",
    "         raise ValueError(f\"è¾“å…¥ DataFrame ç¼ºå°‘å¿…è¦çš„åŸºç¡€åˆ—: {required_base_cols}\")\n",
    "\n",
    "    # --- ç´¢å¼•å¤„ç† (ä¸ calculate_factors ç±»ä¼¼) ---\n",
    "    original_index = df.index\n",
    "    if isinstance(df.index, pd.MultiIndex) and all(name in df.index.names for name in ['code', 'trade_date']):\n",
    "        print(\"æ£€æµ‹åˆ° MultiIndexï¼Œæ­£åœ¨é‡ç½®...\")\n",
    "        df = df.reset_index()\n",
    "        is_multiindex_input = True\n",
    "    elif all(col in df.columns for col in ['code', 'trade_date']):\n",
    "        print(\"æ£€æµ‹åˆ°åˆ— 'code', 'trade_date'ã€‚\")\n",
    "        is_multiindex_input = False\n",
    "    else: # Should not happen if input comes from calculate_factors\n",
    "        raise ValueError(\"è¾“å…¥ DataFrame å¿…é¡»åŒ…å« 'code' å’Œ 'trade_date' åˆ—æˆ–ç´¢å¼•ã€‚\")\n",
    "\n",
    "    # ç¡®ä¿æ’åº (éå¸¸é‡è¦)\n",
    "    if not df.index.is_monotonic_increasing or not df.index.is_unique: # Check if previous sort might be disturbed\n",
    "       df = df.sort_values(by=['code', 'trade_date']).copy()\n",
    "    else:\n",
    "       df = df.copy() # Still make a copy to avoid SettingWithCopyWarning\n",
    "    # --- ç»“æŸç´¢å¼•å¤„ç† ---\n",
    "\n",
    "\n",
    "    # === XVI.b ç§»åŠ¨å¹³å‡çº¿ç³»ç»Ÿ (MA & EMA) åŠå…¶åç¦»åº¦ ===\n",
    "    print(\"è®¡ç®—: XVI.b ç§»åŠ¨å¹³å‡çº¿ç³»ç»Ÿ (MA & EMA) åŠå…¶åç¦»åº¦\")\n",
    "    ma_windows = [5, 10, 20, 30, 50, 60, 120, 250, 500]\n",
    "    if 'close' in df.columns:\n",
    "        print(\"  - è®¡ç®—è½¬å€º MA/EMA åŠåç¦»...\")\n",
    "        for n in ma_windows:\n",
    "            # SMA\n",
    "            df[f'ma_{n}'] = df.groupby('code')['close'].transform(lambda x: ta.SMA(x.astype(float), timeperiod=n))\n",
    "            df[f'ma_dev_{n}'] = safe_division(df['close'], df[f'ma_{n}']) - 1\n",
    "            # EMA\n",
    "            df[f'ema_{n}'] = df.groupby('code')['close'].transform(lambda x: ta.EMA(x.astype(float), timeperiod=n))\n",
    "            df[f'ema_dev_{n}'] = safe_division(df['close'], df[f'ema_{n}']) - 1\n",
    "\n",
    "    if 'close_stk' in df.columns:\n",
    "        print(\"  - è®¡ç®—è‚¡ç¥¨ MA/EMA åŠåç¦»...\")\n",
    "        for n in ma_windows:\n",
    "            # SMA Stk\n",
    "            df[f'ma_{n}_stk'] = df.groupby('code')['close_stk'].transform(lambda x: ta.SMA(x.astype(float), timeperiod=n))\n",
    "            df[f'ma_dev_{n}_stk'] = safe_division(df['close_stk'], df[f'ma_{n}_stk']) - 1\n",
    "            # EMA Stk\n",
    "            df[f'ema_{n}_stk'] = df.groupby('code')['close_stk'].transform(lambda x: ta.EMA(x.astype(float), timeperiod=n))\n",
    "            df[f'ema_dev_{n}_stk'] = safe_division(df['close_stk'], df[f'ema_{n}_stk']) - 1\n",
    "\n",
    "\n",
    "    # === XVII. å¢å¼ºå‹è‚¡å€ºèƒŒç¦»å› å­ ===\n",
    "    print(\"è®¡ç®—: XVII. å¢å¼ºå‹è‚¡å€ºèƒŒç¦»å› å­\")\n",
    "    # å‰æ: calculate_factors å·²è®¡ç®— stk_ret_mean_X, bond_ret_mean_X, dev_bond_vs_stk_X\n",
    "    required_divergence_cols = ['stk_ret_mean_1', 'stk_ret_mean_3', 'bond_ret_mean_1', 'bond_ret_mean_3', 'dev_bond_vs_stk_1', 'dev_bond_vs_stk_3']\n",
    "    if all(c in df.columns for c in required_divergence_cols):\n",
    "        print(\"  - è®¡ç®—è‚¡å¼ºå€ºå¼±/å€ºè¶…è·Œ/å€ºæŠ—è·Œ ä¿¡å·...\")\n",
    "        # è‚¡å¼ºå€ºå¼±ä¿¡å· (ç¤ºä¾‹)\n",
    "        stk_trend_up = (df['stk_ret_mean_3'] > 0.015) & df.get('ma_5_stk', df['close_stk']) > df.get('ma_10_stk', df['close_stk'].shift(5)) # ä½¿ç”¨ get å…¼å®¹ç¼ºå¤± MA\n",
    "        bond_lagging = (df['bond_ret_mean_3'] < 0.005) & (df['dev_bond_vs_stk_3'] < -0.01)\n",
    "        df['stk_strong_bond_lag_signal'] = (stk_trend_up & bond_lagging).astype(int)\n",
    "\n",
    "        # å€ºè¶…è·Œä¿¡å·\n",
    "        df['bond_oversold_vs_stk_signal'] = ((df['stk_ret_mean_1'] >= -0.01) & (df['bond_ret_mean_1'] < -0.015) & (df['dev_bond_vs_stk_1'] < -0.01)).astype(int) # è°ƒæ•´é˜ˆå€¼\n",
    "\n",
    "        # å€ºæŠ—è·Œä¿¡å·\n",
    "        df['bond_resilient_signal'] = ((df['stk_ret_mean_1'] < -0.01) & (df['bond_ret_mean_1'] > -0.005) & (df['dev_bond_vs_stk_1'] > 0.005)).astype(int)\n",
    "\n",
    "        print(\"  - è®¡ç®—è‚¡å€ºèƒŒç¦» Z-Score...\")\n",
    "        # è‚¡å€ºæ”¶ç›Šå·® Z-Score\n",
    "        dev_mean = df.groupby('code')['dev_bond_vs_stk_3'].transform(lambda x: x.rolling(20, min_periods=10).mean())\n",
    "        dev_std = df.groupby('code')['dev_bond_vs_stk_3'].transform(lambda x: x.rolling(20, min_periods=10).std())\n",
    "        df['dev_bond_vs_stk_zscore_3'] = safe_division(df['dev_bond_vs_stk_3'] - dev_mean, dev_std)\n",
    "    else:\n",
    "        print(\"  è­¦å‘Š: ç¼ºå°‘è®¡ç®—å¢å¼ºèƒŒç¦»å› å­æ‰€éœ€çš„åŸºç¡€å› å­ã€‚\")\n",
    "\n",
    "\n",
    "    # === XVIII. åŠ¨é‡åŠ é€Ÿä¸è¶‹åŠ¿æŒç»­æ€§å› å­ ===\n",
    "    print(\"è®¡ç®—: XVIII. åŠ¨é‡åŠ é€Ÿä¸è¶‹åŠ¿æŒç»­æ€§å› å­\")\n",
    "    if all(c in df.columns for c in ['bond_ret_mean_1', 'bond_ret_mean_3']):\n",
    "        print(\"  - è®¡ç®—è½¬å€ºåŠ¨é‡åŠ é€Ÿ...\")\n",
    "        df['bond_ret_accel_1_3'] = df['bond_ret_mean_1'] - df.groupby('code')['bond_ret_mean_3'].shift(1)\n",
    "    if all(c in df.columns for c in ['stk_ret_mean_1', 'stk_ret_mean_3']):\n",
    "        print(\"  - è®¡ç®—è‚¡ç¥¨åŠ¨é‡åŠ é€Ÿ...\")\n",
    "        df['stk_ret_accel_1_3'] = df['stk_ret_mean_1'] - df.groupby('code')['stk_ret_mean_3'].shift(1)\n",
    "\n",
    "    # # ADX / CCI\n",
    "    # ta_adx_cci_cols = ['high', 'low', 'close']\n",
    "    # if all(c in df.columns for c in ta_adx_cci_cols):\n",
    "    #     print(\"  - è®¡ç®—è½¬å€º ADX/CCI...\")\n",
    "    #     df['adx_14'] = df.groupby('code', group_keys=False).apply(apply_ta_func, func=ta.ADX, required_cols=ta_adx_cci_cols, timeperiod=14)\n",
    "    #     df['cci_14'] = df.groupby('code', group_keys=False).apply(apply_ta_func, func=ta.CCI, required_cols=ta_adx_cci_cols, timeperiod=14)\n",
    "\n",
    "    # ta_adx_cci_cols_stk = ['high_stk', 'low_stk', 'close_stk']\n",
    "    # if all(c in df.columns for c in ta_adx_cci_cols_stk):\n",
    "    #     print(\"  - è®¡ç®—è‚¡ç¥¨ ADX/CCI...\")\n",
    "    #     df['adx_14_stk'] = df.groupby('code', group_keys=False).apply(apply_ta_func, func=ta.ADX, required_cols=ta_adx_cci_cols_stk, timeperiod=14)\n",
    "    #     df['cci_14_stk'] = df.groupby('code', group_keys=False).apply(apply_ta_func, func=ta.CCI, required_cols=ta_adx_cci_cols_stk, timeperiod=14)\n",
    "     # ADX / CCI\n",
    "    # ADX / CCI\n",
    "    ta_adx_cci_cols = ['high', 'low', 'close']\n",
    "    if all(c in df.columns for c in ta_adx_cci_cols):\n",
    "        print(\"  - è®¡ç®—è½¬å€º ADX/CCI (using lambda)...\")\n",
    "        # Use lambda to explicitly pass args to the re-defined helper\n",
    "        df['adx_14'] = df.groupby('code', group_keys=False).apply(\n",
    "            lambda group: apply_ta_func(group, func=ta.ADX, required_cols=ta_adx_cci_cols, timeperiod=14)\n",
    "        )\n",
    "        df['cci_14'] = df.groupby('code', group_keys=False).apply(\n",
    "            lambda group: apply_ta_func(group, func=ta.CCI, required_cols=ta_adx_cci_cols, timeperiod=14)\n",
    "        )\n",
    "\n",
    "    ta_adx_cci_cols_stk = ['high_stk', 'low_stk', 'close_stk']\n",
    "    if all(c in df.columns for c in ta_adx_cci_cols_stk):\n",
    "        print(\"  - è®¡ç®—è‚¡ç¥¨ ADX/CCI (using lambda)...\")\n",
    "        df['adx_14_stk'] = df.groupby('code', group_keys=False).apply(\n",
    "            lambda group: apply_ta_func(group, func=ta.ADX, required_cols=ta_adx_cci_cols_stk, timeperiod=14)\n",
    "        )\n",
    "        df['cci_14_stk'] = df.groupby('code', group_keys=False).apply(\n",
    "            lambda group: apply_ta_func(group, func=ta.CCI, required_cols=ta_adx_cci_cols_stk, timeperiod=14)\n",
    "        )\n",
    "\n",
    "    # === XIX. è„‰å†²æ½œåŠ›ä¸ç²¾ç¡®é£é™©è¯„ä¼° ===\n",
    "    print(\"è®¡ç®—: XIX. è„‰å†²æ½œåŠ›ä¸ç²¾ç¡®é£é™©è¯„ä¼°\")\n",
    "    # è„‰å†²å‡†å¤‡åˆ†æ•° (ç¤ºä¾‹)\n",
    "    print(\"  - è®¡ç®—è„‰å†²å‡†å¤‡åˆ†æ•°...\")\n",
    "    # ä½¿ç”¨ .get() ä»¥é˜²åŸºç¡€å› å­ç¼ºå¤±\n",
    "    df['pulse_readiness_score'] = (df.get('vol_shrink_ratio', 1) + # è¶Šå°è¶Šå¥½\n",
    "                                 df.get('vol_std_decay', 1) +    # è¶Šå°è¶Šå¥½\n",
    "                                 df.get('doji_ratio_5', 1) +     # è¶Šå¤šå¯èƒ½éœ‡è¡æœ«ç«¯\n",
    "                                 df.get('body_pct_mean_5', 1) * 5 # å®ä½“è¶Šå°è¶Šå¥½ï¼Œæ”¾å¤§æƒé‡\n",
    "                                 ).rank(pct=True) # ç›´æ¥è½¬æ¢ä¸ºç™¾åˆ†ä½æ’åï¼Œå€¼è¶Šå°è¶Šå¥½\n",
    "\n",
    "    # ä¸Šä¸‹è¡Œæ³¢åŠ¨ç‡å¯¹æ¯”\n",
    "    print(\"  - è®¡ç®—ä¸Šä¸‹è¡Œæ³¢åŠ¨ç‡å¯¹æ¯”...\")\n",
    "    if 'pct_chg' in df.columns:\n",
    "        def calc_up_down_vol(group, window=20): # Use longer window\n",
    "            series = group['pct_chg']\n",
    "            up_vol = series.where(series > 0).rolling(window, min_periods=max(2, int(window*0.5))).std().fillna(0)\n",
    "            down_vol = series.where(series < 0).rolling(window, min_periods=max(2, int(window*0.5))).std().fillna(0)\n",
    "            return pd.DataFrame({'upside_vol_20': up_vol, 'downside_vol_20': down_vol}, index=group.index)\n",
    "\n",
    "        vol_df = df.groupby('code', group_keys=False).apply(calc_up_down_vol)\n",
    "        df = df.join(vol_df)\n",
    "        df['upside_bias_vol_20'] = safe_division(df.get('upside_vol_20'), df.get('downside_vol_20'))\n",
    "\n",
    "    # è¿‘æœŸè„‰å†²æˆåŠŸç‡\n",
    "    print(\"  - è®¡ç®—è¿‘æœŸè„‰å†²æˆåŠŸç‡...\")\n",
    "    if all(c in df.columns for c in ['high', 'open', 'close']):\n",
    "        df['intra_pulse_15'] = (safe_division(df['high'], df['open']) - 1) > 0.015 # æ—¥å†…è„‰å†² > 1.5%\n",
    "        df['pulse_success_15'] = df['intra_pulse_15'] & (df['close'] > df['open']) # è„‰å†²ä¸”æ”¶é˜³\n",
    "        df['recent_pulse_success_rate_20'] = df.groupby('code')['pulse_success_15'].transform(lambda x: x.rolling(20, min_periods=10).mean())\n",
    "\n",
    "    # å†å²é£é™©å›æŠ¥æ¯”\n",
    "    print(\"  - è®¡ç®—å†å²é£é™©å›æŠ¥æ¯”...\")\n",
    "    if 'pct_chg' in df.columns:\n",
    "        mean_up = df.groupby('code')['pct_chg'].transform(lambda x: x[x > 0].rolling(60, min_periods=20).mean())\n",
    "        mean_down_abs = df.groupby('code')['pct_chg'].transform(lambda x: x[x < 0].abs().rolling(60, min_periods=20).mean())\n",
    "        df['risk_reward_ratio_hist_60'] = safe_division(mean_up, mean_down_abs)\n",
    "\n",
    "\n",
    "    # === XX. å¸‚åœºæƒ…ç»ªä¸ç›¸å¯¹å¼ºåº¦å› å­ ===\n",
    "    print(\"è®¡ç®—: XX. å¸‚åœºæƒ…ç»ªä¸ç›¸å¯¹å¼ºåº¦å› å­\")\n",
    "    # Beta (éœ€è¦æŒ‡æ•°æ•°æ®ï¼Œæ­¤å¤„æ³¨é‡Šæ‰)\n",
    "    # if 'index_ret' in df.columns and 'pct_chg' in df.columns:\n",
    "    #     print(\"  - è®¡ç®—æ»šåŠ¨ Beta...\")\n",
    "    #     cov = df.groupby('code').apply(lambda x: x['pct_chg'].rolling(20, min_periods=12).cov(x['index_ret'])).reset_index(level=0, drop=True)\n",
    "    #     var_index = df.groupby('code')['index_ret'].transform(lambda x: x.rolling(20, min_periods=12).var())\n",
    "    #     df['beta_rolling_20'] = safe_division(cov, var_index)\n",
    "    # else:\n",
    "    #     print(\"  - è·³è¿‡ Beta è®¡ç®— (ç¼ºå°‘ 'index_ret' åˆ—)ã€‚\")\n",
    "\n",
    "    # è¡Œä¸šç›¸å¯¹å¼ºåº¦ (éœ€è¦è¡Œä¸šæ•°æ®ï¼Œæ­¤å¤„æ³¨é‡Šæ‰)\n",
    "    # if 'sector_ret' in df.columns and 'pct_chg_stk' in df.columns:\n",
    "    #     print(\"  - è®¡ç®—è¡Œä¸šç›¸å¯¹å¼ºåº¦...\")\n",
    "    #     df['relative_strength_sector'] = df['pct_chg_stk'] - df['sector_ret']\n",
    "    # else:\n",
    "    #     print(\"  - è·³è¿‡è¡Œä¸šç›¸å¯¹å¼ºåº¦è®¡ç®— (ç¼ºå°‘ 'sector_ret' åˆ—)ã€‚\")\n",
    "\n",
    "    # å…³é”®å› å­æˆªé¢æ’å\n",
    "    print(\"  - è®¡ç®—å…³é”®å› å­æˆªé¢æ’å...\")\n",
    "    factors_to_rank = {\n",
    "        'dev_bond_vs_stk_3': True,         # èƒŒç¦»è¶Šå¤§è¶Šå·®? (False) or è¶Šå°è¶Šå¥½ (True)? å‡è®¾ True: å° (æ»æ¶¨) å¥½\n",
    "        'stk_strong_bond_lag_signal': False, # ä¿¡å·=1 å¥½\n",
    "        'pulse_readiness_score': True,     # åˆ†æ•°è¶Šå°è¶Šå¥½\n",
    "        # 'down_freq_20': True,            # å‡è®¾ calculate_factors å·²è®¡ç®—, é¢‘ç‡è¶Šå°è¶Šå¥½\n",
    "        'risk_reward_ratio_hist_60': False,# æ¯”ç‡è¶Šå¤§è¶Šå¥½\n",
    "        'upside_bias_vol_20': False,       # æ¯”ç‡è¶Šå¤§è¶Šå¥½\n",
    "    }\n",
    "    for factor, ascending in factors_to_rank.items():\n",
    "        if factor in df.columns:\n",
    "            df[f'rank_{factor}'] = df.groupby('trade_date')[factor].rank(method='first', ascending=ascending, pct=True) # ä½¿ç”¨ç™¾åˆ†ä½æ’å\n",
    "        else:\n",
    "            print(f\"  è­¦å‘Š: å› å­ '{factor}' ä¸å­˜åœ¨ï¼Œæ— æ³•è®¡ç®—æ’åã€‚\")\n",
    "\n",
    "    # --- Final Cleanup & Optional Index Restore ---\n",
    "    print(\"æ­¥éª¤ XXI: æ¸…ç†å’Œæ¢å¤ç´¢å¼• (å¦‚æœéœ€è¦)...\")\n",
    "\n",
    "    if restore_multiindex and is_multiindex_input:\n",
    "        print(\"  - æ¢å¤ MultiIndex ['code', 'trade_date']...\")\n",
    "        df = df.set_index(['code', 'trade_date'])\n",
    "        # ç¡®ä¿ç´¢å¼•ä»ç„¶æ’åº\n",
    "        if not df.index.is_monotonic_increasing:\n",
    "             df = df.sort_index()\n",
    "    elif restore_multiindex and not is_multiindex_input:\n",
    "        print(\"  - è­¦å‘Š: åŸå§‹è¾“å…¥æ²¡æœ‰ MultiIndexï¼Œæ— æ³•æ¢å¤ã€‚\")\n",
    "\n",
    "\n",
    "    print(\"é«˜çº§å› å­è®¡ç®—å®Œæˆã€‚\")\n",
    "    return df\n",
    "\n",
    "# --- Example Usage ---\n",
    "# 1. é¦–å…ˆè¿è¡ŒåŸºç¡€å› å­è®¡ç®—\n",
    "# df_base_factors = calculate_factors(df_raw.copy(), restore_multiindex=False) # Ensure output has columns\n",
    "\n",
    "# 2. ç„¶åè¿è¡Œé«˜çº§å› å­è®¡ç®—\n",
    "# df_advanced_factors = calculate_advanced_factors(df_base_factors.copy(), restore_multiindex=True) # Can restore index at the end\n",
    "\n",
    "# print(df_advanced_factors.info())\n",
    "# print(df_advanced_factors.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹åˆå¹¶åçš„å› å­è®¡ç®— (V3)...\n",
      "æ£€æµ‹åˆ° 'code' å’Œ 'trade_date' åœ¨ MultiIndex ä¸­ï¼Œæ­£åœ¨é‡ç½®ç´¢å¼•...\n",
      "æ­¥éª¤ 0: å‡†å¤‡æ•°æ®ç±»å‹...\n",
      "è®¡ç®—: I. åŸºæœ¬ä»·æ ¼ä¸æ³¢åŠ¨ç±»å› å­ï¼ˆè½¬å€ºæœ¬èº«ï¼‰\n",
      "  - è®¡ç®— NATR...\n",
      "  - è®¡ç®— MA, Momentum, Volatility...\n",
      "  - è®¡ç®—æ¬¡æ—¥æ­¢ç›ˆç‰¹å¾...\n",
      "è®¡ç®—: II. OBVé‡èƒ½æŒ‡æ ‡ï¼ˆè½¬å€ºï¼‰\n",
      "è®¡ç®—: III. æ¢æ‰‹ä¸å¸‚å€¼ç±»å› å­\n",
      "  - è®¡ç®— turnover ç›¸å…³å› å­...\n",
      "  - è®¡ç®—æµé€šå¸‚å€¼å æ¯” (Cap / Float Share Value)...\n",
      "è®¡ç®—: IV. åŒºé—´æ”¶ç›Šç‡ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\n",
      "  - è®¡ç®—è½¬å€ºåŒºé—´æ”¶ç›Šç‡...\n",
      "  - è®¡ç®—è‚¡ç¥¨åŒºé—´æ”¶ç›Šç‡...\n",
      "è®¡ç®—: V. æˆäº¤é‡å‡å€¼æ¯”å› å­ï¼ˆè½¬å€ºï¼‰\n",
      "  - è®¡ç®—å‡é‡...\n",
      "  - è®¡ç®—é‡æ¯”...\n",
      "è®¡ç®—: VI. æ³¢åŠ¨ç‡ä¸æŒ¯å¹…ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\n",
      "  - è®¡ç®—è‚¡ç¥¨å¹´åŒ–æ³¢åŠ¨ç‡...\n",
      "  - è®¡ç®—è½¬å€ºå¹´åŒ–æ³¢åŠ¨ç‡...\n",
      "  - è®¡ç®—æŒ¯å¹…æ»šåŠ¨æ ‡å‡†å·® & æŒ¯å¹…æ³¢åŠ¨...\n",
      "è®¡ç®—: VII. è·³ç©ºä¸ç¼ºå£ç±»å› å­ï¼ˆè½¬å€ºï¼‰\n",
      "  - è®¡ç®—åŸºç¡€è·³ç©º/ç¼ºå£æŒ‡æ ‡...\n",
      "  - è®¡ç®—è·³ç©º/ç¼ºå£ç»Ÿè®¡ (è®¡æ•°ä¸ç™¾åˆ†ä½)...\n",
      "è®¡ç®—: VIII. Kçº¿ç»“æ„å› å­ï¼ˆè½¬å€ºï¼‰\n",
      "è®¡ç®—: IX. è¶‹åŠ¿åè½¬ç±»Alphaå› å­ï¼ˆè½¬å€ºä¸è‚¡ç¥¨ï¼‰\n",
      "  - è®¡ç®— Alpha å› å­å‰ç½®æ•°æ®...\n",
      "  - è®¡ç®—æˆªé¢æ’å (å¯èƒ½è¾ƒæ…¢)...\n",
      "  - è®¡ç®— Alpha6...\n",
      "  - è®¡ç®— Alpha12...\n",
      "  - è®¡ç®— Alpha83 (ä¿®æ­£ç‰ˆ)...\n",
      "  - è®¡ç®— Alpha18...\n",
      "  - è®¡ç®— Alpha36...\n",
      "  - è®¡ç®— Alpha89 (ä¿®æ­£ç‰ˆ)...\n",
      "  - è®¡ç®— Alpha65...\n",
      "  - è®¡ç®— Alpha76...\n",
      "  - è®¡ç®— Alpha92...\n",
      "  - è®¡ç®— Alpha99...\n",
      "è®¡ç®—: X. è‚¡ç¥¨ä¸è½¬å€ºè”åŠ¨å› å­\n",
      "  - è®¡ç®—æ—¥å†…è”åŠ¨...\n",
      "  - è®¡ç®—å¤šæ—¥è”åŠ¨ (æ»æ¶¨)...\n",
      "è®¡ç®—: XI. æ¨ªçºµå‘èƒŒç¦»å› å­ï¼ˆè‚¡ç¥¨ä¸è½¬å€ºï¼‰\n",
      "  - è®¡ç®—æ¨ªå‘èƒŒç¦» (è‚¡ç¥¨ vs è½¬å€º)...\n",
      "  - è®¡ç®—çºµå‘èƒŒç¦» (è‡ªèº«å†å²å¯¹æ¯”)...\n",
      "è®¡ç®—: XII. é£é™©ä¸å›æ’¤ç›¸å…³å› å­ï¼ˆè½¬å€ºï¼‰\n",
      "  - è®¡ç®—ä½ç‚¹è·ç¦»/æ ‡å‡†å·®/å›æ’¤...\n",
      "  - è®¡ç®—ä¸‹è·Œé£é™©é¢„ä¼°...\n",
      "è®¡ç®—: XIII. éœ‡è¡æ”¶æ•›ç±»å› å­ï¼ˆè½¬å€ºï¼‰ - æ¶¨ä¸åŠ¨/è·Œä¸åŠ¨\n",
      "  - è®¡ç®— ATR/æŒ¯å¹…/ä»·æ ¼æ³¢åŠ¨ æ”¶æ•›...\n",
      "  - è®¡ç®— Kçº¿å®ä½“/å½±çº¿/åå­—æ˜Ÿ ç‰¹å¾...\n",
      "è®¡ç®—: XIV. è„‰å†²ä¸åŠ¨èƒ½å› å­ï¼ˆè½¬å€ºï¼‰\n",
      "  - è®¡ç®—é«˜è„‰å†²ç»Ÿè®¡ (å¤šé˜ˆå€¼/å¤šçª—å£)...\n",
      "  - è®¡ç®—å…¶ä»–è„‰å†²æŒ‡æ ‡...\n",
      "è®¡ç®—: XV. ä¸‹è·Œç»Ÿè®¡ä¸éŸ§æ€§å› å­ï¼ˆè½¬å€ºï¼‰ - è·Œä¸åŠ¨\n",
      "  - è®¡ç®—é•¿çª—å£ä¸‹è·Œç»Ÿè®¡ (é¢‘ç‡/å¹…åº¦/æ ‡å‡†å·®)...\n",
      "  - è®¡ç®—çŸ­çª—å£'è·Œä¸åŠ¨'è¯„åˆ†...\n",
      "è®¡ç®—: XVI. Kçº¿ç»“æ„è¿ç»­æ€§\n",
      "  - è®¡ç®—Kçº¿æ–¹å‘åè½¬ç‡...\n",
      "æ­¥éª¤ XVII: æ¸…ç†ä¸´æ—¶åˆ—å’Œæ¢å¤ç´¢å¼• (å¦‚æœéœ€è¦)...\n",
      "  - Dropping intermediate columns: ['rank_delta_close_10', 'rank_vol', 'rank_mean_close_20', 'rank_close', 'rank_delta_close_10_stk', 'rank_vol_stk', 'rank_mean_close_20_stk', 'rank_close_stk', 'corr_close_vol_10', 'corr_close_vol_10_stk', 'cov_rank_close_vol_5', 'cov_rank_close_vol_5_stk', 'argmin_close_30_idx_pos', 'argmin_close_30_idx_pos_stk', 'turnover_pct', 'delta_close_1', 'delta_vol_1', 'delta_close_5', 'delta_close_10', 'delta_close_1_stk', 'delta_vol_1_stk', 'delta_close_5_stk', 'delta_close_10_stk', 'mean_close_20', 'mean_close_20_stk', 'pct_chg_stk_lag1', 'pct_chg_stk_lag2', 'cb_ret_lag1', 'kline_direction_shift1', 'range_hl', 'range_today', 'body_abs', 'shadow', 'kline_direction']\n",
      "å› å­è®¡ç®—å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  # å½“åˆ—å¤ªå¤šæ—¶ä¸æ¢è¡Œ\n",
    "df = pd.read_parquet('/Users/yiwei/Desktop/git/cb_data.pq') # å¯¼å…¥è½¬å€ºæ•°æ®\n",
    "# index = pd.read_parquet('/Users/yiwei/Desktop/git/index.pq') # å¯¼å…¥æŒ‡æ•°æ•°æ®\n",
    "\n",
    "# df_all = load_and_prepare_data('/Users/yiwei/Desktop/git/cb_data.pq')\n",
    "\n",
    "df_with_factors = calculate_factors_merged_v3(df)\n",
    "\n",
    "\n",
    "df_with_factors.to_parquet('/Users/yiwei/Desktop/git/cb_data_with_factors2.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹è®¡ç®—é«˜çº§å› å­...\n",
      "æ£€æµ‹åˆ°åˆ— 'code', 'trade_date'ã€‚\n",
      "è®¡ç®—: XVI.b ç§»åŠ¨å¹³å‡çº¿ç³»ç»Ÿ (MA & EMA) åŠå…¶åç¦»åº¦\n",
      "  - è®¡ç®—è½¬å€º MA/EMA åŠåç¦»...\n",
      "  - è®¡ç®—è‚¡ç¥¨ MA/EMA åŠåç¦»...\n",
      "è®¡ç®—: XVII. å¢å¼ºå‹è‚¡å€ºèƒŒç¦»å› å­\n",
      "  è­¦å‘Š: ç¼ºå°‘è®¡ç®—å¢å¼ºèƒŒç¦»å› å­æ‰€éœ€çš„åŸºç¡€å› å­ã€‚\n",
      "è®¡ç®—: XVIII. åŠ¨é‡åŠ é€Ÿä¸è¶‹åŠ¿æŒç»­æ€§å› å­\n",
      "  - è®¡ç®—è½¬å€º ADX/CCI (using lambda)...\n",
      "  - è®¡ç®—è‚¡ç¥¨ ADX/CCI (using lambda)...\n",
      "è®¡ç®—: XIX. è„‰å†²æ½œåŠ›ä¸ç²¾ç¡®é£é™©è¯„ä¼°\n",
      "  - è®¡ç®—è„‰å†²å‡†å¤‡åˆ†æ•°...\n",
      "  - è®¡ç®—ä¸Šä¸‹è¡Œæ³¢åŠ¨ç‡å¯¹æ¯”...\n",
      "  - è®¡ç®—è¿‘æœŸè„‰å†²æˆåŠŸç‡...\n",
      "  - è®¡ç®—å†å²é£é™©å›æŠ¥æ¯”...\n",
      "è®¡ç®—: XX. å¸‚åœºæƒ…ç»ªä¸ç›¸å¯¹å¼ºåº¦å› å­\n",
      "  - è®¡ç®—å…³é”®å› å­æˆªé¢æ’å...\n",
      "  è­¦å‘Š: å› å­ 'stk_strong_bond_lag_signal' ä¸å­˜åœ¨ï¼Œæ— æ³•è®¡ç®—æ’åã€‚\n",
      "æ­¥éª¤ XXI: æ¸…ç†å’Œæ¢å¤ç´¢å¼• (å¦‚æœéœ€è¦)...\n",
      "é«˜çº§å› å­è®¡ç®—å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  # å½“åˆ—å¤ªå¤šæ—¶ä¸æ¢è¡Œ\n",
    "df = pd.read_parquet('/Users/yiwei/Desktop/git/cb_data_with_factors2.pq') # å¯¼å…¥è½¬å€ºæ•°æ®\n",
    "\n",
    "cb_data_with_factors_enhanced = calculate_advanced_factors(df)\n",
    "\n",
    "cb_data_with_factors_enhanced.to_parquet('/Users/yiwei/Desktop/git/cb_data_with_factors_enhanced.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress # éœ€è¦å¯¼å…¥ scipy\n",
    "\n",
    "# --- å‡è®¾è¿™äº›è¾…åŠ©å‡½æ•°å·²åœ¨åˆ«å¤„å®šä¹‰æˆ–åœ¨æ­¤å¤„å®šä¹‰ ---\n",
    "\n",
    "def safe_division(numerator, denominator, default=np.nan):\n",
    "    \"\"\"æ‰§è¡Œå®‰å…¨é™¤æ³•\"\"\"\n",
    "    # ... (å‡½æ•°ä½“å¦‚ä¸Šä¸€ä¸ªå›ç­”æ‰€ç¤º) ...\n",
    "    try:\n",
    "        # Ensure inputs are numeric if they are series/arrays\n",
    "        if hasattr(numerator, '__iter__'):\n",
    "            numerator = pd.to_numeric(numerator, errors='coerce')\n",
    "        if hasattr(denominator, '__iter__'):\n",
    "            denominator = pd.to_numeric(denominator, errors='coerce')\n",
    "            denominator = denominator.replace(0, np.nan)\n",
    "        elif isinstance(denominator, (int, float)) and denominator == 0:\n",
    "            denominator = np.nan\n",
    "\n",
    "        result = numerator / denominator\n",
    "        if hasattr(result, '__iter__'):\n",
    "             # Replace inf/-inf that might result from large numbers / small numbers\n",
    "             result = result.replace([np.inf, -np.inf], np.nan)\n",
    "             return result.fillna(default)\n",
    "        elif np.isinf(result) or np.isnan(result):\n",
    "             return default\n",
    "        else:\n",
    "             return result\n",
    "\n",
    "    except (TypeError, ValueError):\n",
    "        # Handle cases where inputs cannot be converted to numeric\n",
    "        if hasattr(numerator, 'shape'):\n",
    "             return pd.Series(default, index=getattr(numerator, 'index', None), dtype=float)\n",
    "        elif hasattr(denominator, 'shape'):\n",
    "             return pd.Series(default, index=getattr(denominator, 'index', None), dtype=float)\n",
    "        else:\n",
    "             return default\n",
    "\n",
    "def calculate_rolling_slope(series):\n",
    "    \"\"\"è®¡ç®— Series çš„çº¿æ€§å›å½’æ–œç‡\"\"\"\n",
    "    # ... (å‡½æ•°ä½“å¦‚ä¸Šä¸€ä¸ªå›ç­”æ‰€ç¤º) ...\n",
    "    y = series.dropna()\n",
    "    if len(y) < 2:\n",
    "        return np.nan\n",
    "    x = np.arange(len(y)) # Use index relative to the window\n",
    "    try:\n",
    "        # ä½¿ç”¨ scipy è®¡ç®—æ–œç‡\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        # Add check for np.isnan(slope) which can happen if x or y has issues\n",
    "        return slope if not np.isnan(slope) else np.nan\n",
    "    except ValueError: # Handle cases like all y values being the same\n",
    "        return 0.0 # Slope is zero if y is constant\n",
    "    except Exception: # Catch any other unexpected errors\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_min_periods(window, factor=0.6, min_required=1):\n",
    "    \"\"\"è·å–æ»šåŠ¨è®¡ç®—çš„æœ€å°å‘¨æœŸæ•°\"\"\"\n",
    "    return max(min_required, int(window * factor))\n",
    "\n",
    "# --- æ–°å¢å› å­è®¡ç®—å‡½æ•° ---\n",
    "\n",
    "def add_custom_factors(df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åœ¨å·²æœ‰çš„å› å­ DataFrame ä¸Šå¢åŠ æŒ‡å®šçš„è‡ªå®šä¹‰å› å­ã€‚\n",
    "    Adds specified custom factors onto an existing factor DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df_input (pd.DataFrame): å·²ç»åŒ…å«åŸºç¡€å› å­çš„ DataFrame (DataFrame already containing base factors).\n",
    "                                 éœ€è¦åŒ…å« 'code', 'trade_date', 'close', 'pct_chg', 'high', 'pre_close',\n",
    "                                 'close_stk', 'pct_chg_stk', 'high_stk'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: æ·»åŠ äº†æ–°å› å­åˆ—çš„ DataFrame (DataFrame with new factor columns added).\n",
    "    \"\"\"\n",
    "    print(\"å¼€å§‹æ·»åŠ è‡ªå®šä¹‰å› å­...\")\n",
    "    df = df_input.copy() # æ“ä½œå‰¯æœ¬ä»¥é˜²ä¿®æ”¹åŸå§‹è¾“å…¥\n",
    "\n",
    "    # 0. ç¡®ä¿æ’åºå’Œå¿…è¦åˆ—å­˜åœ¨\n",
    "    required_cols_cb = ['code', 'trade_date', 'close', 'pct_chg', 'high', 'pre_close']\n",
    "    required_cols_stk = ['close_stk', 'pct_chg_stk', 'high_stk'] # 'code', 'trade_date' å·²åŒ…å«\n",
    "\n",
    "    missing_cols = [col for col in required_cols_cb if col not in df.columns]\n",
    "    # æ£€æŸ¥è‚¡ç¥¨ç›¸å…³åˆ—ï¼Œä½†ä»…å½“å®ƒä»¬å­˜åœ¨æ—¶æ‰è®¡ç®—è‚¡ç¥¨å› å­\n",
    "    has_stk_data = all(col in df.columns for col in required_cols_stk)\n",
    "    if not has_stk_data:\n",
    "        print(\"  è­¦å‘Š: ç¼ºå°‘éƒ¨åˆ†æˆ–å…¨éƒ¨è‚¡ç¥¨ç›¸å…³åˆ—ï¼Œå°†ä»…è®¡ç®—è½¬å€ºå› å­ã€‚\")\n",
    "        missing_stk = [col for col in required_cols_stk if col not in df.columns]\n",
    "        print(f\"  (ç¼ºå¤±çš„è‚¡ç¥¨åˆ—: {missing_stk})\")\n",
    "\n",
    "\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"è¾“å…¥ DataFrame ç¼ºå°‘å¿…è¦çš„è½¬å€ºåˆ—: {missing_cols}\")\n",
    "\n",
    "    # ç¡®ä¿æ’åº\n",
    "    if not isinstance(df.index, pd.MultiIndex) or df.index.names != ['code', 'trade_date']:\n",
    "         if 'code' in df.columns and 'trade_date' in df.columns:\n",
    "             print(\"  - ç¡®ä¿æŒ‰ code, trade_date æ’åº...\")\n",
    "             df = df.sort_values(by=['code', 'trade_date'])\n",
    "         else:\n",
    "             # å¦‚æœæ˜¯ä»¥ MultiIndex è¾“å…¥ä½†åæ¥è¢« reset äº†ï¼Œè¿™é‡Œå°è¯•æ¢å¤ç”¨äºæ’åº\n",
    "              if 'code' in df.columns and 'trade_date' in df.columns:\n",
    "                 df = df.sort_values(by=['code', 'trade_date'])\n",
    "              else:\n",
    "                 print(\"  è­¦å‘Šï¼šæ— æ³•ç¡®è®¤æ’åºï¼Œè¯·ç¡®ä¿è¾“å…¥ DataFrame æŒ‰ code, trade_date æ’åºã€‚\")\n",
    "\n",
    "\n",
    "    # è®¡ç®— pre_close_stk (å¦‚æœä¸å­˜åœ¨) - è‚¡ç¥¨è„‰å†²æ£€æŸ¥éœ€è¦\n",
    "    if has_stk_data and 'pre_close_stk' not in df.columns:\n",
    "        print(\"  - è®¡ç®— pre_close_stk (è‚¡ç¥¨æ˜¨æ—¥æ”¶ç›˜ä»·)...\")\n",
    "        df['pre_close_stk'] = df.groupby('code')['close_stk'].shift(1)\n",
    "\n",
    "    # å®šä¹‰è®¡ç®—çª—å£\n",
    "    windows = [10, 20, 30, 50, 120, 250]\n",
    "    min_slope_periods = 2 # æ–œç‡è®¡ç®—æœ€å°‘éœ€è¦2ä¸ªç‚¹\n",
    "\n",
    "    # 1. è®¡ç®—åŒºé—´ç´¯è®¡æ¶¨å¹…\n",
    "    print(\"è®¡ç®—: 1. åŒºé—´ç´¯è®¡æ¶¨å¹…...\")\n",
    "    if 'pct_chg' in df.columns:\n",
    "        print(f\"  - è®¡ç®—è½¬å€º {windows} æ—¥ç´¯è®¡æ¶¨å¹…...\")\n",
    "        df['_pct_chg_plus_1'] = df['pct_chg'] + 1 # ä¸´æ—¶åˆ—\n",
    "        for win in windows:\n",
    "            df[f'pct_chg_cum_{win}'] = df.groupby('code')['_pct_chg_plus_1'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=1).apply(np.prod, raw=True) # ç´¯ä¹˜ min_periods=1\n",
    "            ) - 1\n",
    "        del df['_pct_chg_plus_1'] # åˆ é™¤ä¸´æ—¶åˆ—\n",
    "\n",
    "    if has_stk_data and 'pct_chg_stk' in df.columns:\n",
    "        print(f\"  - è®¡ç®—è‚¡ç¥¨ {windows} æ—¥ç´¯è®¡æ¶¨å¹…...\")\n",
    "        df['_pct_chg_stk_plus_1'] = df['pct_chg_stk'] + 1 # ä¸´æ—¶åˆ—\n",
    "        for win in windows:\n",
    "            df[f'pct_chg_stk_cum_{win}'] = df.groupby('code')['_pct_chg_stk_plus_1'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=1).apply(np.prod, raw=True)\n",
    "            ) - 1\n",
    "        del df['_pct_chg_stk_plus_1'] # åˆ é™¤ä¸´æ—¶åˆ—\n",
    "\n",
    "    # 2. è®¡ç®—æ»šåŠ¨æ–œç‡\n",
    "    print(\"è®¡ç®—: 2. æ»šåŠ¨æ–œç‡...\")\n",
    "    if 'close' in df.columns:\n",
    "        print(f\"  - è®¡ç®—è½¬å€º {windows} æ—¥æ”¶ç›˜ä»·æ–œç‡...\")\n",
    "        for win in windows:\n",
    "            df[f'slope_{win}'] = df.groupby('code')['close'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=get_min_periods(win, min_required=min_slope_periods))\n",
    "                         .apply(calculate_rolling_slope, raw=False) # raw=False ä¼ é€’ Series å¯¹è±¡\n",
    "            )\n",
    "\n",
    "    if has_stk_data and 'close_stk' in df.columns:\n",
    "        print(f\"  - è®¡ç®—è‚¡ç¥¨ {windows} æ—¥æ”¶ç›˜ä»·æ–œç‡...\")\n",
    "        for win in windows:\n",
    "            df[f'slope_stk_{win}'] = df.groupby('code')['close_stk'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=get_min_periods(win, min_required=min_slope_periods))\n",
    "                         .apply(calculate_rolling_slope, raw=False)\n",
    "            )\n",
    "\n",
    "    # 3. è®¡ç®—æ”¶ç›˜ä»·æ˜¯å¦å¤§äº N æ—¥å‡çº¿\n",
    "    print(\"è®¡ç®—: 3. æ”¶ç›˜ä»· vs Næ—¥å‡çº¿...\")\n",
    "    temp_ma_cols = []\n",
    "    temp_ma_stk_cols = []\n",
    "\n",
    "    # è®¡ç®—å‡çº¿\n",
    "    if 'close' in df.columns:\n",
    "        print(f\"  - è®¡ç®—è½¬å€º {windows} æ—¥å‡çº¿...\")\n",
    "        for win in windows:\n",
    "            col_name = f'_ma_temp_{win}' # ä½¿ç”¨ä¸´æ—¶åç§°\n",
    "            temp_ma_cols.append(col_name)\n",
    "            df[col_name] = df.groupby('code')['close'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=get_min_periods(win)).mean()\n",
    "            )\n",
    "        print(\"  - è®¡ç®—è½¬å€ºæ”¶ç›˜ä»· > å‡çº¿æ ‡å¿—...\")\n",
    "        for win in windows:\n",
    "             ma_col = f'_ma_temp_{win}'\n",
    "             if ma_col in df.columns:\n",
    "                 df[f'close_gt_ma_{win}'] = (df['close'] > df[ma_col]).astype(int)\n",
    "\n",
    "    if has_stk_data and 'close_stk' in df.columns:\n",
    "        print(f\"  - è®¡ç®—è‚¡ç¥¨ {windows} æ—¥å‡çº¿...\")\n",
    "        for win in windows:\n",
    "            col_name = f'_ma_stk_temp_{win}' # ä½¿ç”¨ä¸´æ—¶åç§°\n",
    "            temp_ma_stk_cols.append(col_name)\n",
    "            df[col_name] = df.groupby('code')['close_stk'].transform(\n",
    "                lambda x: x.rolling(win, min_periods=get_min_periods(win)).mean()\n",
    "            )\n",
    "        print(\"  - è®¡ç®—è‚¡ç¥¨æ”¶ç›˜ä»· > å‡çº¿æ ‡å¿—...\")\n",
    "        for win in windows:\n",
    "            ma_col = f'_ma_stk_temp_{win}'\n",
    "            if ma_col in df.columns:\n",
    "                df[f'close_gt_ma_stk_{win}'] = (df['close_stk'] > df[ma_col]).astype(int)\n",
    "\n",
    "    # åˆ é™¤ä¸´æ—¶å‡çº¿åˆ—\n",
    "    if temp_ma_cols:\n",
    "        df = df.drop(columns=temp_ma_cols, errors='ignore')\n",
    "    if temp_ma_stk_cols:\n",
    "        df = df.drop(columns=temp_ma_stk_cols, errors='ignore')\n",
    "\n",
    "\n",
    "    # 4. æ£€æŸ¥æ¯æ—¥æœ€å¤§æ¶¨å¹…/è„‰å†²æ˜¯å¦è¶…è¿‡é˜ˆå€¼\n",
    "    print(\"è®¡ç®—: 4. æ¯æ—¥æœ€å¤§æ¶¨å¹…/è„‰å†²æ£€æŸ¥...\")\n",
    "    thresholds_pct = [0.02, 0.025, 0.028, 0.03, 0.038, 0.048, 0.058, 0.078] # é˜ˆå€¼åˆ—è¡¨\n",
    "    thresholds_bps = [int(t*10000) for t in thresholds_pct] # åŸºç‚¹è¡¨ç¤º (e.g., 200bps for 2%)\n",
    "\n",
    "    if 'high' in df.columns and 'pre_close' in df.columns:\n",
    "        print(\"  - è®¡ç®—è½¬å€ºæ—¥å†…æœ€é«˜æ¶¨å¹…...\")\n",
    "        df['_max_daily_gain_cb'] = safe_division(df['high'], df['pre_close']) - 1\n",
    "        print(\"  - ç”Ÿæˆè½¬å€ºè„‰å†²æ ‡å¿—...\")\n",
    "        for t_pct, t_bps in zip(thresholds_pct, thresholds_bps):\n",
    "            df[f'max_gain_cb_gt_{t_bps}bps'] = (df['_max_daily_gain_cb'] > t_pct).astype(int)\n",
    "        del df['_max_daily_gain_cb'] # åˆ é™¤ä¸´æ—¶åˆ—\n",
    "\n",
    "    if has_stk_data and 'high_stk' in df.columns and 'pre_close_stk' in df.columns:\n",
    "        print(\"  - è®¡ç®—è‚¡ç¥¨æ—¥å†…æœ€é«˜æ¶¨å¹…...\")\n",
    "        df['_max_daily_gain_stk'] = safe_division(df['high_stk'], df['pre_close_stk']) - 1\n",
    "        print(\"  - ç”Ÿæˆè‚¡ç¥¨è„‰å†²æ ‡å¿—...\")\n",
    "        for t_pct, t_bps in zip(thresholds_pct, thresholds_bps):\n",
    "            df[f'max_gain_stk_gt_{t_bps}bps'] = (df['_max_daily_gain_stk'] > t_pct).astype(int)\n",
    "        del df['_max_daily_gain_stk'] # åˆ é™¤ä¸´æ—¶åˆ—\n",
    "    elif has_stk_data:\n",
    "        print(\"  è­¦å‘Š: ç¼ºå°‘ high_stk æˆ– pre_close_stkï¼Œæ— æ³•è®¡ç®—è‚¡ç¥¨æœ€å¤§æ—¥æ¶¨å¹…ã€‚\")\n",
    "\n",
    "\n",
    "    print(\"è‡ªå®šä¹‰å› å­æ·»åŠ å®Œæˆã€‚\")\n",
    "    return df\n",
    "\n",
    "# --- Example Usage ---\n",
    "# print(\"åŠ è½½åŸºç¡€å› å­æ•°æ®...\")\n",
    "# # å‡è®¾ df_factors_v3 æ˜¯å·²ç»è¿è¡Œè¿‡ calculate_factors_merged_v3 çš„ç»“æœ\n",
    "# try:\n",
    "#     # å°è¯•åŠ è½½ V3 æˆ– V4 ç”Ÿæˆçš„æ–‡ä»¶\n",
    "#     df_factors_base = pd.read_parquet('/Users/yiwei/Desktop/git/cb_data_with_factors2.pq')\n",
    "# except FileNotFoundError:\n",
    "#     print(\"é”™è¯¯ï¼šåŸºç¡€å› å­æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ V3/V4 å‡½æ•°ç”Ÿæˆã€‚\")\n",
    "#     # æˆ–è€…åœ¨è¿™é‡Œç›´æ¥è°ƒç”¨ V3/V4\n",
    "#     # print(\"è¿è¡Œ V3/V4 å› å­è®¡ç®—...\")\n",
    "#     # df_raw = pd.read_parquet('/Users/yiwei/Desktop/git/cb_data.pq')\n",
    "#     # df_factors_base = calculate_factors_merged_v4(df_raw) # æˆ– V3\n",
    "\n",
    "# if 'df_factors_base' in locals():\n",
    "#     print(\"æ·»åŠ è‡ªå®šä¹‰å› å­...\")\n",
    "#     df_final_factors = add_custom_factors(df_factors_base)\n",
    "\n",
    "#     print(\"æœ€ç»ˆå› å­è®¡ç®—å®Œæˆã€‚ç»“æœå½¢çŠ¶:\", df_final_factors.shape)\n",
    "#     print(\"Sample output:\")\n",
    "#     # print(df_final_factors.tail())\n",
    "#     print(\"\\næ–°æ·»åŠ çš„åˆ—åç¤ºä¾‹:\")\n",
    "#     new_cols = [col for col in df_final_factors.columns if col not in df_factors_base.columns]\n",
    "#     print(new_cols[:10]) # æ‰“å°å‰10ä¸ªæ–°åˆ—å\n",
    "\n",
    "#     # å¯ä»¥é€‰æ‹©ä¿å­˜æœ€ç»ˆç»“æœ\n",
    "#     # df_final_factors.to_parquet('/Users/yiwei/Desktop/git/cb_data_with_final_factors.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹æ·»åŠ è‡ªå®šä¹‰å› å­...\n",
      "  - ç¡®ä¿æŒ‰ code, trade_date æ’åº...\n",
      "è®¡ç®—: 1. åŒºé—´ç´¯è®¡æ¶¨å¹…...\n",
      "  - è®¡ç®—è½¬å€º [10, 20, 30, 50, 120, 250] æ—¥ç´¯è®¡æ¶¨å¹…...\n",
      "  - è®¡ç®—è‚¡ç¥¨ [10, 20, 30, 50, 120, 250] æ—¥ç´¯è®¡æ¶¨å¹…...\n",
      "è®¡ç®—: 2. æ»šåŠ¨æ–œç‡...\n",
      "  - è®¡ç®—è½¬å€º [10, 20, 30, 50, 120, 250] æ—¥æ”¶ç›˜ä»·æ–œç‡...\n",
      "  - è®¡ç®—è‚¡ç¥¨ [10, 20, 30, 50, 120, 250] æ—¥æ”¶ç›˜ä»·æ–œç‡...\n",
      "è®¡ç®—: 3. æ”¶ç›˜ä»· vs Næ—¥å‡çº¿...\n",
      "  - è®¡ç®—è½¬å€º [10, 20, 30, 50, 120, 250] æ—¥å‡çº¿...\n",
      "  - è®¡ç®—è½¬å€ºæ”¶ç›˜ä»· > å‡çº¿æ ‡å¿—...\n",
      "  - è®¡ç®—è‚¡ç¥¨ [10, 20, 30, 50, 120, 250] æ—¥å‡çº¿...\n",
      "  - è®¡ç®—è‚¡ç¥¨æ”¶ç›˜ä»· > å‡çº¿æ ‡å¿—...\n",
      "è®¡ç®—: 4. æ¯æ—¥æœ€å¤§æ¶¨å¹…/è„‰å†²æ£€æŸ¥...\n",
      "  - è®¡ç®—è½¬å€ºæ—¥å†…æœ€é«˜æ¶¨å¹…...\n",
      "  - ç”Ÿæˆè½¬å€ºè„‰å†²æ ‡å¿—...\n",
      "  - è®¡ç®—è‚¡ç¥¨æ—¥å†…æœ€é«˜æ¶¨å¹…...\n",
      "  - ç”Ÿæˆè‚¡ç¥¨è„‰å†²æ ‡å¿—...\n",
      "è‡ªå®šä¹‰å› å­æ·»åŠ å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('/Users/yiwei/Desktop/git/cb_data_with_factors_enhanced.pq') # å¯¼å…¥è½¬å€ºæ•°æ®\n",
    "\n",
    "cb_data_with_factors_enhanced2 = add_custom_factors(df)\n",
    "\n",
    "cb_data_with_factors_enhanced2.to_parquet('/Users/yiwei/Desktop/git/cb_data_with_factors_enhanced_with_junxian.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–°å¢éƒ¨åˆ†ï¼šæ¶¨ä¸åŠ¨ + è·Œä¸åŠ¨ + è„‰å†²å¯èƒ½æ€§å› å­ç»„åˆï¼ˆå«ç»„åˆç­›é€‰ç¤ºä¾‹ï¼‰\n",
    "# =========================\n",
    "\n",
    "# ...ï¼ˆåŸæœ‰å› å­ä¿ç•™ï¼Œæ­¤å¤„çœç•¥ï¼‰...\n",
    "\n",
    "# =========================\n",
    "# DEMOï¼šç»„åˆ signal ç¤ºä¾‹ï¼ˆç­›é€‰åé‡æ–°æ’åï¼‰\n",
    "# =========================\n",
    "\n",
    "# ç›®æ ‡ï¼šé€‰å‡ºâ€œè·Œä¸åŠ¨ + æ”¶æ•›â€ä¸­çš„ä¸ªè‚¡ï¼Œå†å¯¹å…¶åœ¨ turnover å’Œè„‰å†²æ½œåŠ›ä¸Šé‡æ–°æ‰“åˆ†\n",
    "\n",
    "# 1ï¸âƒ£ ç­›é€‰æ¡ä»¶ï¼ˆå¦‚ï¼šè·Œä¸åŠ¨ + æ”¶æ•›ï¼‰\n",
    "filter_mask = (df['no_fall_score_10'] > 0.01) & (df['atr_decay_5_10'] < 0.8)\n",
    "df_filtered = df[filter_mask].copy()\n",
    "\n",
    "# 2ï¸âƒ£ åœ¨å­é›†å†…é‡æ–°æ¨ªæˆªé¢æ’åï¼ˆæ‰“åˆ†å› å­ï¼šturnover + è„‰å†²æ½œåŠ›ï¼‰\n",
    "df_filtered['turnover_score'] = df_filtered.groupby('trade_date')['turnover'].rank(pct=True)\n",
    "df_filtered['surge_score'] = df_filtered.groupby('trade_date')['jump_atr_5'].rank(pct=True)\n",
    "\n",
    "# 3ï¸âƒ£ ç»¼åˆæ‰“åˆ†\n",
    "# æƒé‡å¯ä»¥è°ƒæ•´ï¼Œè¿™é‡Œé»˜è®¤ 0.5 + 0.5\n",
    "df_filtered['combo_score'] = 0.5 * df_filtered['turnover_score'] + 0.5 * df_filtered['surge_score']\n",
    "\n",
    "# 4ï¸âƒ£ è¾“å‡ºæœ€ç»ˆ signalï¼ˆå¦‚ï¼šå¾—åˆ† > 80%ï¼‰\n",
    "df_filtered['signal_combo_top20'] = (df_filtered.groupby('trade_date')['combo_score'].rank(pct=True) > 0.8).astype(int)\n",
    "\n",
    "# 5ï¸âƒ£ å¯é€‰ï¼šå°†ä¿¡å·å›å¡«å›ä¸» dfï¼ˆéå¿…é¡»ï¼‰\n",
    "df = df.merge(df_filtered[['code', 'trade_date', 'signal_combo_top20']], on=['code', 'trade_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt v1:\n",
    "# å¯è½¬å€º + æ­£è‚¡ å› å­è®¡ç®—æ¨¡å—ï¼ˆå‰”é™¤æ‰“åˆ†/æ’åé€»è¾‘ï¼ŒæŒ‰åˆ†ç±»é¡ºåºæ•´ç†ï¼‰\n",
    "# =========================\n",
    "\n",
    "# === ğŸŸ¡ æ³¢åŠ¨/æ”¶æ•›ç±»å› å­ ===\n",
    "df['atr_5'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(5).mean()).reset_index(0, drop=True)\n",
    "df['atr_20'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(20).mean()).reset_index(0, drop=True)\n",
    "df['atr_5_decay'] = df['atr_5'] / df['atr_20']\n",
    "\n",
    "# æŒ¯å¹…æ”¶æ•›\n",
    "df['zhengfu_5'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(5).mean()).reset_index(0, drop=True)\n",
    "df['zhengfu_20'] = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(20).mean()).reset_index(0, drop=True)\n",
    "df['zhengfu_decay_5_20'] = df['zhengfu_5'] / df['zhengfu_20']\n",
    "range_5 = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(5).mean()).reset_index(0, drop=True)\n",
    "range_20 = df.groupby('code').apply(lambda x: (x['high'] - x['low']).rolling(20).mean()).reset_index(0, drop=True)\n",
    "df['range_ratio_5_20'] = range_5 / range_20\n",
    "\n",
    "# Kçº¿ç»“æ„æ”¶æ•›\n",
    "df['body'] = (df['close'] - df['open']).abs()\n",
    "df['shadow'] = (df['high'] - df['low']) - df['body']\n",
    "df['small_body_shadow_ratio'] = df['shadow'] / (df['body'] + 1e-6)\n",
    "df['is_doji'] = (df['body'] / (df['high'] - df['low'] + 1e-6)) < 0.15\n",
    "df['doji_ratio_5'] = df.groupby('code')['is_doji'].rolling(5).mean().reset_index(0, drop=True)\n",
    "\n",
    "# === âš¡ï¸ è„‰å†²ç±»å› å­ ===\n",
    "for thres in [0.015, 0.02, 0.03, 0.04, 0.05, 0.06]:\n",
    "    df[f'high_jump_{int(thres*1000)}'] = ((df['high'] / df['pre_close'] - 1) > thres).astype(int)\n",
    "\n",
    "for n in [3, 5, 10]:\n",
    "    high_mean = df.groupby('code')['high'].rolling(n).mean().reset_index(0, drop=True)\n",
    "    close_mean = df.groupby('code')['close'].rolling(n).mean().reset_index(0, drop=True)\n",
    "    close_std = df.groupby('code')['close'].rolling(n).std().reset_index(0, drop=True)\n",
    "    df[f'jump_atr_{n}'] = (df['high'] - close_mean) / (close_std + 1e-6)\n",
    "\n",
    "df['zscore_pctchg_20'] = df.groupby('code')['pct_chg'].transform(lambda x: (x - x.rolling(20).mean()) / (x.rolling(20).std() + 1e-6))\n",
    "df['range_today'] = df['high'] - df['low']\n",
    "df['range_atr_5'] = df['range_today'] / df.groupby('code')['range_today'].rolling(5).mean().reset_index(0, drop=True)\n",
    "df['range_jump_potential'] = (df['range_atr_5'] > 1.5).astype(int)\n",
    "\n",
    "# === ğŸ“‰ è·Œä¸åŠ¨ç±»å› å­ ===\n",
    "for win in [5, 10]:\n",
    "    df[f'down_freq_{win}'] = df.groupby('code')['pct_chg'].apply(lambda x: x.rolling(win).apply(lambda s: (s < 0).mean())).reset_index(0, drop=True)\n",
    "    df[f'down_amp_{win}'] = df.groupby('code')['pct_chg'].apply(lambda x: x.rolling(win).apply(lambda s: s[s < 0].mean() if (s < 0).any() else 0)).reset_index(0, drop=True)\n",
    "    df[f'no_fall_score_{win}'] = (1 - df[f'down_freq_{win}']) * (-df[f'down_amp_{win}'])\n",
    "\n",
    "# === ğŸ” æƒ…ç»ªä¸ç»“æ„ç±»å› å­ ===\n",
    "vol_ma20 = df.groupby('code')['volume'].rolling(20).mean().reset_index(0, drop=True)\n",
    "df['vol_spike_ratio'] = df['volume'] / (vol_ma20 + 1e-6)\n",
    "vol_std_5 = df.groupby('code')['volume'].rolling(5).std().reset_index(0, drop=True)\n",
    "vol_std_20 = df.groupby('code')['volume'].rolling(20).std().reset_index(0, drop=True)\n",
    "df['vol_std_decay'] = vol_std_5 / (vol_std_20 + 1e-6)\n",
    "df['gap_and_go_flag'] = ((df['open'] > df['pre_close'] * 1.02) & (df['close'] > df['open'])).astype(int)\n",
    "df['gap_body_ratio'] = (df['open'] - df['pre_close']) / (df['close'] - df['open']).replace(0, np.nan)\n",
    "\n",
    "# === ğŸ“ˆ æ­£è‚¡ç‰ˆæœ¬ï¼ˆå¸¦ _stkï¼‰å¯é€‰é•œåƒå­—æ®µ ===\n",
    "# æ³¨ï¼šä¸‹æ–¹æ˜¯æ­£è‚¡ä¸è½¬å€ºå› å­é•œåƒï¼Œä¾¿äºåç»­è”åŠ¨å¯¹æ¯”åˆ†æ\n",
    "df['jump_atr_5_stk'] = (df['high_stk'] - df.groupby('code')['close_stk'].rolling(5).mean().reset_index(0, drop=True)) / \\\n",
    "                        (df.groupby('code')['close_stk'].rolling(5).std().reset_index(0, drop=True) + 1e-6)\n",
    "df['vol_spike_ratio_stk'] = df['vol_stk'] / (df.groupby('code')['vol_stk'].rolling(20).mean().reset_index(0, drop=True) + 1e-6)\n",
    "df['gap_and_go_flag_stk'] = ((df['open_stk'] > df['pre_close_stk'] * 1.02) & (df['close_stk'] > df['open_stk'])).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # å½“åˆ—å¤ªå¤šæ—¶ä¸æ¢è¡Œ\n",
    "df = pd.read_parquet('/Users/yiwei/Desktop/git/cb_data.pq') # å¯¼å…¥è½¬å€ºæ•°æ®\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'pre_close', 'open', 'high', 'low', 'close', 'limit',\n",
       "       'close_ma_5', 'bias_5', 'pct_chg', 'adj_factor', 'vol', 'vol_5',\n",
       "       'amount', 'amount_5', 'volatility', 'code_stk', 'name_stk',\n",
       "       'pre_close_stk', 'open_stk', 'high_stk', 'low_stk', 'close_stk',\n",
       "       'pct_chg_stk', 'adj_factor_stk', 'vol_stk', 'amount_stk', 'pe_ttm',\n",
       "       'pb', 'ps_ttm', 'dv_ratio', 'total_share', 'float_share', 'total_mv',\n",
       "       'circ_mv', 'debt_to_assets', 'volatility_stk', 'is_call', 'conv_price',\n",
       "       'conv_value', 'conv_prem', 'dblow', 'issue_size', 'remain_size',\n",
       "       'remain_cap', 'turnover', 'turnover_5', 'cap_mv_rate', 'list_date',\n",
       "       'list_days', 'conv_start_date', 'left_conv_start_days', 'conv_end_date',\n",
       "       'left_years', 'ytm', 'pure_value', 'bond_prem', 'option_value',\n",
       "       'theory_value', 'theory_bias', 'rating', 'yy_rating', 'orgform', 'area',\n",
       "       'industry_1', 'industry_2', 'industry_3', 'maturity_put_price',\n",
       "       'maturity', 'popularity_ranking', 'pct_chg_5', 'pct_chg_5_stk',\n",
       "       'alpha_pct_chg_5', 'theory_conv_prem', 'mod_conv_prem', 'open_pct_chg',\n",
       "       'high_pct_chg', 'low_pct_chg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
