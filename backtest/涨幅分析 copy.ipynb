{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import quantstats as qs # Not directly used for IC/IR calculation\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr # Needed for IC calculation\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Quantstats might issue user warnings\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "# Helper for safe division (if needed, otherwise simple division is fine)\n",
    "def safe_division(numerator, denominator, default=np.nan):\n",
    "    \"\"\"Performs division, returning default if denominator is zero or NaN.\"\"\"\n",
    "    mask = (denominator == 0) | denominator.isna() | numerator.isna()\n",
    "    # Ensure denominator is float for np.where compatibility if it might be int 0\n",
    "    denominator_float = denominator.astype(float)\n",
    "    result = np.where(mask, default, numerator / denominator_float)\n",
    "    return result\n",
    "\n",
    "# Step 1: Load Data (Mostly Unchanged - Ensure required columns exist)\n",
    "def load_data(cb_path, index_path=None): # Made index_path optional\n",
    "    \"\"\"Loads CB data, ensures DatetimeIndex.\"\"\"\n",
    "    print(\"--- Step 1: Loading Data ---\")\n",
    "    try:\n",
    "        df = pd.read_parquet(cb_path)\n",
    "\n",
    "        # Ensure df has correct MultiIndex with DatetimeIndex for trade_date\n",
    "        required_levels = ['code', 'trade_date']\n",
    "        if not (isinstance(df.index, pd.MultiIndex) and\n",
    "                all(level in df.index.names for level in required_levels)):\n",
    "            print(\"Attempting to set MultiIndex ['code', 'trade_date']...\")\n",
    "            if all(col in df.columns for col in required_levels):\n",
    "                df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "                df = df.set_index(required_levels)\n",
    "                print(\"MultiIndex set successfully.\")\n",
    "            else:\n",
    "                raise ValueError(\"CB data missing 'code' or 'trade_date' columns for index.\")\n",
    "\n",
    "        if not isinstance(df.index.levels[df.index.names.index('trade_date')], pd.DatetimeIndex):\n",
    "             print(\"Converting 'trade_date' level to DatetimeIndex...\")\n",
    "             df.index = df.index.set_levels(pd.to_datetime(df.index.levels[df.index.names.index('trade_date')]), level='trade_date')\n",
    "             print(\"'trade_date' level converted.\")\n",
    "\n",
    "        # --- Add check for required return calculation columns ---\n",
    "        # Need T close for pulse logic, T+1 open/high/close for fwd returns\n",
    "        required_cols = ['open', 'high', 'close']\n",
    "        # pct_chg is useful if defined as T-1 close to T close for easy fwd ret calc\n",
    "        if 'pct_chg' not in df.columns:\n",
    "             print(\"Warning: 'pct_chg' column missing. Forward close-to-close return will be calculated manually if possible.\")\n",
    "        else:\n",
    "             required_cols.append('pct_chg')\n",
    "\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "             # Allow proceeding if pct_chg is missing but others are present\n",
    "             if not ('open' in df.columns and 'high' in df.columns and 'close' in df.columns):\n",
    "                  raise ValueError(f\"Required columns for return calculation missing: {missing_cols}\")\n",
    "\n",
    "        print(f\"Loaded CB data shape: {df.shape}\")\n",
    "\n",
    "        # Load index data if path provided (not strictly needed for core analysis)\n",
    "        index_df = None\n",
    "        if index_path:\n",
    "            try:\n",
    "                index_df = pd.read_parquet(index_path)\n",
    "                if not isinstance(index_df.index, pd.DatetimeIndex):\n",
    "                    index_df.index = pd.to_datetime(index_df.index)\n",
    "                print(f\"Loaded Index data shape: {index_df.shape}\")\n",
    "            except Exception as e_idx:\n",
    "                print(f\"Warning: Could not load or process index data from {index_path}: {e_idx}\")\n",
    "\n",
    "        return df, index_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Step 2: Filter Data (Ensure factors_to_analyze exist before filtering)\n",
    "def filter_data(df, start_date, end_date, filter_rules, factors_to_analyze):\n",
    "    \"\"\"Applies date range and custom filters.\"\"\"\n",
    "    print(\"--- Step 2: Filtering Data ---\")\n",
    "    if df is None: return None\n",
    "\n",
    "    # Check if factor columns exist BEFORE filtering\n",
    "    all_cols_needed = factors_to_analyze + ['open', 'high', 'close'] # Need OHLC for returns\n",
    "    if 'pct_chg' in df.columns: all_cols_needed.append('pct_chg')\n",
    "    # Add columns used in filter_rules if they aren't factors already\n",
    "    for rule in filter_rules:\n",
    "        # Basic parsing to find potential column names in rules\n",
    "        import re\n",
    "        potential_cols = re.findall(r'`([^`]*)`|(\\b[a-zA-Z_][a-zA-Z0-9_]*\\b)', rule)\n",
    "        # Flatten list of tuples and remove empty strings/keywords if any\n",
    "        rule_cols = {item for sublist in potential_cols for item in sublist if item and item not in ['and', 'or', 'not', 'isin']}\n",
    "        all_cols_needed.extend(list(rule_cols))\n",
    "    all_cols_needed = list(set(all_cols_needed)) # Unique columns\n",
    "\n",
    "    missing_factors = [f for f in all_cols_needed if f not in df.columns]\n",
    "    if missing_factors:\n",
    "        print(f\"Error: Required columns missing from data: {missing_factors}\")\n",
    "        return None\n",
    "\n",
    "    # Date filtering\n",
    "    try:\n",
    "        if 'trade_date' not in df.index.names:\n",
    "             raise KeyError(\"'trade_date' not found in DataFrame index levels.\")\n",
    "        trade_date_level = df.index.get_level_values('trade_date')\n",
    "        date_mask = (trade_date_level >= pd.to_datetime(start_date)) & (trade_date_level <= pd.to_datetime(end_date))\n",
    "        df_filtered = df[date_mask].copy()\n",
    "        if df_filtered.empty: raise ValueError(f\"No data remaining after date filtering ({start_date} to {end_date}).\")\n",
    "        print(f\"Filtered by date: {start_date} to {end_date}. Shape: {df_filtered.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during date filtering: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Apply standard filters (Optional - Add if needed, e.g., Redemption, Listing days)\n",
    "    df_filtered['filter_out'] = False\n",
    "    # Example:\n",
    "    # redeem_statuses = ['已公告强赎', '公告到期赎回', '公告实施强赎', '公告提示强赎', '已满足强赎条件']\n",
    "    # if 'is_call' in df_filtered.columns: df_filtered.loc[df_filtered['is_call'].isin(redeem_statuses), 'filter_out'] = True\n",
    "    # if 'list_days' in df_filtered.columns: df_filtered.loc[df_filtered['list_days'] <= 3, 'filter_out'] = True\n",
    "\n",
    "    # Apply custom filters\n",
    "    print(\"Applying custom filters...\")\n",
    "    initial_eligible = len(df_filtered[~df_filtered['filter_out']])\n",
    "    for rule in filter_rules:\n",
    "        try:\n",
    "            # Apply filter to non-filtered-out rows only to avoid errors on already filtered data\n",
    "            current_eligible_indices = df_filtered[~df_filtered['filter_out']].index\n",
    "            if not current_eligible_indices.empty:\n",
    "                 # Query on the subset of eligible rows\n",
    "                 filtered_indices = df_filtered.loc[current_eligible_indices].query(rule).index\n",
    "                 # Mark these as filtered out in the original df_filtered\n",
    "                 df_filtered.loc[filtered_indices, 'filter_out'] = True\n",
    "                 print(f\" - Applied: {rule}. Marked {len(filtered_indices)} additional rows.\")\n",
    "            else:\n",
    "                 print(f\" - Skipping rule (no eligible rows left): {rule}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  - Warning: Could not apply filter rule '{rule}'. Error: {e}\")\n",
    "            # Consider whether a failing filter should halt the process or just be skipped\n",
    "\n",
    "    # --- IMPORTANT: Filter out rows where any needed factor is NaN ---\n",
    "    # We need valid factor values *today* (day T) to correlate with *tomorrow's* return (T+1)\n",
    "    print(\"Filtering rows with NaN in factor values...\")\n",
    "    nan_mask = df_filtered[factors_to_analyze].isna().any(axis=1)\n",
    "    df_filtered.loc[nan_mask, 'filter_out'] = True\n",
    "\n",
    "    final_eligible_count = len(df_filtered[~df_filtered['filter_out']])\n",
    "    filtered_by_nan = initial_eligible - final_eligible_count # Rough count, might double count if filters also hit NaNs\n",
    "    print(f\"NaN factor check complete. Marked approx {filtered_by_nan} additional rows due to NaN factors.\")\n",
    "    print(f\"Filtering complete. Eligible bond-days for return calculation: {final_eligible_count}\")\n",
    "\n",
    "    if final_eligible_count == 0:\n",
    "        print(\"Warning: No bonds eligible after applying all filters and NaN checks.\")\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "# --- NEW Step 3: Calculate Multiple Forward Returns ---\n",
    "def calculate_multiple_fwd_returns(df, pulse_percentages):\n",
    "    \"\"\"\n",
    "    Calculates various next-day return metrics for each bond.\n",
    "    - 'fwd_ret_close': Raw percentage change from current close to next close.\n",
    "    - 'fwd_ret_pulse_X': Return based on pulse stop-profit logic at X%.\n",
    "    \"\"\"\n",
    "    print(\"--- Step 3: Calculating Multiple Forward Returns ---\")\n",
    "    if df is None: return None, []\n",
    "    required_cols = ['open', 'high', 'close'] # Minimum required for pulse logic\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Error: Missing required columns for return calc: {required_cols}\")\n",
    "        return df, [] # Return original df and empty list\n",
    "    if not isinstance(df.index, pd.MultiIndex) or 'code' not in df.index.names:\n",
    "        print(\"Error: DataFrame needs MultiIndex with 'code' level for forward returns.\")\n",
    "        return df, []\n",
    "\n",
    "    df_with_fwd = df.copy()\n",
    "    grouped = df_with_fwd.groupby(level='code', group_keys=False) # group_keys=False avoids adding group keys to index\n",
    "\n",
    "    # Get next day's data by shifting within each group\n",
    "    print(\"  - Shifting data to get next day's OHLC...\")\n",
    "    df_with_fwd['next_open'] = grouped['open'].shift(-1)\n",
    "    df_with_fwd['next_high'] = grouped['high'].shift(-1)\n",
    "    df_with_fwd['next_close'] = grouped['close'].shift(-1)\n",
    "\n",
    "    # Calculate Forward Close-to-Close Return\n",
    "    # Option 1: Use shifted pct_chg if available and reliable\n",
    "    if 'pct_chg' in df_with_fwd.columns:\n",
    "        print(\"  - Calculating fwd_ret_close using shifted 'pct_chg'...\")\n",
    "        # Assuming pct_chg is (close_T / close_{T-1}) - 1\n",
    "        # Then shifted pct_chg is (close_{T+1} / close_T) - 1 which is what we want\n",
    "        df_with_fwd['fwd_ret_close'] = grouped['pct_chg'].shift(-1)\n",
    "    # Option 2: Calculate manually if pct_chg is missing or unreliable\n",
    "    else:\n",
    "        print(\"  - Calculating fwd_ret_close manually from close and next_close...\")\n",
    "        df_with_fwd['fwd_ret_close'] = safe_division(df_with_fwd['next_close'], df_with_fwd['close']) - 1\n",
    "\n",
    "    # Calculate pulse returns\n",
    "    print(f\"  - Calculating forward pulse returns for thresholds: {pulse_percentages}%...\")\n",
    "    current_close = df_with_fwd['close']\n",
    "    next_open = df_with_fwd['next_open']\n",
    "    next_high = df_with_fwd['next_high']\n",
    "    # next_close = df_with_fwd['next_close'] # Needed only if manual fwd_ret_close calc is used below\n",
    "    raw_next_day_ret = df_with_fwd['fwd_ret_close']\n",
    "\n",
    "    # Handle cases where next day data is missing (last day for a bond) or current close is invalid\n",
    "    valid_next_day = next_open.notna() & next_high.notna() & current_close.notna() & (current_close > 0) & raw_next_day_ret.notna()\n",
    "\n",
    "    return_cols = ['fwd_ret_close'] # Start with the base return column name\n",
    "\n",
    "    for pct in pulse_percentages:\n",
    "        ret_col_name = f'fwd_ret_pulse_{pct:.1f}' # e.g., fwd_ret_pulse_2.5\n",
    "        return_cols.append(ret_col_name)\n",
    "        stop_profit_pct = pct / 100.0\n",
    "        # Calculate threshold price based on current close\n",
    "        threshold_price = current_close * (1 + stop_profit_pct)\n",
    "\n",
    "        # Initialize return column with NaN\n",
    "        df_with_fwd[ret_col_name] = np.nan\n",
    "\n",
    "        # --- Vectorized Calculation for Pulse Return ---\n",
    "        # Condition 1: Triggered at open (next_open >= threshold) -> Return is based on next_open\n",
    "        # Use safe_division in case current_close is somehow zero despite filter\n",
    "        open_return = safe_division(next_open, current_close) - 1\n",
    "\n",
    "        # Condition 2: Triggered intraday (next_high >= threshold but next_open < threshold) -> Return is stop_profit_pct\n",
    "        intra_return = stop_profit_pct\n",
    "\n",
    "        # Condition 3: Not triggered -> Return is raw_next_day_ret (close-to-close)\n",
    "\n",
    "        # Apply conditions using np.select or chained .loc\n",
    "        conditions = [\n",
    "            valid_next_day & (next_open >= threshold_price),                     # Triggered at open\n",
    "            valid_next_day & (next_open < threshold_price) & (next_high >= threshold_price), # Triggered intraday\n",
    "            valid_next_day & (next_high < threshold_price)                       # Not triggered\n",
    "        ]\n",
    "        choices = [\n",
    "            open_return,        # Use calculated open_return\n",
    "            intra_return,       # Use the fixed stop-profit percentage\n",
    "            raw_next_day_ret    # Use the close-to-close return\n",
    "        ]\n",
    "\n",
    "        # Apply the logic using np.select for efficiency\n",
    "        df_with_fwd[ret_col_name] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "    # Clean up intermediate columns if desired\n",
    "    df_with_fwd = df_with_fwd.drop(columns=['next_open', 'next_high', 'next_close'], errors='ignore')\n",
    "\n",
    "    # Report NaN counts for the new return columns\n",
    "    nan_counts = df_with_fwd[return_cols].isna().sum()\n",
    "    total_rows = len(df_with_fwd)\n",
    "    print(f\"Calculated forward returns ({len(return_cols)} types). Example NaN counts (out of {total_rows}):\\n{nan_counts}\")\n",
    "\n",
    "    # Check if ALL return columns are completely NaN for the eligible rows\n",
    "    eligible_returns = df_with_fwd.loc[~df_with_fwd['filter_out'], return_cols]\n",
    "    if eligible_returns.isna().all().all():\n",
    "         print(\"Warning: All calculated forward returns are NaN for the eligible data points. IC calculation will yield no results.\")\n",
    "\n",
    "    return df_with_fwd, return_cols\n",
    "\n",
    "\n",
    "# --- MODIFIED Step 4: Analyze Factor vs. Each Return Type Relationship (IC/IR) ---\n",
    "def analyze_factor_return_relationships(df, factors, return_cols):\n",
    "    \"\"\"\n",
    "    Calculates Information Coefficient (IC) and Information Ratio (IR)\n",
    "    for each factor against each specified forward return column.\n",
    "    Uses Spearman rank correlation for IC.\n",
    "    \"\"\"\n",
    "    print(f\"--- Step 4: Analyzing Factor Relationships with {len(return_cols)} Return Types ---\")\n",
    "    if df is None:\n",
    "        print(\"Error: DataFrame is missing.\")\n",
    "        return None\n",
    "    if 'filter_out' not in df.columns:\n",
    "        print(\"Error: 'filter_out' column missing. Cannot select eligible rows.\")\n",
    "        return None\n",
    "    if not return_cols:\n",
    "        print(\"Error: No forward return columns provided for analysis.\")\n",
    "        return None\n",
    "    if not isinstance(df.index, pd.MultiIndex) or 'trade_date' not in df.index.names:\n",
    "        print(\"Error: DataFrame needs MultiIndex with 'trade_date' level for IC calc.\")\n",
    "        return None\n",
    "\n",
    "    all_ic_results = {}\n",
    "    # daily_ic_data = {} # Optional: Store all daily ICs for plotting if needed\n",
    "\n",
    "    # --- Use eligible rows only based on initial filtering ---\n",
    "    # Forward returns were calculated for all rows, but we only analyze rows that were eligible *before* return calc.\n",
    "    df_eligible_base = df[~df['filter_out']].copy() # Select rows eligible on day T\n",
    "\n",
    "    if df_eligible_base.empty:\n",
    "        print(\"Warning: No eligible bond-days found based on initial filters. Cannot calculate IC.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Iterate through each type of forward return calculated in Step 3\n",
    "    for return_col in tqdm(return_cols, desc=\"Analyzing Return Types\"):\n",
    "        if return_col not in df_eligible_base.columns:\n",
    "            print(f\"Warning: Return column '{return_col}' not found in eligible data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n-- Analyzing Factors vs. Return: '{return_col}' --\")\n",
    "\n",
    "        # Prepare data for this specific return type:\n",
    "        # Need factor values from day T and return value for T+1 (which is stored in return_col on day T's row)\n",
    "        # Drop rows where *this specific* return is NaN or the factor is NaN (factor NaNs should be handled by filter_out, but double check)\n",
    "        analysis_subset = df_eligible_base[factors + [return_col]].dropna(subset=[return_col] + factors)\n",
    "\n",
    "        if analysis_subset.empty:\n",
    "            print(f\"Warning: No valid (non-NaN) Factor/Return pairs found for '{return_col}' in eligible data. Skipping IC calculation for this return type.\")\n",
    "            continue\n",
    "\n",
    "        # Group by date to calculate daily IC\n",
    "        grouped = analysis_subset.groupby(level='trade_date')\n",
    "        num_days = len(grouped)\n",
    "        if num_days == 0 :\n",
    "             print(f\"Warning: No trade dates found after filtering NaNs for {return_col}. Skipping.\")\n",
    "             continue\n",
    "        print(f\"Analyzing {num_days} days for '{return_col}' with {len(analysis_subset)} total valid pairs...\")\n",
    "\n",
    "        # Iterate through each factor for the current return type\n",
    "        for factor in factors:\n",
    "            if factor not in analysis_subset.columns: # Should exist, but check\n",
    "                continue\n",
    "\n",
    "            # Function to safely calculate Spearman correlation per day\n",
    "            # It receives a DataFrame group for a specific day\n",
    "            def safe_spearman(group):\n",
    "                # Check if factor and return columns exist in the group\n",
    "                if factor not in group.columns or return_col not in group.columns:\n",
    "                    return np.nan\n",
    "                # Double check for NaNs within the group for this pair (should be removed already, but safety first)\n",
    "                group_cleaned = group[[factor, return_col]].dropna()\n",
    "                if len(group_cleaned) < 5: # Need sufficient pairs for meaningful correlation (adjust threshold if needed)\n",
    "                    return np.nan\n",
    "                # Check for zero variance in either column within the group\n",
    "                if group_cleaned[factor].nunique() <= 1 or group_cleaned[return_col].nunique() <= 1:\n",
    "                     # print(f\"Warning: Zero variance detected for {factor} or {return_col} on date {group.name}. Returning NaN for IC.\")\n",
    "                     return np.nan\n",
    "                try:\n",
    "                    # Use rank correlation (Spearman) for IC\n",
    "                    corr, p_val = spearmanr(group_cleaned[factor], group_cleaned[return_col])\n",
    "                    # Handle potential NaN result from spearmanr itself\n",
    "                    return corr if not np.isnan(corr) else np.nan\n",
    "                except Exception as e: # Catch any other unexpected errors\n",
    "                    # print(f\"Warning: spearmanr failed for {factor}/{return_col} on {group.name}. Error: {e}. Returning NaN.\")\n",
    "                    return np.nan\n",
    "\n",
    "            # Apply the function to each day's group using .apply()\n",
    "            try:\n",
    "                # This applies safe_spearman to each group (each day's subset of data)\n",
    "                # The index of daily_ic will be the 'trade_date'\n",
    "                daily_ic = grouped.apply(safe_spearman)\n",
    "                daily_ic_clean = daily_ic.dropna() # Remove days where IC calculation failed or wasn't possible\n",
    "\n",
    "                # Store daily IC series if needed later\n",
    "                # daily_ic_data[(factor, return_col)] = daily_ic_clean\n",
    "\n",
    "                # Calculate summary statistics for this factor/return pair\n",
    "                if daily_ic_clean.empty:\n",
    "                    # print(f\" - Factor '{factor}': No valid daily ICs calculated.\")\n",
    "                    mean_ic, std_ic, ir, ic_positive_ratio, num_obs = np.nan, np.nan, np.nan, np.nan, 0\n",
    "                elif len(daily_ic_clean) < 2:\n",
    "                    # print(f\" - Factor '{factor}': Only 1 valid daily IC. Cannot calculate Std Dev/IR.\")\n",
    "                    mean_ic = daily_ic_clean.mean()\n",
    "                    std_ic = np.nan # Cannot calculate std dev with 1 point\n",
    "                    ir = np.nan\n",
    "                    ic_positive_ratio = (daily_ic_clean > 0).mean()\n",
    "                    num_obs = len(daily_ic_clean)\n",
    "                else:\n",
    "                    mean_ic = daily_ic_clean.mean()\n",
    "                    std_ic = daily_ic_clean.std()\n",
    "                    # Calculate IR, handle std_dev being zero or NaN\n",
    "                    if pd.notna(std_ic) and std_ic != 0:\n",
    "                        ir = mean_ic / std_ic\n",
    "                    else:\n",
    "                        ir = np.nan\n",
    "                    ic_positive_ratio = (daily_ic_clean > 0).mean()\n",
    "                    num_obs = len(daily_ic_clean) # Number of days with valid IC\n",
    "\n",
    "                # Store the results\n",
    "                all_ic_results[(factor, return_col)] = {\n",
    "                    'Mean IC': mean_ic,\n",
    "                    'IC Std Dev': std_ic,\n",
    "                    'IR (IC Mean/Std)': ir,\n",
    "                    'IC > 0 Ratio': ic_positive_ratio,\n",
    "                    'Num Observations (Days)': num_obs\n",
    "                }\n",
    "                # Optional: Print summary per factor immediately\n",
    "                # print(f\"   - Factor '{factor}': Mean IC={mean_ic:.4f}, IR={ir:.4f}, Obs={num_obs}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during IC calculation loop for factor '{factor}' vs '{return_col}': {e}\")\n",
    "                # Ensure a placeholder entry exists if the loop fails catastrophically\n",
    "                if (factor, return_col) not in all_ic_results:\n",
    "                     all_ic_results[(factor, return_col)] = {\n",
    "                         'Mean IC': np.nan, 'IC Std Dev': np.nan, 'IR (IC Mean/Std)': np.nan,\n",
    "                         'IC > 0 Ratio': np.nan, 'Num Observations (Days)': 0\n",
    "                     }\n",
    "\n",
    "    print(\"\\nIC/IR calculation complete for all factor/return pairs.\")\n",
    "    if not all_ic_results:\n",
    "        print(\"Warning: No IC results were generated.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Format results into a DataFrame\n",
    "    try:\n",
    "        results_df = pd.DataFrame.from_dict(all_ic_results, orient='index')\n",
    "        results_df.index = pd.MultiIndex.from_tuples(results_df.index, names=['Factor', 'Return Type'])\n",
    "        results_df = results_df.sort_index()\n",
    "    except Exception as e_format:\n",
    "        print(f\"Error formatting IC results into DataFrame: {e_format}\")\n",
    "        return None\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Step 5: Analyze Factor Correlation (Optional, Unchanged logic, ensure input data is correct)\n",
    "def analyze_factor_correlation(df, factors):\n",
    "    \"\"\"Calculates and displays the correlation matrix for the selected factors.\"\"\"\n",
    "    print(\"--- Step 5: Analyzing Factor Correlation (Optional) ---\")\n",
    "    if df is None: print(\"Error: DataFrame missing.\"); return None\n",
    "    if 'filter_out' not in df.columns: print(\"Error: 'filter_out' column missing.\"); return None\n",
    "    if not factors: print(\"Error: No factors provided for correlation analysis.\"); return None\n",
    "\n",
    "    # Use data *before* forward returns were added, but after filtering\n",
    "    # Make sure to use only eligible rows\n",
    "    df_eligible = df.loc[~df['filter_out']].copy()\n",
    "\n",
    "    if df_eligible.empty: print(\"Warning: No eligible bonds for correlation.\"); return None\n",
    "\n",
    "    missing_factors = [f for f in factors if f not in df_eligible.columns]\n",
    "    if missing_factors: print(f\"Warning: Factors missing for correlation: {missing_factors}\");\n",
    "    present_factors = [f for f in factors if f in df_eligible.columns]\n",
    "    if len(present_factors) < 2: print(\"Warning: Need at least 2 present factors for correlation.\"); return None\n",
    "\n",
    "    factor_data = df_eligible[present_factors]\n",
    "\n",
    "    # Handle potential infinite values before calculating correlation\n",
    "    factor_data = factor_data.replace([np.inf, -np.inf], np.nan)\n",
    "    # Drop rows where *any* of the present factors are NaN for the correlation calculation\n",
    "    factor_data = factor_data.dropna()\n",
    "\n",
    "    if len(factor_data) < 5: # Need a few points for correlation\n",
    "        print(f\"Warning: Less than 5 valid data points ({len(factor_data)}) after NaN drop for factor correlation.\"); return None\n",
    "\n",
    "    # Optional Sampling for very large datasets\n",
    "    if len(factor_data) > 100000: # Adjust sample size as needed\n",
    "        print(f\"Sampling {100000} rows from {len(factor_data)} for factor correlation calculation...\")\n",
    "        factor_data = factor_data.sample(100000, random_state=42)\n",
    "\n",
    "    print(f\"Calculating Spearman rank correlation matrix on {len(factor_data)} observations...\")\n",
    "    try:\n",
    "        correlation_matrix = factor_data.corr(method='spearman')\n",
    "        print(\"Factor Correlation Matrix:\")\n",
    "        plt.figure(figsize=(max(6, len(present_factors)*0.8), max(5, len(present_factors)*0.6))) # Adjust size\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, annot_kws={\"size\": 8})\n",
    "        plt.title('Factor Spearman Rank Correlation Heatmap')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return correlation_matrix\n",
    "    except Exception as corr_e:\n",
    "         print(f\"Error calculating or plotting correlation: {corr_e}\")\n",
    "         # Try calculating without plotting if plotting fails\n",
    "         try:\n",
    "             correlation_matrix = factor_data.corr(method='spearman')\n",
    "             print(\"Correlation matrix calculated but plotting failed.\")\n",
    "             print(correlation_matrix)\n",
    "             return correlation_matrix\n",
    "         except Exception as corr_e2:\n",
    "             print(f\"Error calculating correlation even without plotting: {corr_e2}\")\n",
    "             return None\n",
    "\n",
    "\n",
    "# --- Main Execution Function ---\n",
    "def run_simplified_factor_analysis(config):\n",
    "    \"\"\"Orchestrates the simplified factor vs. return analysis.\"\"\"\n",
    "\n",
    "    # Step 1: Load Data\n",
    "    df_cb_raw, _ = load_data(config['cb_data_path'], config.get('index_data_path')) # Index not strictly needed now\n",
    "    if df_cb_raw is None:\n",
    "        print(\"Stopping analysis: Data loading failed.\")\n",
    "        return None\n",
    "\n",
    "    # Extract config parameters\n",
    "    start_date = config['start_date']\n",
    "    end_date = config['end_date']\n",
    "    filters = config['filters']\n",
    "    factors_to_analyze = config['factors_to_analyze'] # List of factor names\n",
    "    pulse_percentages = config['pulse_percentages']\n",
    "\n",
    "    # Validate factors exist\n",
    "    missing_factors_init = [f for f in factors_to_analyze if f not in df_cb_raw.columns]\n",
    "    if missing_factors_init:\n",
    "        print(f\"Error: Factors specified in config are missing from the loaded data: {missing_factors_init}\")\n",
    "        print(\"Stopping analysis.\")\n",
    "        return None\n",
    "    print(f\"Factors to analyze: {factors_to_analyze}\")\n",
    "    print(f\"Pulse percentages for forward returns: {pulse_percentages}%\")\n",
    "\n",
    "\n",
    "    # Step 2: Filter Data\n",
    "    # Pass factors_to_analyze to ensure NaNs in these are checked\n",
    "    df_filtered = filter_data(df_cb_raw, start_date, end_date, filters, factors_to_analyze)\n",
    "    if df_filtered is None:\n",
    "        print(\"Stopping analysis: Data filtering failed.\")\n",
    "        return None\n",
    "    if df_filtered[~df_filtered['filter_out']].empty:\n",
    "        print(\"Stopping analysis: No eligible data remaining after filtering.\")\n",
    "        return None\n",
    "\n",
    "    # Step 3: Calculate Multiple Forward Returns\n",
    "    df_with_returns, return_cols = calculate_multiple_fwd_returns(df_filtered, pulse_percentages)\n",
    "    if df_with_returns is None or not return_cols:\n",
    "         print(\"Stopping analysis: Failed to calculate forward returns or no return columns generated.\")\n",
    "         # Return the filtered data for inspection if returns failed\n",
    "         return {\"final_data_before_returns\": df_filtered}\n",
    "\n",
    "    # Step 4: Analyze Factor vs. Each Return Type Relationship\n",
    "    ic_results_df = analyze_factor_return_relationships(df_with_returns, factors_to_analyze, return_cols)\n",
    "\n",
    "    # Step 5: Analyze Factor Correlation (Optional)\n",
    "    factor_correlation_matrix = None\n",
    "    if config.get('analyze_factor_correlation', False): # Add a flag in config\n",
    "        # Pass df_filtered (before returns were added) is safer, as return calc might have failed partially\n",
    "        # or df_with_returns if you want to correlate factors on the dataset that was used for IC\n",
    "        # Using df_filtered ensures we use the data exactly as it was before return calc attempts\n",
    "        print(\"\\nRunning Optional Factor Correlation Analysis...\")\n",
    "        factor_correlation_matrix = analyze_factor_correlation(df_filtered, factors_to_analyze)\n",
    "\n",
    "\n",
    "    # --- Step 6: Report Results ---\n",
    "    print(\"\\n\" + \"=\"*30 + \" Factor Analysis Report \" + \"=\"*30)\n",
    "\n",
    "    # --- IC / IR Results ---\n",
    "    print(\"\\n--- Factor vs. Forward Return Relationship Analysis (IC/IR) ---\")\n",
    "    if ic_results_df is not None and not ic_results_df.empty:\n",
    "        # Display the full IC/IR table\n",
    "        print(\"Full IC/IR Results Table:\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None): # Show all rows/cols\n",
    "            display(ic_results_df.style.format({\n",
    "                'Mean IC': '{:.4f}',\n",
    "                'IC Std Dev': '{:.4f}',\n",
    "                'IR (IC Mean/Std)': '{:.3f}',\n",
    "                'IC > 0 Ratio': '{:.1%}',\n",
    "                'Num Observations (Days)': '{:,.0f}'\n",
    "            }).background_gradient(cmap='coolwarm', subset=['Mean IC', 'IR (IC Mean/Std)'], vmin=-0.1, vmax=0.1)) # Added gradient\n",
    "\n",
    "\n",
    "        # --- Highlight Strongest Relationships based on Mean IC ---\n",
    "        print(\"\\n--- Strongest Relationships (Highest Absolute Mean IC per Return Type) ---\")\n",
    "        try:\n",
    "            # Find index of max absolute IC for each return type\n",
    "            idx = ic_results_df.loc[ic_results_df.groupby(level='Return Type')['Mean IC'].apply(lambda x: x.abs().idxmax())]\n",
    "            display(idx.style.format({\n",
    "                'Mean IC': '{:.4f}', 'IC Std Dev': '{:.4f}', 'IR (IC Mean/Std)': '{:.3f}',\n",
    "                'IC > 0 Ratio': '{:.1%}', 'Num Observations (Days)': '{:,.0f}'\n",
    "            }).set_caption(\"Factors with Highest Absolute Mean IC for each Return Type\"))\n",
    "        except Exception as e_report_ic:\n",
    "             print(f\"Could not determine strongest relationships based on Mean IC: {e_report_ic}\")\n",
    "\n",
    "        # --- Highlight Strongest Relationships based on IR ---\n",
    "        print(\"\\n--- Strongest Relationships (Highest Absolute IR per Return Type) ---\")\n",
    "        try:\n",
    "            # Create Abs IR column for sorting, handle NaNs\n",
    "            ic_results_df_ir = ic_results_df.copy()\n",
    "            ic_results_df_ir['Abs IR'] = ic_results_df_ir['IR (IC Mean/Std)'].abs()\n",
    "            # Find index of max absolute IR for each return type, ignoring NaNs in Abs IR\n",
    "            idx_ir = ic_results_df_ir.loc[ic_results_df_ir.dropna(subset=['Abs IR']).groupby(level='Return Type')['Abs IR'].idxmax()]\n",
    "\n",
    "            if not idx_ir.empty:\n",
    "                 display(idx_ir.drop(columns=['Abs IR']).style.format({ # Display original columns\n",
    "                     'Mean IC': '{:.4f}', 'IC Std Dev': '{:.4f}', 'IR (IC Mean/Std)': '{:.3f}',\n",
    "                     'IC > 0 Ratio': '{:.1%}', 'Num Observations (Days)': '{:,.0f}'\n",
    "                 }).set_caption(\"Factors with Highest Absolute IR for each Return Type\"))\n",
    "            else:\n",
    "                 print(\"Could not determine strongest relationships based on IR (perhaps all IR values were NaN?).\")\n",
    "        except Exception as e_report_ir:\n",
    "             print(f\"Could not determine strongest relationships based on IR: {e_report_ir}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Factor vs. Return relationship results (IC/IR) are not available or empty.\")\n",
    "\n",
    "    # --- Factor Correlation Results ---\n",
    "    if config.get('analyze_factor_correlation', False):\n",
    "        print(\"\\n--- Factor Correlation Matrix ---\")\n",
    "        if factor_correlation_matrix is not None:\n",
    "            print(\"(See heatmap plot above if generated, or matrix output)\")\n",
    "            # Optionally display the matrix again if plotting failed but calculation succeeded\n",
    "            # print(factor_correlation_matrix)\n",
    "        else:\n",
    "            print(\"Factor correlation matrix could not be calculated or plotted.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30 + \" Analysis Complete \" + \"=\"*30)\n",
    "\n",
    "    # Return key results\n",
    "    return {\n",
    "        \"factor_return_ic_ir\": ic_results_df,\n",
    "        \"factor_correlation\": factor_correlation_matrix,\n",
    "        # Include data for inspection (might be large!)\n",
    "        # \"final_data_with_returns\": df_with_returns\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Data ---\n",
      "Attempting to set MultiIndex ['code', 'trade_date']...\n",
      "MultiIndex set successfully.\n",
      "Loaded CB data shape: (593654, 513)\n",
      "Loaded Index data shape: (1765, 8)\n",
      "Factors to analyze: ['conv_prem', 'close']\n",
      "Pulse percentages for forward returns: [3.0]%\n",
      "--- Step 2: Filtering Data ---\n",
      "Filtered by date: 2022-08-01 to 2024-12-31. Shape: (297663, 513)\n",
      "Applying custom filters...\n",
      " - Applied: `close` < 150. Marked 272022 additional rows.\n",
      " - Applied: `close` > 105. Marked 25641 additional rows.\n",
      "Filtering rows with NaN in factor values...\n",
      "NaN factor check complete. Marked approx 297663 additional rows due to NaN factors.\n",
      "Filtering complete. Eligible bond-days for return calculation: 0\n",
      "Warning: No bonds eligible after applying all filters and NaN checks.\n",
      "Stopping analysis: No eligible data remaining after filtering.\n"
     ]
    }
   ],
   "source": [
    "# --- Example Configuration ---\n",
    "# Use the structure relevant to *this* analysis (factor vs. return correlation)\n",
    "# The second CONFIG example you provided seemed designed for a different task (composite scoring).\n",
    "CONFIG = {\n",
    "    # File Paths\n",
    "    'cb_data_path': '/Users/yiwei/Desktop/git/cb_data_with_factors_enhanced_with_junxian.pq', # *** CHANGE TO YOUR PATH ***\n",
    "    'index_data_path': '/Users/yiwei/Desktop/git/index.pq', # Optional, not used in core IC calc\n",
    "\n",
    "    # Analysis Time Period\n",
    "    'start_date': '2022-08-01', # Use YYYY-MM-DD format\n",
    "    'end_date': '2024-12-31',   # Use YYYY-MM-DD format\n",
    "\n",
    "    # Data Filtering Rules (applied before return calc and IC)\n",
    "    # Use pandas query syntax (backticks for special characters)\n",
    "    'filters': [\n",
    "        # \"`转股溢价率` < 0.5\",   # Example: Premium < 50%\n",
    "        # \"`剩余规模` > 0.1\",     # Example: Remaining size > 0.1 Billion\n",
    "        \"`close` < 150\",       # Example: Price < 150\n",
    "        \"`close` > 105\",       # Example: Price > 105\n",
    "        # Add more relevant filters based on your strategy/universe\n",
    "        # Example: Exclude bonds near redemption\n",
    "        # \"`is_call`.isin(['已公告强赎', '公告到期赎回', '公告实施强赎', '公告提示强赎', '已满足强赎条件']) == False\",\n",
    "        # Example: Exclude newly listed bonds\n",
    "        # \"`list_days` > 3\"\n",
    "    ],\n",
    "\n",
    "    # Factors to Analyze (Column names from your Parquet file)\n",
    "    'factors_to_analyze': [\n",
    "        # 'ytm',              # Yield to maturity\n",
    "        'conv_prem',        # Conversion premium % (e.g., 0.2 for 20%)\n",
    "        # '剩余规模',         # Remaining size (e.g., in Billion Yuan)\n",
    "        # '成交量比',          # Example: Volume ratio (make sure column exists)\n",
    "        # '双低值',            # Example: Double low value (make sure column exists)\n",
    "        # '纯债价值',          # Example: Pure bond value\n",
    "        'close',            # Closing price itself can be a factor\n",
    "        # 'pb_stk',           # Underlying stock PB ratio\n",
    "        # '总市值',           # Market cap of underlying stock\n",
    "        # '波动率_10d',       # 10-day volatility (ensure exists)\n",
    "        # Add ALL factor column names you want to analyze\n",
    "    ],\n",
    "\n",
    "    # Pulse Percentages for Forward Return Calculation\n",
    "    # Defines the stop-profit thresholds (e.g., 2.0 means 2%)\n",
    "    # 'pulse_percentages': [2.0, 2.5, 2.8, 3.0, 3.5, 3.8, 4.0, 5.0],\n",
    "    'pulse_percentages': [3.0],\n",
    "\n",
    "    # Optional: Run Factor Correlation Analysis\n",
    "    'analyze_factor_correlation': True # Set to True to calculate and plot factor correlations\n",
    "}\n",
    "\n",
    "analysis_results = run_simplified_factor_analysis(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Run the Analysis ---\n",
    "# if __name__ == \"__main__\": # Ensures code runs only when script is executed directly\n",
    "#     # Make sure the file paths in CONFIG are correct before running!\n",
    "#     print(\"Starting Factor Analysis...\")\n",
    "#     analysis_results = run_simplified_factor_analysis(CONFIG)\n",
    "#     print(\"Analysis finished.\")\n",
    "\n",
    "#     # You can access results for further processing if needed:\n",
    "#     if analysis_results and \"factor_return_ic_ir\" in analysis_results:\n",
    "#         ic_ir_df = analysis_results[\"factor_return_ic_ir\"]\n",
    "#         if ic_ir_df is not None:\n",
    "#              print(\"\\n --- IC/IR Results DataFrame Head (for verification) ---\")\n",
    "#              display(ic_ir_df.head())\n",
    "#         else:\n",
    "#              print(\"IC/IR DataFrame was not generated.\")\n",
    "#     else:\n",
    "#         print(\"Analysis did not complete successfully or did not return IC/IR results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
